{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e622f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:29:49.432972Z",
     "start_time": "2023-04-12T13:29:49.145386Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36099285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:29:52.027467Z",
     "start_time": "2023-04-12T13:29:49.442105Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import math\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44fd02bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:29:52.353427Z",
     "start_time": "2023-04-12T13:29:52.038276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "\n",
    "# Get the absolute path to the precal_hazard directory\n",
    "# Which is two directories above notebooks/exploration/\n",
    "abs_dir = os.path.abspath(Path(os.getcwd()).parents[1])\n",
    "# Get raw data directory\n",
    "fr = join(abs_dir, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "fi = join(abs_dir, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "fp = join(abs_dir, 'data', 'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97219c6c",
   "metadata": {},
   "source": [
    "# Get National Structure Inventory Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b709c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T19:48:39.287228Z",
     "start_time": "2023-04-11T19:48:24.973083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call the NSI API by fips\n",
    "# Camden, County NJ\n",
    "fips_list = ['34007']\n",
    "\n",
    "# Get the URL\n",
    "url = \"https://nsi.sec.usace.army.mil/nsiapi/structures\"\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the NSI API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for NSI DFs\n",
    "nsi_df_list = []\n",
    "\n",
    "for fips in fips_list:\n",
    "    # GET Request\n",
    "    nsi_get = requests.get(url + '?fips=' + fips)\n",
    "    \n",
    "    # Temp data frame\n",
    "    temp = pd.json_normalize(nsi_get.json()['features'])\n",
    "    \n",
    "    # Add to list\n",
    "    nsi_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nsi = pd.concat(nsi_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nsi.to_parquet(join(fr, 'exposure', 'nsi.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a21dc",
   "metadata": {},
   "source": [
    "# Download NFIP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33323f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T20:29:43.736876Z",
     "start_time": "2023-04-11T20:29:14.101436Z"
    }
   },
   "outputs": [],
   "source": [
    "# pol Policies for Camden\n",
    "# Call the pol API by fips\n",
    "# Camden, County NJ\n",
    "fips_list = ['34007']\n",
    "\n",
    "# Get the URL for querying policies\n",
    "url = \"https://www.fema.gov/api/open/v1/FimaNfipPolicies?$\"\n",
    "# Get the URL for # policies that meet request\n",
    "check = url + \"inlinecount=allpages&$top=1&$select=id&$\"\n",
    "\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the Pols API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for Pols DFs\n",
    "pol_df_list = []\n",
    "\n",
    "# NFIP API usage adapts R code here: https://docs.ropensci.org/rfema/\n",
    "# And follows OpenFEMA guide: \n",
    "# https://www.fema.gov/about/openfema/working-with-large-data-sets#app-a\n",
    "\n",
    "for fips in fips_list:\n",
    "    # County endpoint\n",
    "    c_end = \"filter=countyCode%20eq%20%27\" + fips + \"%27\"\n",
    "    \n",
    "    # First, get the total number of records\n",
    "    records = requests.get(check + c_end)\n",
    "    n_rec = pd.json_normalize(records.json())['metadata.count'][0]\n",
    "    \n",
    "    # Get iterations needed (1,000 record limit)\n",
    "    iterations = math.ceil(n_rec / 1000)\n",
    "    \n",
    "    # Now, download 1,000 records at a time and store in list\n",
    "    # Loop through required iterations and keep appending policy \n",
    "    # data from the GET request to the pol_df_list\n",
    "    for i in range(iterations):\n",
    "        skip_str = \"&$skip=\" + str(i*1000)\n",
    "    \n",
    "        # GET Request\n",
    "        pol_get = requests.get(url + c_end + skip_str)\n",
    "\n",
    "        # Temp data frame\n",
    "        temp = pd.json_normalize(pol_get.json()['FimaNfipPolicies'])\n",
    "\n",
    "        # Add to list\n",
    "        pol_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nfip_pol = pd.concat(pol_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nfip_pol.to_parquet(join(fr, 'exposure', 'nfip_pols.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13b482c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T20:44:22.121531Z",
     "start_time": "2023-04-11T20:44:20.436086Z"
    }
   },
   "outputs": [],
   "source": [
    "# NFIP Claims for Camden\n",
    "# claim claimicies for Camden\n",
    "# Call the claim API by fips\n",
    "# Camden, County NJ\n",
    "fips_list = ['34007']\n",
    "\n",
    "# Get the URL for querying claimicies\n",
    "url = \"https://www.fema.gov/api/open/v1/FimaNfipClaims?$\"\n",
    "# Get the URL for # claimicies that meet request\n",
    "check = url + \"inlinecount=allpages&$top=1&$select=id&$\"\n",
    "\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the claims API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for claims DFs\n",
    "claim_df_list = []\n",
    "\n",
    "# NFIP API usage adapts R code here: https://docs.ropensci.org/rfema/\n",
    "# And follows OpenFEMA guide: \n",
    "# https://www.fema.gov/about/openfema/working-with-large-data-sets#app-a\n",
    "\n",
    "for fips in fips_list:\n",
    "    # County endpoint\n",
    "    c_end = \"filter=countyCode%20eq%20%27\" + fips + \"%27\"\n",
    "    \n",
    "    # First, get the total number of records\n",
    "    records = requests.get(check + c_end)\n",
    "    n_rec = pd.json_normalize(records.json())['metadata.count'][0]\n",
    "    \n",
    "    # Get iterations needed (1,000 record limit)\n",
    "    iterations = math.ceil(n_rec / 1000)\n",
    "    \n",
    "    # Now, download 1,000 records at a time and store in list\n",
    "    # Loop through required iterations and keep appending claimicy \n",
    "    # data from the GET request to the claim_df_list\n",
    "    for i in range(iterations):\n",
    "        skip_str = \"&$skip=\" + str(i*1000)\n",
    "    \n",
    "        # GET Request\n",
    "        claim_get = requests.get(url + c_end + skip_str)\n",
    "\n",
    "        # Temp data frame\n",
    "        temp = pd.json_normalize(claim_get.json()['FimaNfipClaims'])\n",
    "\n",
    "        # Add to list\n",
    "        claim_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nfip_claim = pd.concat(claim_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nfip_claim.to_parquet(join(fr, 'exposure', 'nfip_claims.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e83940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add all data sources currently in keller-lab/data\n",
    "# Remove those data sources from that repo, makes sense\n",
    "# to instead use project by project data model\n",
    "# since we need the data accessible for reproducibility\n",
    "# HMGP, IHP, PA\n",
    "# Also Camden County NFHL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb22f3",
   "metadata": {},
   "source": [
    "# Download Camden County Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "50729ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T22:29:55.961811Z",
     "start_time": "2023-04-11T22:29:46.487448Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid field type <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 56\u001b[0m\n\u001b[1;32m     51\u001b[0m pars_geo \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame(pars,\n\u001b[1;32m     52\u001b[0m                             crs\u001b[38;5;241m=\u001b[39mEPSG,\n\u001b[1;32m     53\u001b[0m                             geometry\u001b[38;5;241m=\u001b[39mpars[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Write data to file\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mpars_geo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexposure\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpc.gpkg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGPKG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/jumbo/keller-lab/Applications/miniconda3/envs/gc_elev/lib/python3.10/site-packages/geopandas/geodataframe.py:1203\u001b[0m, in \u001b[0;36mGeoDataFrame.to_file\u001b[0;34m(self, filename, driver, schema, index, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write the ``GeoDataFrame`` to a file.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03mBy default, an ESRI shapefile is written, but any OGR data source\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;124;03m>>> gdf.to_file('dataframe.shp', mode=\"a\")  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _to_file\n\u001b[0;32m-> 1203\u001b[0m \u001b[43m_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/jumbo/keller-lab/Applications/miniconda3/envs/gc_elev/lib/python3.10/site-packages/geopandas/io/file.py:545\u001b[0m, in \u001b[0;36m_to_file\u001b[0;34m(df, filename, driver, schema, index, mode, crs, engine, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn names longer than 10 characters will be truncated when saved to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mESRI Shapefile.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    541\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 545\u001b[0m     \u001b[43m_to_file_fiona\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    547\u001b[0m     _to_file_pyogrio(df, filename, driver, schema, crs, mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/jumbo/keller-lab/Applications/miniconda3/envs/gc_elev/lib/python3.10/site-packages/geopandas/io/file.py:575\u001b[0m, in \u001b[0;36m_to_file_fiona\u001b[0;34m(df, filename, driver, schema, crs, mode, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m     crs_wkt \u001b[38;5;241m=\u001b[39m crs\u001b[38;5;241m.\u001b[39mto_wkt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWKT1_GDAL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    573\u001b[0m     filename, mode\u001b[38;5;241m=\u001b[39mmode, driver\u001b[38;5;241m=\u001b[39mdriver, crs_wkt\u001b[38;5;241m=\u001b[39mcrs_wkt, schema\u001b[38;5;241m=\u001b[39mschema, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    574\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m colxn:\n\u001b[0;32m--> 575\u001b[0m     \u001b[43mcolxn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterecords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/jumbo/keller-lab/Applications/miniconda3/envs/gc_elev/lib/python3.10/site-packages/fiona/collection.py:361\u001b[0m, in \u001b[0;36mCollection.writerecords\u001b[0;34m(self, records)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection not open for writing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterecs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget_length()\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:1291\u001b[0m, in \u001b[0;36mfiona.ogrext.WritingSession.writerecs\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:466\u001b[0m, in \u001b[0;36mfiona.ogrext.OGRFeatureBuilder.build\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid field type <class 'list'>"
     ]
    }
   ],
   "source": [
    "# Parcels\n",
    "par_df_list = []\n",
    "\n",
    "# Data is in epsg: 3424\n",
    "EPSG = '3424'\n",
    "\n",
    "# Store base URL\n",
    "par_url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/Parcel_Data_2021_Redacted/FeatureServer/1/query?outFields=*\"\n",
    "close_str = \"&f=geojson\"\n",
    "\n",
    "# Filter on municipality\n",
    "mun_str = \"&where=MUNICIPALITY%3D%27GLOUCESTER+CITY%27\"\n",
    "# Record count, 2000 at a time\n",
    "rec_str = \"&resultRecordCount=2000\"\n",
    "# Update resultOffset by 2000 at a time\n",
    "rec_n = 2000\n",
    "\n",
    "# Get number of records\n",
    "num_rec_url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/Parcel_Data_2021_Redacted/FeatureServer/1/query?where=MUNICIPALITY%3D%27GLOUCESTER+CITY%27&returnCountOnly=true&f=json\"\n",
    "num_r = requests.get(num_rec_url).json()['count']\n",
    "\n",
    "# Get iterations needed \n",
    "iterations = math.ceil(num_r / rec_n)\n",
    "\n",
    "# Now, download 2,000 records at a time and store in list\n",
    "# Loop through required iterations and keep appending claimicy \n",
    "# data from the GET request to the claim_df_list\n",
    "for i in range(iterations):\n",
    "    skip_str = \"&resultOffset=\" + str(i*rec_n)\n",
    "\n",
    "    # GET Request\n",
    "    par_get = requests.get(par_url + mun_str + skip_str + rec_str + close_str)\n",
    "\n",
    "    # Temp data frame\n",
    "    temp = par_get.json()['features']\n",
    "    temp_df = pd.json_normalize(temp)\n",
    "    temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "    # Geodataframe with temp_df & temp_geo linked\n",
    "    par_geo = gpd.GeoDataFrame(temp_df,\n",
    "                               crs=EPSG,\n",
    "                               geometry=temp_geo) \n",
    "\n",
    "    # Add to list\n",
    "    par_df_list.append(par_geo)\n",
    "\n",
    "# Concat\n",
    "pars = pd.concat(par_df_list, axis=0)\n",
    "\n",
    "# Get back to geodataframe\n",
    "pars_geo = gpd.GeoDataFrame(pars,\n",
    "                            crs=EPSG,\n",
    "                            geometry=pars['geometry'])\n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "pars_geo = pars_geo.drop(columns=drop_col)\n",
    "\n",
    "# Write data to file\n",
    "pars_geo.to_file(join(fr, 'exposure', 'pc.gpkg'),\n",
    "                 driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d9e6c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:52:05.932498Z",
     "start_time": "2023-04-12T14:52:04.902335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tracts\n",
    "# tract Codes\n",
    "url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/CensusTracts/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\"\n",
    "\n",
    "# Data in epsg: 3424\n",
    "EPSG = '4326'\n",
    "\n",
    "# GET Request\n",
    "tract_get = requests.get(url)\n",
    "\n",
    "# No loop needed, just the one tracticipality\n",
    "temp = tract_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "tract_geo = gpd.GeoDataFrame(temp_df,\n",
    "                           crs=EPSG,\n",
    "                           geometry=temp_geo)\n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "tract_geo = tract_geo.drop(columns=drop_col)\n",
    "\n",
    "# Reproject to 3424\n",
    "OUT_EPSG = '3424'\n",
    "tract_geo = tract_geo.to_crs(epsg=OUT_EPSG)\n",
    "\n",
    "# Write data to file\n",
    "tract_geo.to_file(join(fr, 'ref', 'tracts.gpkg'),\n",
    "                driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e58790a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:50:16.817840Z",
     "start_time": "2023-04-12T14:50:15.299858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Zip Codes\n",
    "url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/Zip_Codes/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\"\n",
    "\n",
    "# Data in epsg: 3424\n",
    "EPSG = '4326'\n",
    "\n",
    "# GET Request\n",
    "zip_get = requests.get(url)\n",
    "\n",
    "# No loop needed, just the one zipicipality\n",
    "temp = zip_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "zip_geo = gpd.GeoDataFrame(temp_df,\n",
    "                           crs=EPSG,\n",
    "                           geometry=temp_geo)\n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "zip_geo = zip_geo.drop(columns=drop_col)\n",
    "\n",
    "# Reproject to 3424\n",
    "OUT_EPSG = '3424'\n",
    "zip_geo = zip_geo.to_crs(epsg=OUT_EPSG)\n",
    "\n",
    "# Write data to file\n",
    "zip_geo.to_file(join(fr, 'ref', 'zipcodes.gpkg'),\n",
    "                driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cca8026f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:40:23.199234Z",
     "start_time": "2023-04-12T13:40:22.290814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Municipalities for Camden (useful for clipping other data to GC)\n",
    "url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/CamdenCountyMunicipalLayer/FeatureServer/0/query?f=geojson&where=(NAMELSAD%20IN%20(%27Gloucester%20City%20city%27))&outFields=*\"\n",
    "\n",
    "# Data is in epsg: 4326\n",
    "# Metadata suggests 26918 but\n",
    "# local download shows epsg is 4326\n",
    "EPSG = '4326'\n",
    "\n",
    "# GET Request\n",
    "mun_get = requests.get(url)\n",
    "\n",
    "# No loop needed, just the one municipality\n",
    "temp = mun_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "mun_geo = gpd.GeoDataFrame(temp_df,\n",
    "                           crs=EPSG,\n",
    "                           geometry=temp_geo)\n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "mun_geo = mun_geo.drop(columns=drop_col)\n",
    "\n",
    "# Reproject to 3424\n",
    "OUT_EPSG = '3424'\n",
    "mun_geo = mun_geo.to_crs(epsg=OUT_EPSG)\n",
    "\n",
    "# Write data to file\n",
    "mun_geo.to_file(join(fr, 'ref', 'city_clip.gpkg'),\n",
    "                driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7fd117a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T23:16:17.778971Z",
     "start_time": "2023-04-11T23:16:17.151445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Land Uses\n",
    "url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/DVRPC_2010_Land_Use/FeatureServer/0/query?f=geojson&where=(Mun_Name%20IN%20(%27Gloucester%20City%27))&outFields=*\"\n",
    "# Data is in epsg: 3424\n",
    "EPSG = '3424'\n",
    "\n",
    "# GET Request\n",
    "lu_get = requests.get(url)\n",
    "\n",
    "# No loop needed because only ~100 records\n",
    "# Temp data frame\n",
    "temp = lu_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "lu_geo = gpd.GeoDataFrame(temp_df,\n",
    "                          crs=EPSG,\n",
    "                          geometry=temp_geo)\n",
    "   \n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "lu_geo = lu_geo.drop(columns=drop_col)\n",
    "\n",
    "# Write data to file\n",
    "lu_geo.to_file(join(fr, 'exposure', 'landuse.gpkg'),\n",
    "               driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41efff2",
   "metadata": {},
   "source": [
    "# Download Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5399841d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T12:47:23.236447Z",
     "start_time": "2023-04-12T12:47:21.353241Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOAA SOVI\n",
    "url = 'https://coast.noaa.gov/htdata/SocioEconomic/SoVI2010/SoVI_2010_NJ.zip'\n",
    "save_path = join(fr, 'vulnerability', 'social', 'noaa.zip')\n",
    "\n",
    "# Request and writing zip from here\n",
    "# https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
    "chunk_size = 128\n",
    "r = requests.get(url, stream=True)\n",
    "with open(save_path, 'wb') as fd:\n",
    "    for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "        fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89b9b0ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:19:47.681132Z",
     "start_time": "2023-04-12T14:19:46.784987Z"
    }
   },
   "outputs": [],
   "source": [
    "# NJ env. burd communities\n",
    "url = \"https://services1.arcgis.com/QWdNfRs7lkPq4g4Q/ArcGIS/rest/services/Overburdened_Communities_2020_Hosted/FeatureServer/0/query?where=NAME%3D%27Gloucester+City%27&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&relationParam=&returnGeodetic=false&outFields=*&returnGeometry=true&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&defaultSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token=\"\n",
    "# Data is supposed to be in epsg: 3424\n",
    "# metadata says so\n",
    "# but it's in epsg 4326\n",
    "EPSG = '4326'\n",
    "\n",
    "# GET Request\n",
    "ob_get = requests.get(url)\n",
    "\n",
    "# No loop needed because only ~100 records\n",
    "# Temp data frame\n",
    "temp = ob_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "ob_geo = gpd.GeoDataFrame(temp_df,\n",
    "                          crs=EPSG,\n",
    "                          geometry=temp_geo)\n",
    "   \n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "ob_geo = ob_geo.drop(columns=drop_col)\n",
    "\n",
    "# Reproject to 3424\n",
    "OUT_EPSG = '3424'\n",
    "ob_geo = ob_geo.to_crs(epsg=OUT_EPSG)\n",
    "\n",
    "# Write data to file\n",
    "ob_geo.to_file(join(fr, 'vulnerability', 'social', 'overburdened.gpkg'),\n",
    "               driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53c5e72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T19:54:05.847566Z",
     "start_time": "2023-04-11T19:54:05.619626Z"
    }
   },
   "outputs": [],
   "source": [
    "# CEJST (download later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff58608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gc_elev",
   "language": "python",
   "name": "gc_elev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

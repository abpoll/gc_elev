{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e622f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T19:07:54.488615Z",
     "start_time": "2023-04-12T19:07:54.265174Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36099285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T19:07:57.545168Z",
     "start_time": "2023-04-12T19:07:54.493995Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import math\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44fd02bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T19:07:57.892487Z",
     "start_time": "2023-04-12T19:07:57.550034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "\n",
    "# Get the absolute path to the precal_hazard directory\n",
    "# Which is two directories above notebooks/exploration/\n",
    "abs_dir = os.path.abspath(Path(os.getcwd()).parents[1])\n",
    "# Get raw data directory\n",
    "fr = join(abs_dir, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "fi = join(abs_dir, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "fp = join(abs_dir, 'data', 'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97219c6c",
   "metadata": {},
   "source": [
    "# Get National Structure Inventory Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b709c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T19:48:39.287228Z",
     "start_time": "2023-04-11T19:48:24.973083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call the NSI API by fips\n",
    "# Camden, County NJ\n",
    "fips_list = ['34007']\n",
    "\n",
    "# Get the URL\n",
    "url = \"https://nsi.sec.usace.army.mil/nsiapi/structures\"\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the NSI API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for NSI DFs\n",
    "nsi_df_list = []\n",
    "\n",
    "for fips in fips_list:\n",
    "    # GET Request\n",
    "    nsi_get = requests.get(url + '?fips=' + fips)\n",
    "    \n",
    "    # Temp data frame\n",
    "    temp = pd.json_normalize(nsi_get.json()['features'])\n",
    "    \n",
    "    # Add to list\n",
    "    nsi_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nsi = pd.concat(nsi_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nsi.to_parquet(join(fr, 'exposure', 'nsi.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a21dc",
   "metadata": {},
   "source": [
    "# Download NFIP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33323f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T20:29:43.736876Z",
     "start_time": "2023-04-11T20:29:14.101436Z"
    }
   },
   "outputs": [],
   "source": [
    "# pol Policies for Camden\n",
    "# Call the pol API by fips\n",
    "# Camden, County NJ\n",
    "fips_list = ['34007']\n",
    "\n",
    "# Get the URL for querying policies\n",
    "url = \"https://www.fema.gov/api/open/v1/FimaNfipPolicies?$\"\n",
    "# Get the URL for # policies that meet request\n",
    "check = url + \"inlinecount=allpages&$top=1&$select=id&$\"\n",
    "\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the Pols API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for Pols DFs\n",
    "pol_df_list = []\n",
    "\n",
    "# NFIP API usage adapts R code here: https://docs.ropensci.org/rfema/\n",
    "# And follows OpenFEMA guide: \n",
    "# https://www.fema.gov/about/openfema/working-with-large-data-sets#app-a\n",
    "\n",
    "for fips in fips_list:\n",
    "    # County endpoint\n",
    "    c_end = \"filter=countyCode%20eq%20%27\" + fips + \"%27\"\n",
    "    \n",
    "    # First, get the total number of records\n",
    "    records = requests.get(check + c_end)\n",
    "    n_rec = pd.json_normalize(records.json())['metadata.count'][0]\n",
    "    \n",
    "    # Get iterations needed (1,000 record limit)\n",
    "    iterations = math.ceil(n_rec / 1000)\n",
    "    \n",
    "    # Now, download 1,000 records at a time and store in list\n",
    "    # Loop through required iterations and keep appending policy \n",
    "    # data from the GET request to the pol_df_list\n",
    "    for i in range(iterations):\n",
    "        skip_str = \"&$skip=\" + str(i*1000)\n",
    "    \n",
    "        # GET Request\n",
    "        pol_get = requests.get(url + c_end + skip_str)\n",
    "\n",
    "        # Temp data frame\n",
    "        temp = pd.json_normalize(pol_get.json()['FimaNfipPolicies'])\n",
    "\n",
    "        # Add to list\n",
    "        pol_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nfip_pol = pd.concat(pol_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nfip_pol.to_parquet(join(fr, 'exposure', 'nfip_pols.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13b482c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T20:44:22.121531Z",
     "start_time": "2023-04-11T20:44:20.436086Z"
    }
   },
   "outputs": [],
   "source": [
    "# NFIP Claims for Camden\n",
    "# claim claimicies for Camden\n",
    "# Call the claim API by fips\n",
    "# Camden, County NJ\n",
    "fips_list = ['34007']\n",
    "\n",
    "# Get the URL for querying claimicies\n",
    "url = \"https://www.fema.gov/api/open/v1/FimaNfipClaims?$\"\n",
    "# Get the URL for # claimicies that meet request\n",
    "check = url + \"inlinecount=allpages&$top=1&$select=id&$\"\n",
    "\n",
    "\n",
    "# Loop through counties, \n",
    "# Get the data from the claims API\n",
    "# Store in dataframe\n",
    "# Add to list\n",
    "# Concat all the dfs\n",
    "\n",
    "# List for claims DFs\n",
    "claim_df_list = []\n",
    "\n",
    "# NFIP API usage adapts R code here: https://docs.ropensci.org/rfema/\n",
    "# And follows OpenFEMA guide: \n",
    "# https://www.fema.gov/about/openfema/working-with-large-data-sets#app-a\n",
    "\n",
    "for fips in fips_list:\n",
    "    # County endpoint\n",
    "    c_end = \"filter=countyCode%20eq%20%27\" + fips + \"%27\"\n",
    "    \n",
    "    # First, get the total number of records\n",
    "    records = requests.get(check + c_end)\n",
    "    n_rec = pd.json_normalize(records.json())['metadata.count'][0]\n",
    "    \n",
    "    # Get iterations needed (1,000 record limit)\n",
    "    iterations = math.ceil(n_rec / 1000)\n",
    "    \n",
    "    # Now, download 1,000 records at a time and store in list\n",
    "    # Loop through required iterations and keep appending claimicy \n",
    "    # data from the GET request to the claim_df_list\n",
    "    for i in range(iterations):\n",
    "        skip_str = \"&$skip=\" + str(i*1000)\n",
    "    \n",
    "        # GET Request\n",
    "        claim_get = requests.get(url + c_end + skip_str)\n",
    "\n",
    "        # Temp data frame\n",
    "        temp = pd.json_normalize(claim_get.json()['FimaNfipClaims'])\n",
    "\n",
    "        # Add to list\n",
    "        claim_df_list.append(temp)\n",
    "\n",
    "# Concat\n",
    "nfip_claim = pd.concat(claim_df_list, axis=0)\n",
    "\n",
    "# Write to file\n",
    "nfip_claim.to_parquet(join(fr, 'exposure', 'nfip_claims.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e83940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add all data sources currently in keller-lab/data\n",
    "# Remove those data sources from that repo, makes sense\n",
    "# to instead use project by project data model\n",
    "# since we need the data accessible for reproducibility\n",
    "# HMGP, IHP, PA\n",
    "# Also Camden County NFHL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb22f3",
   "metadata": {},
   "source": [
    "# Download Camden County Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50729ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T19:08:40.124899Z",
     "start_time": "2023-04-12T19:08:28.237703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parcels\n",
    "par_df_list = []\n",
    "\n",
    "# Data is in epsg: 3424\n",
    "EPSG = '4326'\n",
    "\n",
    "# Store base URL\n",
    "par_url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/Parcel_Data_2021_Redacted/FeatureServer/1/query?outFields=*\"\n",
    "close_str = \"&f=geojson\"\n",
    "\n",
    "# Filter on municipality\n",
    "mun_str = \"&where=MUNICIPALITY%3D%27GLOUCESTER+CITY%27\"\n",
    "# Record count, 2000 at a time\n",
    "rec_str = \"&resultRecordCount=2000\"\n",
    "# Update resultOffset by 2000 at a time\n",
    "rec_n = 2000\n",
    "\n",
    "# Get number of records\n",
    "num_rec_url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/Parcel_Data_2021_Redacted/FeatureServer/1/query?where=MUNICIPALITY%3D%27GLOUCESTER+CITY%27&returnCountOnly=true&f=json\"\n",
    "num_r = requests.get(num_rec_url).json()['count']\n",
    "\n",
    "# Get iterations needed \n",
    "iterations = math.ceil(num_r / rec_n)\n",
    "\n",
    "# Now, download 2,000 records at a time and store in list\n",
    "# Loop through required iterations and keep appending claimicy \n",
    "# data from the GET request to the claim_df_list\n",
    "for i in range(iterations):\n",
    "    skip_str = \"&resultOffset=\" + str(i*rec_n)\n",
    "\n",
    "    # GET Request\n",
    "    par_get = requests.get(par_url + mun_str + skip_str + rec_str + close_str)\n",
    "\n",
    "    # Temp data frame\n",
    "    temp = par_get.json()['features']\n",
    "    temp_df = pd.json_normalize(temp)\n",
    "    temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "    # Geodataframe with temp_df & temp_geo linked\n",
    "    par_geo = gpd.GeoDataFrame(temp_df,\n",
    "                               crs=EPSG,\n",
    "                               geometry=temp_geo) \n",
    "\n",
    "    # Add to list\n",
    "    par_df_list.append(par_geo)\n",
    "\n",
    "# Concat\n",
    "pars = pd.concat(par_df_list, axis=0)\n",
    "\n",
    "# Get back to geodataframe\n",
    "pars_geo = gpd.GeoDataFrame(pars,\n",
    "                            crs=EPSG,\n",
    "                            geometry=pars['geometry'])\n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "pars_geo = pars_geo.drop(columns=drop_col)\n",
    "\n",
    "# Reproject\n",
    "pars_geo = pars_geo.to_crs(epsg='3424')\n",
    "\n",
    "# Write data to file\n",
    "pars_geo.to_file(join(fr, 'exposure', 'pc.gpkg'),\n",
    "                 driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d9e6c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:52:05.932498Z",
     "start_time": "2023-04-12T14:52:04.902335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tracts\n",
    "# tract Codes\n",
    "url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/CensusTracts/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\"\n",
    "\n",
    "# Data in epsg: 3424\n",
    "EPSG = '4326'\n",
    "\n",
    "# GET Request\n",
    "tract_get = requests.get(url)\n",
    "\n",
    "# No loop needed, just the one tracticipality\n",
    "temp = tract_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "tract_geo = gpd.GeoDataFrame(temp_df,\n",
    "                           crs=EPSG,\n",
    "                           geometry=temp_geo)\n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "tract_geo = tract_geo.drop(columns=drop_col)\n",
    "\n",
    "# Reproject to 3424\n",
    "OUT_EPSG = '3424'\n",
    "tract_geo = tract_geo.to_crs(epsg=OUT_EPSG)\n",
    "\n",
    "# Write data to file\n",
    "tract_geo.to_file(join(fr, 'ref', 'tracts.gpkg'),\n",
    "                driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e58790a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:50:16.817840Z",
     "start_time": "2023-04-12T14:50:15.299858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Zip Codes\n",
    "url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/Zip_Codes/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\"\n",
    "\n",
    "# Data in epsg: 3424\n",
    "EPSG = '4326'\n",
    "\n",
    "# GET Request\n",
    "zip_get = requests.get(url)\n",
    "\n",
    "# No loop needed, just the one zipicipality\n",
    "temp = zip_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "zip_geo = gpd.GeoDataFrame(temp_df,\n",
    "                           crs=EPSG,\n",
    "                           geometry=temp_geo)\n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "zip_geo = zip_geo.drop(columns=drop_col)\n",
    "\n",
    "# Reproject to 3424\n",
    "OUT_EPSG = '3424'\n",
    "zip_geo = zip_geo.to_crs(epsg=OUT_EPSG)\n",
    "\n",
    "# Write data to file\n",
    "zip_geo.to_file(join(fr, 'ref', 'zipcodes.gpkg'),\n",
    "                driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cca8026f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:40:23.199234Z",
     "start_time": "2023-04-12T13:40:22.290814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Municipalities for Camden (useful for clipping other data to GC)\n",
    "url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/CamdenCountyMunicipalLayer/FeatureServer/0/query?f=geojson&where=(NAMELSAD%20IN%20(%27Gloucester%20City%20city%27))&outFields=*\"\n",
    "\n",
    "# Data is in epsg: 4326\n",
    "# Metadata suggests 26918 but\n",
    "# local download shows epsg is 4326\n",
    "EPSG = '4326'\n",
    "\n",
    "# GET Request\n",
    "mun_get = requests.get(url)\n",
    "\n",
    "# No loop needed, just the one municipality\n",
    "temp = mun_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "mun_geo = gpd.GeoDataFrame(temp_df,\n",
    "                           crs=EPSG,\n",
    "                           geometry=temp_geo)\n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "mun_geo = mun_geo.drop(columns=drop_col)\n",
    "\n",
    "# Reproject to 3424\n",
    "OUT_EPSG = '3424'\n",
    "mun_geo = mun_geo.to_crs(epsg=OUT_EPSG)\n",
    "\n",
    "# Write data to file\n",
    "mun_geo.to_file(join(fr, 'ref', 'city_clip.gpkg'),\n",
    "                driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7fd117a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T23:16:17.778971Z",
     "start_time": "2023-04-11T23:16:17.151445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Land Uses\n",
    "url = \"https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/DVRPC_2010_Land_Use/FeatureServer/0/query?f=geojson&where=(Mun_Name%20IN%20(%27Gloucester%20City%27))&outFields=*\"\n",
    "# Data is in epsg: 3424\n",
    "EPSG = '3424'\n",
    "\n",
    "# GET Request\n",
    "lu_get = requests.get(url)\n",
    "\n",
    "# No loop needed because only ~100 records\n",
    "# Temp data frame\n",
    "temp = lu_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "lu_geo = gpd.GeoDataFrame(temp_df,\n",
    "                          crs=EPSG,\n",
    "                          geometry=temp_geo)\n",
    "   \n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "lu_geo = lu_geo.drop(columns=drop_col)\n",
    "\n",
    "# Write data to file\n",
    "lu_geo.to_file(join(fr, 'exposure', 'landuse.gpkg'),\n",
    "               driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41efff2",
   "metadata": {},
   "source": [
    "# Download Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5399841d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T12:47:23.236447Z",
     "start_time": "2023-04-12T12:47:21.353241Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOAA SOVI\n",
    "url = 'https://coast.noaa.gov/htdata/SocioEconomic/SoVI2010/SoVI_2010_NJ.zip'\n",
    "save_path = join(fr, 'vulnerability', 'social', 'noaa.zip')\n",
    "\n",
    "# Request and writing zip from here\n",
    "# https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
    "chunk_size = 128\n",
    "r = requests.get(url, stream=True)\n",
    "with open(save_path, 'wb') as fd:\n",
    "    for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "        fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89b9b0ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:19:47.681132Z",
     "start_time": "2023-04-12T14:19:46.784987Z"
    }
   },
   "outputs": [],
   "source": [
    "# NJ env. burd communities\n",
    "url = \"https://services1.arcgis.com/QWdNfRs7lkPq4g4Q/ArcGIS/rest/services/Overburdened_Communities_2020_Hosted/FeatureServer/0/query?where=NAME%3D%27Gloucester+City%27&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&relationParam=&returnGeodetic=false&outFields=*&returnGeometry=true&returnCentroid=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&defaultSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token=\"\n",
    "# Data is supposed to be in epsg: 3424\n",
    "# metadata says so\n",
    "# but it's in epsg 4326\n",
    "EPSG = '4326'\n",
    "\n",
    "# GET Request\n",
    "ob_get = requests.get(url)\n",
    "\n",
    "# No loop needed because only ~100 records\n",
    "# Temp data frame\n",
    "temp = ob_get.json()['features']\n",
    "temp_df = pd.json_normalize(temp)\n",
    "temp_geo = [shape(i['geometry']) for i in temp]\n",
    "\n",
    "# Final df\n",
    "ob_geo = gpd.GeoDataFrame(temp_df,\n",
    "                          crs=EPSG,\n",
    "                          geometry=temp_geo)\n",
    "   \n",
    "\n",
    "# Drop type, id, geometry.type, geometry.coordinates\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "ob_geo = ob_geo.drop(columns=drop_col)\n",
    "\n",
    "# Reproject to 3424\n",
    "OUT_EPSG = '3424'\n",
    "ob_geo = ob_geo.to_crs(epsg=OUT_EPSG)\n",
    "\n",
    "# Write data to file\n",
    "ob_geo.to_file(join(fr, 'vulnerability', 'social', 'overburdened.gpkg'),\n",
    "               driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53c5e72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T19:54:05.847566Z",
     "start_time": "2023-04-11T19:54:05.619626Z"
    }
   },
   "outputs": [],
   "source": [
    "# CEJST (download later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff58608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gc_elev",
   "language": "python",
   "name": "gc_elev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

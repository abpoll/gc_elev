{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9630f49e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:05:28.284881Z",
     "start_time": "2023-04-12T13:05:28.051787Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff1249c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:05:35.428058Z",
     "start_time": "2023-04-12T13:05:31.029360Z"
    }
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import glob\n",
    "from os.path import join\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio \n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec570f2",
   "metadata": {},
   "source": [
    "# Move .zip Directories to Interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475958ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:05:48.339664Z",
     "start_time": "2023-04-12T13:05:47.806043Z"
    }
   },
   "outputs": [],
   "source": [
    "# It could make sense to have a lib/ style directory\n",
    "# like PLACES has for common functionality\n",
    "# and this code block would be useful there for getting\n",
    "# a fr() path\n",
    "\n",
    "# Get the absolute path to the precal_hazard directory\n",
    "# Which is two directories above notebooks/exploration/\n",
    "abs_dir = os.path.abspath(Path(os.getcwd()).parents[1])\n",
    "# Get raw data directory\n",
    "fr = join(abs_dir, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "fi = join(abs_dir, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "fp = join(abs_dir, 'data', 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd79409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:06:24.774116Z",
     "start_time": "2023-04-12T13:06:24.046940Z"
    }
   },
   "outputs": [],
   "source": [
    "# For each .zip directory in fr\n",
    "# Create needed subdirectories in interim/\n",
    "# Unzip in the appropriate interim/ subdirectory\n",
    "\n",
    "for path in Path(fr).rglob('*.zip'):\n",
    "    # Avoid hidden files and files in directories\n",
    "    if path.name[0] != '.':\n",
    "        # Get root for the directory this .zip file is in\n",
    "        zip_root = path.relative_to(fr).parents[0]\n",
    "\n",
    "        # Get path to interim/zip_root\n",
    "        zip_to_path = join(fi, zip_root)\n",
    "\n",
    "        # Make directory, including parents\n",
    "        # No need to check if directory exists bc\n",
    "        # it is only created when this script is run\n",
    "        Path(zip_to_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Unzip to zip_to_path\n",
    "        with ZipFile(path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(zip_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b7f76",
   "metadata": {},
   "source": [
    "# Clip Raw Data to Location Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fc4058e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:41:01.870000Z",
     "start_time": "2023-04-12T13:41:00.983027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reference the GC clip file\n",
    "boundary_filep = join(fr, 'ref', 'city_clip.gpkg')\n",
    "# Read boundary\n",
    "boundary = gpd.read_file(boundary_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff78f7",
   "metadata": {},
   "source": [
    "## NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516e1af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:18:28.429960Z",
     "start_time": "2023-04-12T13:18:26.142459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read full NSI from all the counties\n",
    "nsi_filep = join(fr, 'exposure', 'nsi.pqt')\n",
    "# Read and reset index\n",
    "nsi_full = pd.read_parquet(nsi_filep).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37c6ac6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:20:25.830422Z",
     "start_time": "2023-04-12T13:20:24.866682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_full['properties.x'],\n",
    "                            nsi_full['properties.y'])\n",
    "# The NSI CRS is EPSG 4326\n",
    "nsi_gdf_f = gpd.GeoDataFrame(nsi_full, geometry=geometry,\n",
    "                             crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "220b3bfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:41:24.106604Z",
     "start_time": "2023-04-12T13:41:23.431499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project nsi_gdf_f coordinates to EPSG 3424 so that they\n",
    "# match the boundary CRS\n",
    "nsi_gdf_f = nsi_gdf_f.to_crs(boundary.crs)\n",
    "\n",
    "# Use spatial join to get nsi locations within location boundary\n",
    "nsi_gdf = gpd.sjoin(nsi_gdf_f, boundary[['geometry']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "285fbf00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:43:33.960478Z",
     "start_time": "2023-04-12T13:43:33.518472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates', 'index_right']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d564cf8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T13:43:42.182218Z",
     "start_time": "2023-04-12T13:43:39.808056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the NSI data to interim\n",
    "int_exp_filep = join(fi, 'exposure')\n",
    "Path(int_exp_filep).mkdir(parents=True, exist_ok=True)\n",
    "nsi_gdf.to_file(join(int_exp_filep, 'nsi.gpkg'), driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5633d",
   "metadata": {},
   "source": [
    "## Camden Depth Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9765fb36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:01:52.069442Z",
     "start_time": "2023-04-12T14:01:51.253325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Depth grid reference\n",
    "dg_filep = join(fr, 'hazard', 'camden_depthgrid', 'cst_dpth01pct.tif')\n",
    "\n",
    "# Reprojected temp file\n",
    "# Ensure directory exists\n",
    "dg_reproj_dir = join(fi, 'hazard', 'tmp')\n",
    "Path(dg_reproj_dir).mkdir(parents=True, exist_ok=True)\n",
    "dg_reproj_filep = join(dg_reproj_dir, 'cst_depth01_r.tif')\n",
    "\n",
    "# Reproj & clipped file\n",
    "# Goes in interim\n",
    "dg_out_filep = join(fi, 'hazard', 'cst_depth01.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73940b8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:02:22.566052Z",
     "start_time": "2023-04-12T14:02:20.642709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reproject depth grid to epsg: 3424\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "# Straight from the source\n",
    "# https://rasterio.readthedocs.io/en/stable/topics/reproject.html\n",
    "\n",
    "dst_crs = 'EPSG:3424'\n",
    "\n",
    "with rasterio.open(dg_filep) as src:\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "    kwargs = src.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': dst_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height\n",
    "    })\n",
    "\n",
    "    with rasterio.open(dg_reproj_filep, 'w', **kwargs) as dst:\n",
    "        for i in range(1, src.count + 1):\n",
    "            reproject(\n",
    "                source=rasterio.band(src, i),\n",
    "                destination=rasterio.band(dst, i),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=dst_crs,\n",
    "                resampling=Resampling.nearest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11b9998e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:04:38.270950Z",
     "start_time": "2023-04-12T14:04:37.724182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clip depth grid to GC boundaries\n",
    "import rasterio.mask\n",
    "# Straight from the source\n",
    "# https://rasterio.readthedocs.io/en/stable/topics/masking-by-shapefile.html\n",
    "\n",
    "# Replace shapes with boundary['geometry']\n",
    "\n",
    "with rasterio.open(dg_reproj_filep) as src:\n",
    "    out_image, out_transform = rasterio.mask.mask(src, boundary['geometry'],\n",
    "                                                  crop=True)\n",
    "    out_meta = src.meta\n",
    "out_meta.update({\"driver\": \"GTiff\",\n",
    "                 \"height\": out_image.shape[1],\n",
    "                 \"width\": out_image.shape[2],\n",
    "                 \"transform\": out_transform})\n",
    "\n",
    "with rasterio.open(dg_out_filep, \"w\", **out_meta) as dest:\n",
    "    dest.write(out_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ac556",
   "metadata": {},
   "source": [
    "## Camden Flood Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21c340a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:27:57.120210Z",
     "start_time": "2023-04-12T14:27:29.021471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load nfhl.gdb, flood zone layer\n",
    "nfhl_filep = join(fr, 'hazard', 'nfhl.gdb')\n",
    "nfhl = gpd.read_file(nfhl_filep, layer='S_Fld_Haz_Ar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4cd0ea9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:28:59.614948Z",
     "start_time": "2023-04-12T14:28:41.137554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reproject and clip\n",
    "nfhl_r = nfhl.to_crs(boundary.crs)\n",
    "nfhl_clip = gpd.clip(nfhl_r, boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f532060c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:34:10.009901Z",
     "start_time": "2023-04-12T14:34:09.150369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep FLD_ZONE, FLD_AR_ID, STATIC_BFE, geometry\n",
    "keep_cols = ['FLD_ZONE', 'FLD_AR_ID', 'STATIC_BFE', 'ZONE_SUBTY',\n",
    "             'geometry']\n",
    "nfhl_f = nfhl_clip[keep_cols]\n",
    "\n",
    "# Adjust .2 pct X zones to X_500\n",
    "nfhl_f.loc[nfhl_f['ZONE_SUBTY'] == '.2 PCT ANNUAL CHANCE FLOOD HAZARD',\n",
    "           'FLD_ZONE'] = nfhl_f['FLD_ZONE'] + '_500'\n",
    "\n",
    "# Update column names\n",
    "# Lower case\n",
    "nfhl_f.columns = [x.lower() for x in nfhl_f.columns]\n",
    "\n",
    "# Drop ZONE_SUBTY\n",
    "nfhl_f = nfhl_f.drop(columns=['zone_subty'])\n",
    "\n",
    "# Write file\n",
    "nfhl_out_filep = join(fi, 'hazard', 'floodzones.gpkg')\n",
    "nfhl_f.to_file(nfhl_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3379d",
   "metadata": {},
   "source": [
    "## CE JST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f31c98c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:55:00.445499Z",
     "start_time": "2023-04-12T14:54:37.521353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepath\n",
    "ce_filep = join(fr, 'vulnerability', 'social', 'cejst', 'usa.shp')\n",
    "# Read file\n",
    "ce_geo = gpd.read_file(ce_filep)\n",
    "# Subset to camden county\n",
    "ce_camden = ce_geo[(ce_geo['SF'] == 'New Jersey') &\n",
    "                   (ce_geo['CF'] == 'Camden County')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "62155b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T15:10:35.143487Z",
     "start_time": "2023-04-12T15:10:34.025264Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reproject\n",
    "ce_camden = ce_camden.to_crs(boundary.crs)\n",
    "\n",
    "# Clip\n",
    "ce_gc = gpd.clip(ce_camden, boundary)\n",
    "\n",
    "# Write file\n",
    "ce_gc_out_filep = join(fi, 'vulnerability', 'cejst.gpkg')\n",
    "ce_gc.to_file(ce_gc_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace13ccf",
   "metadata": {},
   "source": [
    "## Ref files (bg, tract, zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f1743b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T15:27:56.052414Z",
     "start_time": "2023-04-12T15:27:52.979434Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of raw files\n",
    "raw_filep = ['blockgroups/tl_2021_34_bg.shp',\n",
    "             'tracts.gpkg', 'zipcodes.gpkg']\n",
    "\n",
    "# List of output files\n",
    "out_filep = ['bg.gpkg', 'tracts.gpkg', 'zips.gpkg']\n",
    "\n",
    "# Input file directory\n",
    "filedir_in = join(fr, 'ref')\n",
    "\n",
    "# Output file directory\n",
    "filedir_out = join(fp, 'ref')\n",
    "\n",
    "# Loop through files\n",
    "# Reproject each (if needed)\n",
    "# Clip and write\n",
    "for i, fp in enumerate(raw_filep):\n",
    "    input_filep = join(filedir_in, fp)\n",
    "    output_filep = join(filedir_out, out_filep[i])\n",
    "    \n",
    "    ref = gpd.read_file(input_filep)\n",
    "    \n",
    "    if ref.crs != boundary.crs:\n",
    "        ref = ref.to_crs(boundary.crs)\n",
    "    \n",
    "    ref_clip = gpd.clip(ref, boundary)\n",
    "    \n",
    "    ref_clip.to_file(output_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be84326",
   "metadata": {},
   "source": [
    "# Get Processed Structure Inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8281164e",
   "metadata": {},
   "source": [
    "## Link NSI and Parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0361b4ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T15:27:04.866194Z",
     "start_time": "2023-03-14T15:27:04.825214Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, process parcel data and write out\n",
    "# Next, link relevant parcel data with NSI \n",
    "# Merge reference files (bg, tract, zip) with NSI points\n",
    "# Link NSI data to lmi, cejst, etc.\n",
    "# Link NSI data to bfe, depth grids\n",
    "\n",
    "# There's filtering/cleaning\n",
    "# There's a series of attribute joins\n",
    "# There's a series of spatial joins\n",
    "\n",
    "# Want to write out all the id/var combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1285b394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T15:27:05.387672Z",
     "start_time": "2023-03-14T15:27:05.345384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.025',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0175',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.1',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.01',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.06',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0275',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.08',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.02',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.035',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.045',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.05',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0375',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0125',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.07',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0225',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.04',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.03',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.09',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0325',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.015']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haz_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "056810c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T15:38:57.320939Z",
     "start_time": "2023-03-14T15:38:46.601696Z"
    }
   },
   "outputs": [],
   "source": [
    "# metadata from .txt file\n",
    "# ncols         2276\n",
    "# nrows         1564\n",
    "# xllcorner     -75.4159722219145\n",
    "# yllcorner     40.0026388902255\n",
    "# cellsize      9.2592593e-05\n",
    "# NODATA_value  -9999\n",
    "\n",
    "# Prepare directory for writing out\n",
    "HAZ_OUT_DIR = join(fp, 'hazard', 'depths')\n",
    "Path(HAZ_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Constant for peak_flood depth.txt\n",
    "DEPTH_FILEP = \"peak_flood_depth.txt\"\n",
    "\n",
    "# Constants from metadata\n",
    "EPSG = 4269\n",
    "NODATA = -9999\n",
    "NROWS = 1564\n",
    "RES = 9.2592593e-05\n",
    "XLL = -75.4159722219145\n",
    "YLL = 40.0026388902255\n",
    "# Get the CRS \n",
    "crs = CRS.from_user_input(EPSG)\n",
    "\n",
    "# Calculate the y coordinate for the origin\n",
    "# by adding the cell resolution * raster height (#rows)\n",
    "# to the y lower left coordinate\n",
    "# xll and yll mean x lower left and y lower left\n",
    "YUL = YLL + RES*NROWS\n",
    "\n",
    "# Get transform\n",
    "trans = rasterio.transform.from_origin(XLL,\n",
    "                                       YUL,\n",
    "                                       RES, RES)\n",
    "\n",
    "\n",
    "# Use \"output*\" wildcard in glob to find\n",
    "# all subdirectories in interim/hazard/\n",
    "# that have peak_flood_depth.txt files in them\n",
    "# Use numpy to load text, then reshape the data\n",
    "# Use rasterio to provide the CRS\n",
    "# The datum is NAD83, EPSG: 4269\n",
    "haz_filedir = join(fi, 'hazard')\n",
    "haz_dirs = glob.glob(join(haz_filedir, \"output*\"))\n",
    "\n",
    "# Loop through directories in haz_dirs\n",
    "# Convert each peak_flood_depth.txt\n",
    "# into a raster\n",
    "# Use the wildcard component\n",
    "# after \"output\" as the index\n",
    "for hd in haz_dirs:\n",
    "    haz_filep = join(hd, DEPTH_FILEP)\n",
    "    # Suffix correspondes to parameter\n",
    "    # values used to generate depths\n",
    "    # Useful to keep this in the processing/writing\n",
    "    # of files\n",
    "    file_suf = hd.split(\"output\")[1]\n",
    "\n",
    "    # Load peak_flood_depth.txt\n",
    "    fld_depths_in = np.loadtxt(haz_filep, skiprows=6)\n",
    "\n",
    "    # Unique filename for each depth grid\n",
    "    # Join haz_out_dir defined as a constant above\n",
    "    # with peak_fld_depth, the file_suf, and .tif\n",
    "    filename = 'peak_fld_depth_' + file_suf + '.tif'\n",
    "    haz_out_filep = join(HAZ_OUT_DIR, filename)\n",
    "\n",
    "    # Write raster \n",
    "    haz_r = rasterio.open(haz_out_filep, 'w', driver='GTiff',\n",
    "                          height=fld_depths_in.shape[0],\n",
    "                          width=fld_depths_in.shape[1],\n",
    "                          count=1, dtype=str(fld_depths_in.dtype),\n",
    "                          crs=crs, nodata=NODATA, transform=trans)\n",
    "\n",
    "    haz_r.write(fld_depths_in, 1)\n",
    "    haz_r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8acd66",
   "metadata": {},
   "source": [
    "## Link depths to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3bb4393",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T16:24:37.122318Z",
     "start_time": "2023-03-14T16:24:37.079378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peak_fld_depth_0.0375.tif',\n",
       " 'peak_fld_depth_0.0125.tif',\n",
       " 'peak_fld_depth_0.035.tif',\n",
       " 'peak_fld_depth_0.05.tif',\n",
       " 'peak_fld_depth_0.02.tif',\n",
       " 'peak_fld_depth_0.0275.tif',\n",
       " 'peak_fld_depth_0.025.tif',\n",
       " 'peak_fld_depth_0.0225.tif',\n",
       " 'peak_fld_depth_0.0175.tif',\n",
       " 'peak_fld_depth_0.0325.tif',\n",
       " 'peak_fld_depth_0.03.tif',\n",
       " 'peak_fld_depth_0.04.tif',\n",
       " 'peak_fld_depth_0.1.tif',\n",
       " 'peak_fld_depth_0.07.tif',\n",
       " 'peak_fld_depth_0.045.tif',\n",
       " 'peak_fld_depth_0.09.tif',\n",
       " 'peak_fld_depth_0.08.tif',\n",
       " 'peak_fld_depth_0.01.tif',\n",
       " 'peak_fld_depth_0.06.tif',\n",
       " 'peak_fld_depth_0.015.tif']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b26f65c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T16:25:44.719812Z",
     "start_time": "2023-03-14T16:25:09.109307Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in NSI data\n",
    "INT_EXP_FILEP = join(fi, 'exposure')\n",
    "nsi_gdf = gpd.read_file(join(INT_EXP_FILEP, 'nsi.gpkg'))\n",
    "\n",
    "# Get coordinate list\n",
    "coord_list = [(x, y) for x, y in\n",
    "              zip(nsi_gdf['geometry'].x,\n",
    "                  nsi_gdf['geometry'].y)]\n",
    "\n",
    "# List of depth series\n",
    "depth_list = []\n",
    "\n",
    "# For each depth raster, link up unique property\n",
    "# coordinates with the corresponding depth values\n",
    "# Write out file of coord/id index & depth_suf columns\n",
    "depth_filenames = os.listdir(HAZ_OUT_DIR)\n",
    "\n",
    "for d_fn in depth_filenames:\n",
    "    # Filepath and load\n",
    "    d_grid_fp = join(HAZ_OUT_DIR, d_fn)\n",
    "    # Open the depth raster in read mode\n",
    "    d_grid = rasterio.open(d_grid_fp)\n",
    "\n",
    "    # Get the suffix\n",
    "    # First, get the pre .tif str component\n",
    "    filepre = d_fn.split('.tif')[0]\n",
    "    # Then get last element splitting on \"_\"\n",
    "    d_suf = filepre.split('_')[-1]\n",
    "\n",
    "    # Sample points from the raster based on nsi coordinates\n",
    "    # Get sampled values from pixels\n",
    "    sampled_depths = [x[0] for x in d_grid.sample(coord_list)]\n",
    "\n",
    "    # Store as series with name\n",
    "    # Index by fd_id\n",
    "    # depth_d_suf\n",
    "    depth_series = pd.Series(sampled_depths,\n",
    "                             index=nsi_gdf['fd_id'],\n",
    "                             name='depth_' + d_suf)\n",
    "\n",
    "    # Convert depth to ft\n",
    "    depth_series = depth_series * 3.281\n",
    "\n",
    "    # Store in list\n",
    "    depth_list.append(depth_series)\n",
    "    \n",
    "# Concat into dataframe\n",
    "depths = pd.concat(depth_list, axis=1)\n",
    "\n",
    "# Write data frame to file\n",
    "# Exposure/depths links depths to properties\n",
    "EXP_OUT_DIR = join(fp, 'exposure')\n",
    "Path(EXP_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "DEPTHS_OUT_FILEP = join(EXP_OUT_DIR, 'depths.pqt')\n",
    "# fd_id is index, so set index=True\n",
    "depths.to_parquet(DEPTHS_OUT_FILEP,\n",
    "                  index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40920f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:18:14.237090Z",
     "start_time": "2023-02-05T17:18:13.531318Z"
    }
   },
   "source": [
    "## Subset to residential structures and write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16d958cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:09:24.255096Z",
     "start_time": "2023-03-14T20:09:13.068019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['st_damcat'] == 'RES']\n",
    "\n",
    "# TODO: Need to update occtype variable to OPEN or ENC\n",
    "# when pile or pier found_type exists, but not\n",
    "# relevant for this first case study so avoiding the code\n",
    "\n",
    "# Write out to processed/exposure/\n",
    "EXP_OUT_FILEP = join(EXP_OUT_DIR, 'nsi_res.gpkg')\n",
    "nsi_res.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4cda8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T17:01:13.595886Z",
     "start_time": "2023-02-06T17:01:13.146084Z"
    }
   },
   "source": [
    "# Process depth damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c010ecb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T17:07:00.593165Z",
     "start_time": "2023-02-06T17:07:00.065186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepath to NACCS depth damage functions\n",
    "vul_dir = join(fr, 'vulnerability')\n",
    "# Read ddfs\n",
    "naccs = pd.read_csv(join(vul_dir, 'naccs_ddfs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4afe6ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T17:31:49.308619Z",
     "start_time": "2023-02-06T17:31:49.244649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need to write file in tidy format\n",
    "\n",
    "# Drop Description and Source columns\n",
    "# Melt on occupancy damage category\n",
    "# Each depth is associated with a percent damage\n",
    "dropcols = ['Description', 'Source']\n",
    "idvars = ['Occupancy', 'DamageCategory']\n",
    "naccs_melt = naccs.drop(columns=dropcols).melt(id_vars=idvars,\n",
    "                                               var_name='depth_str',\n",
    "                                               value_name='pctdam')\n",
    "\n",
    "# Need to convert depth_ft into a number\n",
    "# Replace ft with empty character\n",
    "# If string ends with m, make negative number\n",
    "# Else, make positive number\n",
    "naccs_melt['depth_str'] = naccs_melt['depth_str'].str.replace('ft', '')\n",
    "negdepth = naccs_melt.loc[naccs_melt['depth_str'].str[-1] == \n",
    "                          'm']['depth_str'].str[:-1].astype(float)*-1\n",
    "posdepth = naccs_melt.loc[naccs_melt['depth_str'].str[-1] != \n",
    "                          'm']['depth_str'].astype(float)\n",
    "\n",
    "naccs_melt.loc[naccs_melt['depth_str'].str[-1] == 'm',\n",
    "               'depth_ft'] = negdepth\n",
    "naccs_melt.loc[naccs_melt['depth_str'].str[-1] != 'm',\n",
    "               'depth_ft'] = posdepth\n",
    "\n",
    "# Divide pctdam by 100\n",
    "naccs_melt['reldam'] = naccs_melt['pctdam']/100\n",
    "\n",
    "# Delete depth_str and pctdam and standardize\n",
    "# column names\n",
    "dropcols = ['depth_str', 'pctdam']\n",
    "newcols = ['occtype', 'damcat', 'depth_ft', 'reldam']\n",
    "naccs_melt = naccs_melt.drop(columns=dropcols)\n",
    "naccs_melt.columns = newcols\n",
    "\n",
    "# Write out to processed/vulnerability/\n",
    "vuln_out_dir = join(fp, 'vulnerability')\n",
    "Path(vuln_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "vuln_out_filep = join(vuln_out_dir, 'naccs_ddfs.csv')\n",
    "naccs_melt.to_csv(vuln_out_filep, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icom_risk",
   "language": "python",
   "name": "icom_risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9630f49e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T14:51:28.950761Z",
     "start_time": "2023-03-14T14:51:28.758278Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff1249c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T14:51:49.993222Z",
     "start_time": "2023-03-14T14:51:29.409324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2738410/399597061.py:7: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import glob\n",
    "from os.path import join\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio \n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec570f2",
   "metadata": {},
   "source": [
    "# Move .zip Directories to Interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475958ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T14:51:50.140767Z",
     "start_time": "2023-03-14T14:51:49.999058Z"
    }
   },
   "outputs": [],
   "source": [
    "# It could make sense to have a lib/ style directory\n",
    "# like PLACES has for common functionality\n",
    "# and this code block would be useful there for getting\n",
    "# a fr() path\n",
    "\n",
    "# Get the absolute path to the precal_hazard directory\n",
    "# Which is two directories above notebooks/exploration/\n",
    "abs_dir = os.path.abspath(Path(os.getcwd()).parents[1])\n",
    "# Get raw data directory\n",
    "fr = join(abs_dir, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "fi = join(abs_dir, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "fp = join(abs_dir, 'data', 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd79409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T15:22:37.944755Z",
     "start_time": "2023-03-14T15:19:48.074417Z"
    }
   },
   "outputs": [],
   "source": [
    "# For each .zip directory in fr\n",
    "# Create needed subdirectories in interim/\n",
    "# Unzip in the appropriate interim/ subdirectory\n",
    "\n",
    "for path in Path(fr).rglob('*.zip'):\n",
    "    # Avoid hidden files and files in directories\n",
    "    if path.name[0] != '.':\n",
    "        # Get root for the directory this .zip file is in\n",
    "        zip_root = path.relative_to(fr).parents[0]\n",
    "\n",
    "        # Get path to interim/zip_root\n",
    "        zip_to_path = join(fi, zip_root)\n",
    "\n",
    "        # Make directory, including parents\n",
    "        # No need to check if directory exists bc\n",
    "        # it is only created when this script is run\n",
    "        Path(zip_to_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Unzip to zip_to_path\n",
    "        with ZipFile(path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(zip_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b7f76",
   "metadata": {},
   "source": [
    "# Clip Raw Data to Location Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fc4058e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T18:40:14.050114Z",
     "start_time": "2023-02-01T18:40:13.616430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepath to location boundary\n",
    "boundary_filep = join(fi, 'hazard', '020402031007.shp')\n",
    "# Read boundary\n",
    "boundary = gpd.read_file(boundary_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff78f7",
   "metadata": {},
   "source": [
    "## NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "516e1af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T18:59:28.292880Z",
     "start_time": "2023-02-01T18:59:25.327571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read full NSI from all the counties\n",
    "nsi_filep = join(fr, 'exposure', 'nsi.pqt')\n",
    "# Read and reset index\n",
    "nsi_full = pd.read_parquet(nsi_filep).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37c6ac6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T18:59:29.147495Z",
     "start_time": "2023-02-01T18:59:28.296551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_full['properties.x'],\n",
    "                            nsi_full['properties.y'])\n",
    "# The NSI CRS is EPSG 3426\n",
    "nsi_gdf_f = gpd.GeoDataFrame(nsi_full, geometry=geometry,\n",
    "                             crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "220b3bfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T18:59:31.956301Z",
     "start_time": "2023-02-01T18:59:29.150347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project nsi_gdf_f coordinates to EPSG 4269 so that they\n",
    "# match the boundary CRS\n",
    "nsi_gdf_f = nsi_gdf_f.to_crs(boundary.crs)\n",
    "\n",
    "# Use spatial join to get nsi locations within location boundary\n",
    "nsi_gdf = gpd.sjoin(nsi_gdf_f, boundary[['geometry']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "285fbf00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T18:59:31.981026Z",
     "start_time": "2023-02-01T18:59:31.958448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates', 'index_right']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d564cf8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T18:59:52.297087Z",
     "start_time": "2023-02-01T18:59:39.706788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the NSI data to interim\n",
    "int_exp_filep = join(fi, 'exposure')\n",
    "Path(int_exp_filep).mkdir(parents=True, exist_ok=True)\n",
    "nsi_gdf.to_file(join(int_exp_filep, 'nsi.gpkg'), driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be84326",
   "metadata": {},
   "source": [
    "# Process Hazard Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8281164e",
   "metadata": {},
   "source": [
    "## Get input hazard data into spatially referenced format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0361b4ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T15:27:04.866194Z",
     "start_time": "2023-03-14T15:27:04.825214Z"
    }
   },
   "outputs": [],
   "source": [
    "haz_filedir = join(fi, 'hazard')\n",
    "haz_dirs = glob.glob(join(haz_filedir, \"output*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1285b394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T15:27:05.387672Z",
     "start_time": "2023-03-14T15:27:05.345384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.025',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0175',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.1',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.01',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.06',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0275',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.08',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.02',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.035',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.045',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.05',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0375',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0125',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.07',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0225',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.04',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.03',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.09',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.0325',\n",
       " '/jumbo/keller-lab/projects/icom/precal/precal_hazard/data/interim/hazard/output0.015']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haz_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "056810c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T15:38:57.320939Z",
     "start_time": "2023-03-14T15:38:46.601696Z"
    }
   },
   "outputs": [],
   "source": [
    "# metadata from .txt file\n",
    "# ncols         2276\n",
    "# nrows         1564\n",
    "# xllcorner     -75.4159722219145\n",
    "# yllcorner     40.0026388902255\n",
    "# cellsize      9.2592593e-05\n",
    "# NODATA_value  -9999\n",
    "\n",
    "# Prepare directory for writing out\n",
    "HAZ_OUT_DIR = join(fp, 'hazard', 'depths')\n",
    "Path(HAZ_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Constant for peak_flood depth.txt\n",
    "DEPTH_FILEP = \"peak_flood_depth.txt\"\n",
    "\n",
    "# Constants from metadata\n",
    "EPSG = 4269\n",
    "NODATA = -9999\n",
    "NROWS = 1564\n",
    "RES = 9.2592593e-05\n",
    "XLL = -75.4159722219145\n",
    "YLL = 40.0026388902255\n",
    "# Get the CRS \n",
    "crs = CRS.from_user_input(EPSG)\n",
    "\n",
    "# Calculate the y coordinate for the origin\n",
    "# by adding the cell resolution * raster height (#rows)\n",
    "# to the y lower left coordinate\n",
    "# xll and yll mean x lower left and y lower left\n",
    "YUL = YLL + RES*NROWS\n",
    "\n",
    "# Get transform\n",
    "trans = rasterio.transform.from_origin(XLL,\n",
    "                                       YUL,\n",
    "                                       RES, RES)\n",
    "\n",
    "\n",
    "# Use \"output*\" wildcard in glob to find\n",
    "# all subdirectories in interim/hazard/\n",
    "# that have peak_flood_depth.txt files in them\n",
    "# Use numpy to load text, then reshape the data\n",
    "# Use rasterio to provide the CRS\n",
    "# The datum is NAD83, EPSG: 4269\n",
    "haz_filedir = join(fi, 'hazard')\n",
    "haz_dirs = glob.glob(join(haz_filedir, \"output*\"))\n",
    "\n",
    "# Loop through directories in haz_dirs\n",
    "# Convert each peak_flood_depth.txt\n",
    "# into a raster\n",
    "# Use the wildcard component\n",
    "# after \"output\" as the index\n",
    "for hd in haz_dirs:\n",
    "    haz_filep = join(hd, DEPTH_FILEP)\n",
    "    # Suffix correspondes to parameter\n",
    "    # values used to generate depths\n",
    "    # Useful to keep this in the processing/writing\n",
    "    # of files\n",
    "    file_suf = hd.split(\"output\")[1]\n",
    "\n",
    "    # Load peak_flood_depth.txt\n",
    "    fld_depths_in = np.loadtxt(haz_filep, skiprows=6)\n",
    "\n",
    "    # Unique filename for each depth grid\n",
    "    # Join haz_out_dir defined as a constant above\n",
    "    # with peak_fld_depth, the file_suf, and .tif\n",
    "    filename = 'peak_fld_depth_' + file_suf + '.tif'\n",
    "    haz_out_filep = join(HAZ_OUT_DIR, filename)\n",
    "\n",
    "    # Write raster \n",
    "    haz_r = rasterio.open(haz_out_filep, 'w', driver='GTiff',\n",
    "                          height=fld_depths_in.shape[0],\n",
    "                          width=fld_depths_in.shape[1],\n",
    "                          count=1, dtype=str(fld_depths_in.dtype),\n",
    "                          crs=crs, nodata=NODATA, transform=trans)\n",
    "\n",
    "    haz_r.write(fld_depths_in, 1)\n",
    "    haz_r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8acd66",
   "metadata": {},
   "source": [
    "## Link depths to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3bb4393",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T16:24:37.122318Z",
     "start_time": "2023-03-14T16:24:37.079378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peak_fld_depth_0.0375.tif',\n",
       " 'peak_fld_depth_0.0125.tif',\n",
       " 'peak_fld_depth_0.035.tif',\n",
       " 'peak_fld_depth_0.05.tif',\n",
       " 'peak_fld_depth_0.02.tif',\n",
       " 'peak_fld_depth_0.0275.tif',\n",
       " 'peak_fld_depth_0.025.tif',\n",
       " 'peak_fld_depth_0.0225.tif',\n",
       " 'peak_fld_depth_0.0175.tif',\n",
       " 'peak_fld_depth_0.0325.tif',\n",
       " 'peak_fld_depth_0.03.tif',\n",
       " 'peak_fld_depth_0.04.tif',\n",
       " 'peak_fld_depth_0.1.tif',\n",
       " 'peak_fld_depth_0.07.tif',\n",
       " 'peak_fld_depth_0.045.tif',\n",
       " 'peak_fld_depth_0.09.tif',\n",
       " 'peak_fld_depth_0.08.tif',\n",
       " 'peak_fld_depth_0.01.tif',\n",
       " 'peak_fld_depth_0.06.tif',\n",
       " 'peak_fld_depth_0.015.tif']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b26f65c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T16:25:44.719812Z",
     "start_time": "2023-03-14T16:25:09.109307Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in NSI data\n",
    "INT_EXP_FILEP = join(fi, 'exposure')\n",
    "nsi_gdf = gpd.read_file(join(INT_EXP_FILEP, 'nsi.gpkg'))\n",
    "\n",
    "# Get coordinate list\n",
    "coord_list = [(x, y) for x, y in\n",
    "              zip(nsi_gdf['geometry'].x,\n",
    "                  nsi_gdf['geometry'].y)]\n",
    "\n",
    "# List of depth series\n",
    "depth_list = []\n",
    "\n",
    "# For each depth raster, link up unique property\n",
    "# coordinates with the corresponding depth values\n",
    "# Write out file of coord/id index & depth_suf columns\n",
    "depth_filenames = os.listdir(HAZ_OUT_DIR)\n",
    "\n",
    "for d_fn in depth_filenames:\n",
    "    # Filepath and load\n",
    "    d_grid_fp = join(HAZ_OUT_DIR, d_fn)\n",
    "    # Open the depth raster in read mode\n",
    "    d_grid = rasterio.open(d_grid_fp)\n",
    "\n",
    "    # Get the suffix\n",
    "    # First, get the pre .tif str component\n",
    "    filepre = d_fn.split('.tif')[0]\n",
    "    # Then get last element splitting on \"_\"\n",
    "    d_suf = filepre.split('_')[-1]\n",
    "\n",
    "    # Sample points from the raster based on nsi coordinates\n",
    "    # Get sampled values from pixels\n",
    "    sampled_depths = [x[0] for x in d_grid.sample(coord_list)]\n",
    "\n",
    "    # Store as series with name\n",
    "    # Index by fd_id\n",
    "    # depth_d_suf\n",
    "    depth_series = pd.Series(sampled_depths,\n",
    "                             index=nsi_gdf['fd_id'],\n",
    "                             name='depth_' + d_suf)\n",
    "\n",
    "    # Convert depth to ft\n",
    "    depth_series = depth_series * 3.281\n",
    "\n",
    "    # Store in list\n",
    "    depth_list.append(depth_series)\n",
    "    \n",
    "# Concat into dataframe\n",
    "depths = pd.concat(depth_list, axis=1)\n",
    "\n",
    "# Write data frame to file\n",
    "# Exposure/depths links depths to properties\n",
    "EXP_OUT_DIR = join(fp, 'exposure')\n",
    "Path(EXP_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "DEPTHS_OUT_FILEP = join(EXP_OUT_DIR, 'depths.pqt')\n",
    "# fd_id is index, so set index=True\n",
    "depths.to_parquet(DEPTHS_OUT_FILEP,\n",
    "                  index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40920f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:18:14.237090Z",
     "start_time": "2023-02-05T17:18:13.531318Z"
    }
   },
   "source": [
    "## Subset to residential structures and write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16d958cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:09:24.255096Z",
     "start_time": "2023-03-14T20:09:13.068019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['st_damcat'] == 'RES']\n",
    "\n",
    "# TODO: Need to update occtype variable to OPEN or ENC\n",
    "# when pile or pier found_type exists, but not\n",
    "# relevant for this first case study so avoiding the code\n",
    "\n",
    "# Write out to processed/exposure/\n",
    "EXP_OUT_FILEP = join(EXP_OUT_DIR, 'nsi_res.gpkg')\n",
    "nsi_res.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4cda8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T17:01:13.595886Z",
     "start_time": "2023-02-06T17:01:13.146084Z"
    }
   },
   "source": [
    "# Process depth damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c010ecb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T17:07:00.593165Z",
     "start_time": "2023-02-06T17:07:00.065186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepath to NACCS depth damage functions\n",
    "vul_dir = join(fr, 'vulnerability')\n",
    "# Read ddfs\n",
    "naccs = pd.read_csv(join(vul_dir, 'naccs_ddfs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4afe6ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T17:31:49.308619Z",
     "start_time": "2023-02-06T17:31:49.244649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need to write file in tidy format\n",
    "\n",
    "# Drop Description and Source columns\n",
    "# Melt on occupancy damage category\n",
    "# Each depth is associated with a percent damage\n",
    "dropcols = ['Description', 'Source']\n",
    "idvars = ['Occupancy', 'DamageCategory']\n",
    "naccs_melt = naccs.drop(columns=dropcols).melt(id_vars=idvars,\n",
    "                                               var_name='depth_str',\n",
    "                                               value_name='pctdam')\n",
    "\n",
    "# Need to convert depth_ft into a number\n",
    "# Replace ft with empty character\n",
    "# If string ends with m, make negative number\n",
    "# Else, make positive number\n",
    "naccs_melt['depth_str'] = naccs_melt['depth_str'].str.replace('ft', '')\n",
    "negdepth = naccs_melt.loc[naccs_melt['depth_str'].str[-1] == \n",
    "                          'm']['depth_str'].str[:-1].astype(float)*-1\n",
    "posdepth = naccs_melt.loc[naccs_melt['depth_str'].str[-1] != \n",
    "                          'm']['depth_str'].astype(float)\n",
    "\n",
    "naccs_melt.loc[naccs_melt['depth_str'].str[-1] == 'm',\n",
    "               'depth_ft'] = negdepth\n",
    "naccs_melt.loc[naccs_melt['depth_str'].str[-1] != 'm',\n",
    "               'depth_ft'] = posdepth\n",
    "\n",
    "# Divide pctdam by 100\n",
    "naccs_melt['reldam'] = naccs_melt['pctdam']/100\n",
    "\n",
    "# Delete depth_str and pctdam and standardize\n",
    "# column names\n",
    "dropcols = ['depth_str', 'pctdam']\n",
    "newcols = ['occtype', 'damcat', 'depth_ft', 'reldam']\n",
    "naccs_melt = naccs_melt.drop(columns=dropcols)\n",
    "naccs_melt.columns = newcols\n",
    "\n",
    "# Write out to processed/vulnerability/\n",
    "vuln_out_dir = join(fp, 'vulnerability')\n",
    "Path(vuln_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "vuln_out_filep = join(vuln_out_dir, 'naccs_ddfs.csv')\n",
    "naccs_melt.to_csv(vuln_out_filep, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icom_risk",
   "language": "python",
   "name": "icom_risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

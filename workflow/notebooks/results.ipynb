{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3437bec-20a3-47a7-81c7-982313ea7336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T16:13:09.327491Z",
     "iopub.status.busy": "2024-01-03T16:13:09.326955Z",
     "iopub.status.idle": "2024-01-03T16:13:09.610478Z",
     "shell.execute_reply": "2024-01-03T16:13:09.608674Z",
     "shell.execute_reply.started": "2024-01-03T16:13:09.327442Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98978fcf-f72e-4a6e-87e6-4d7e2f100b47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T16:13:13.129318Z",
     "iopub.status.busy": "2024-01-03T16:13:13.128765Z",
     "iopub.status.idle": "2024-01-03T16:13:24.252759Z",
     "shell.execute_reply": "2024-01-03T16:13:24.250822Z",
     "shell.execute_reply.started": "2024-01-03T16:13:13.129268Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import glob\n",
    "from os.path import join\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio \n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio.plot\n",
    "import rasterio.mask\n",
    "import json\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d906166-743e-47df-89b4-1700d74567ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T16:13:24.255528Z",
     "iopub.status.busy": "2024-01-03T16:13:24.254865Z",
     "iopub.status.idle": "2024-01-03T16:13:24.311114Z",
     "shell.execute_reply": "2024-01-03T16:13:24.309292Z",
     "shell.execute_reply.started": "2024-01-03T16:13:24.255475Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd63f8-662b-44e7-b244-246aab80fb1e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32345773-b12d-44aa-932d-5cae71e49dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T16:16:23.097647Z",
     "iopub.status.busy": "2024-01-03T16:16:23.097067Z",
     "iopub.status.idle": "2024-01-03T16:16:27.730053Z",
     "shell.execute_reply": "2024-01-03T16:16:27.728936Z",
     "shell.execute_reply.started": "2024-01-03T16:16:23.097595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load geospatial data\n",
    "fz_geo = gpd.read_file(join(POL_DIR_I, FIPS, \"fld_zones.gpkg\"))\n",
    "clip_geo = gpd.read_file(join(REF_DIR_I, FIPS, \"clip.gpkg\"))\n",
    "tract_geo = gpd.read_file(join(REF_DIR_I, FIPS, \"tract.gpkg\"))\n",
    "bg_geo = gpd.read_file(join(REF_DIR_I, FIPS, \"bg.gpkg\"))\n",
    "nsi_geo = gpd.read_file(join(EXP_DIR_I, FIPS, \"nsi_sf.gpkg\"))\n",
    "\n",
    "# including vulnerability data\n",
    "lmi = gpd.read_file(join(VULN_DIR_I, 'social', FIPS, 'lmi.gpkg'))\n",
    "ovb = gpd.read_file(join(VULN_DIR_I, 'social', FIPS, 'ovb.gpkg'))\n",
    "cejst = gpd.read_file(join(VULN_DIR_I, 'social', FIPS, 'cejst.gpkg'))\n",
    "sovi = gpd.read_file(join(VULN_DIR_I, 'social', FIPS, 'sovi.gpkg'))\n",
    "# and the indicators dataframe to subset for maps\n",
    "c_ind = pd.read_parquet(join(VULN_DIR_I, 'social', FIPS, 'c_indicators.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccf0b3-0897-43fb-a2e2-f663e3a4d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the figures, we need the following data\n",
    "\n",
    "# The objective evaluations for each policy\n",
    "obj_filep = join(FO, 'pol_obj_vals.pqt')\n",
    "objs = pd.read_parquet(obj_filep)\n",
    "\n",
    "# The houses that are elevated for each policy\n",
    "elev_ids_filep = join(FO, 'pol_elev_ids.json')\n",
    "with open(elev_ids_filep, 'r') as fp:\n",
    "    elev_ids = json.load(fp)\n",
    "\n",
    "# The policies on the full pareto front\n",
    "pareto_f_filep = join(FO, 'pareto_full.pqt')\n",
    "# The pareto front for community based policies (may not need)\n",
    "pareto_c_filep = join(FO, 'pareto_community.pqt')\n",
    "\n",
    "pareto_f = pd.read_parquet(pareto_f_filep)\n",
    "pareto_c = pd.read_parquet(pareto_c_filep)\n",
    "\n",
    "# Metrics aggregated across SOWs at the household level\n",
    "ens_agg_filep = join(FO, 'ens_agg.pqt')\n",
    "ens_agg = pd.read_parquet(ens_agg_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06e291-5016-4575-bba2-030176cd45c4",
   "metadata": {},
   "source": [
    "# Main Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f3d79-01ba-4705-a935-398da53f787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1\n",
    "# Each row will correspond to a different \n",
    "# community prioritization definition\n",
    "# First column will be the maps with the boundaries highlighted, \n",
    "# and all the structures plotted with eal. \n",
    "# Second column will be the same but for rel_eal. \n",
    "# This figure totally sets the stage for issues \n",
    "# with using these boundaries and the sharp discontinuities \n",
    "# they create over space. It also raises the point of spatial \n",
    "# resolution (SOVI is tracts, not block group. So, while you \n",
    "# think you’re doing better because you are getting finer \n",
    "# resolution community characteristics, you actually can leave \n",
    "# out more homes with higher risk - even after taking out \n",
    "# the influence of structure value). \n",
    "\n",
    "# We need the vulnerability geodata & indicator df\n",
    "# We need the ens_agg dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80524bee-2308-4e34-b96c-b5523a031c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2\n",
    "# val_s vs. rel_eal plots for each community definition\n",
    "# marker color shading based on the rel_eal_reduction \n",
    "# from optimal elevation.  So we can have 2 columns again. \n",
    "# The left column is rel_eal y-axis and right column is eal y-axis \n",
    "# (or we can do this in rows to share the y-axis. Wider could \n",
    "# be better, we’ll see). Maybe we want val_s on y-axis, actually, \n",
    "# since we’ll be motivating our equity indicators from this. \n",
    "# We’ll show what gets prioritized for elevation\n",
    "# under the different community sorting rules and going by eal\n",
    "# versus normalized. This has the nice feature of showing some\n",
    "# unintended results you get with community rules\n",
    "# I think this is good motivation for wanting to \n",
    "# measure on the intensive & extensive margin to get at equity. \n",
    "\n",
    "# We need the indicator df linked to the ens_agg dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88356beb-2e74-4c9d-8f57-adf428362b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3\n",
    "# Want to show marginal distributions for the 2 equity metrics\n",
    "# and the npv for the different sorting rules. We will do\n",
    "# community and household rules on different rows. We will\n",
    "# do the different metrics as different columns\n",
    "# So this is a grid axis\n",
    "# Do histograms (budgets are changing)\n",
    "# Says nothing direclty about the pareto front, but hints at what\n",
    "# policies dominate on different dimensions\n",
    "# And this is especially important for the story of our paper\n",
    "# comparing community prioritization criteria to \n",
    "# household equity indicators\n",
    "# Gives us the visual cues to talk about sovi dominating cejst\n",
    "# even though FEMA just changed from sovi to cejst\n",
    "# Allows us to explain that new FEMA critical disaster \n",
    "# resilience zones are not even in our community\n",
    "# Allows us to show that the rel_eal based sorting (household\n",
    "# equity indicators) dominates efficiency based sorting (i.e. npv)\n",
    "# Need to make it so that all axes share the same direction\n",
    "# of prference. I think that is visually nicer\n",
    "# Community rules are different colors & dotted\n",
    "# Household \"\" & solid (different colors than above)\n",
    "# No fill on the histograms\n",
    "# Share x axis no columns & y axis all\n",
    "\n",
    "# We need the objs dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f2ab5-1e72-4cd7-bfab-75fdb7ea0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4\n",
    "# Parallel axis plot\n",
    "# 2 panels where top is a horizontal bar\n",
    "# Shows policies on y-axis sorted from top to bottom in terms\n",
    "# of how many pareto policies come from the sorting rule\n",
    "# And x-axis shows the count\n",
    "# We do horizontal for increased readability of the policy names\n",
    "# This sneakily serves as our plot legend since we will\n",
    "# color the bars\n",
    "\n",
    "# The bottom panel uses the colors for each bar for plotting\n",
    "# the lines associated with each policy\n",
    "# I just want to plot the pareto optimal policies\n",
    "# The key is to highlight tradeoffs across objectives on a per-policy\n",
    "# basis\n",
    "# It might also be worth showing -- in grayed out lines -- \n",
    "# the policies that are pareto optimal if you only use community\n",
    "# prioritization criteria. This is just to show how much\n",
    "# these are pareto dominated by the full pareto front. But\n",
    "# it might be too much clutter. It might be easy enough\n",
    "# to compare the hypervolume separating the two pareto fronts\n",
    "# and make this point in text as opposed to visually, though\n",
    "# both would be ideal\n",
    "\n",
    "# We need the pareto_f dataframe, and maybe the pareto_c one\n",
    "# for testing out the second idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50534a10-2689-47e0-923f-448d8291ffe7",
   "metadata": {},
   "source": [
    "# Main Summary Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011fa6c1-914b-48e4-a5e7-03d103b9f37e",
   "metadata": {},
   "source": [
    "# Supplementary Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd263b-f77d-4baf-9120-b93a4876230c",
   "metadata": {},
   "source": [
    "# Supplementary Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce148c4-ee82-45fa-bf39-85bfcdc2d856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

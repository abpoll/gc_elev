{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df130da-8ff7-481a-9f41-49220ebbb9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:18:06.780826Z",
     "iopub.status.busy": "2024-03-07T17:18:06.780412Z",
     "iopub.status.idle": "2024-03-07T17:18:06.899800Z",
     "shell.execute_reply": "2024-03-07T17:18:06.899114Z",
     "shell.execute_reply.started": "2024-03-07T17:18:06.780797Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea6c73e-9b4f-4bf1-a40c-0d4105e9eafb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:18:06.935058Z",
     "iopub.status.busy": "2024-03-07T17:18:06.934725Z",
     "iopub.status.idle": "2024-03-07T17:18:17.428806Z",
     "shell.execute_reply": "2024-03-07T17:18:17.427983Z",
     "shell.execute_reply.started": "2024-03-07T17:18:06.935035Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a43d5d-4b14-43c7-80ec-eb029a6e5529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:18:17.430698Z",
     "iopub.status.busy": "2024-03-07T17:18:17.430235Z",
     "iopub.status.idle": "2024-03-07T17:18:17.450858Z",
     "shell.execute_reply": "2024-03-07T17:18:17.450218Z",
     "shell.execute_reply.started": "2024-03-07T17:18:17.430673Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6111f5a-c61f-469a-92b2-265c90f3cd6d",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120b6681-df66-4e80-8f4b-d2e7495fe878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:18:17.451880Z",
     "iopub.status.busy": "2024-03-07T17:18:17.451663Z",
     "iopub.status.idle": "2024-03-07T17:18:17.486259Z",
     "shell.execute_reply": "2024-03-07T17:18:17.485601Z",
     "shell.execute_reply.started": "2024-03-07T17:18:17.451859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate structure ensemble\n",
    "# Merge hazard data in\n",
    "# Sample from the depth grids\n",
    "# Add our vulnerability uncertainty\n",
    "# (it's conditioned on the depth value in \n",
    "# a particular state of the world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23370d86-7c25-4b42-afad-11395b3a819f",
   "metadata": {},
   "source": [
    "## Load and subset exposure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59719ccf-11fb-4507-bc0a-86cf020e58d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:18:17.487852Z",
     "iopub.status.busy": "2024-03-07T17:18:17.487563Z",
     "iopub.status.idle": "2024-03-07T17:18:19.697404Z",
     "shell.execute_reply": "2024-03-07T17:18:19.696413Z",
     "shell.execute_reply.started": "2024-03-07T17:18:17.487829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the single family homes,\n",
    "# the fd_id/reference file\n",
    "# the fd_id/depths file\n",
    "# the fd_id flood zone file\n",
    "nsi_struct = gpd.read_file(join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg'))\n",
    "nsi_ref = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_ref.pqt'))\n",
    "nsi_depths = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_depths.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcd531e-6d19-4f32-908e-04184401466e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:49:42.211653Z",
     "iopub.status.busy": "2024-03-07T17:49:42.211381Z",
     "iopub.status.idle": "2024-03-07T17:49:42.979076Z",
     "shell.execute_reply": "2024-03-07T17:49:42.977580Z",
     "shell.execute_reply.started": "2024-03-07T17:49:42.211630Z"
    }
   },
   "outputs": [],
   "source": [
    "# The point of the code below is to make it so that\n",
    "# we can draw from triangular distributions\n",
    "# This is a very case-study specific solution\n",
    "# Ultimately, we'd like to have a pdf of depths\n",
    "# to draw from - either a distribution & fitted parameters\n",
    "# or an empirical distribution\n",
    "# So, we are going to round depths to the nearest foot\n",
    "# and we're going to retain the rows that have non-zero\n",
    "# foot relative to grade depths\n",
    "# Don't want to bake this processing into the process_haz\n",
    "# scripts because it is case-study specific\n",
    "\n",
    "## Note - we're not doing triangular anymore, for now\n",
    "# Instead, we need to do processing that\n",
    "# let's us treat these as separate scenarios\n",
    "\n",
    "# Filter to properties with > 0 \n",
    "nsi_depths = nsi_depths[nsi_depths.iloc[:,1:].sum(axis=1) > 0]\n",
    "\n",
    "# We need to melt our dataframe\n",
    "# Split return periods and scenarios\n",
    "# then pivot with fd_id and scenarios as our id vars\n",
    "nsi_d_melt = nsi_depths.melt(id_vars='fd_id', value_name='depth_ft')\n",
    "nsi_d_melt['rp'] = nsi_d_melt['variable'].str.split('_').str[0]\n",
    "nsi_d_melt['scen'] = nsi_d_melt['variable'].str.split('_').str[1]\n",
    "depths_df = nsi_d_melt.pivot(index=['fd_id', 'scen'], columns=['rp'],\n",
    "                             values='depth_ft').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57e6ce7-c2d3-4fa4-9613-a52c31e004a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:49:44.187707Z",
     "iopub.status.busy": "2024-03-07T17:49:44.187367Z",
     "iopub.status.idle": "2024-03-07T17:49:44.267510Z",
     "shell.execute_reply": "2024-03-07T17:49:44.265774Z",
     "shell.execute_reply.started": "2024-03-07T17:49:44.187676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need foundation type, number stories, structure value\n",
    "# for our ensemble. Structure value will be the center of \n",
    "# the distribution and will be passed to the loss estimation\n",
    "# function. Foundation type will be drawn from the implicit\n",
    "# distribution in the NSI data. For each census block, \n",
    "# we are going to get the multinomial probabilities of \n",
    "# a building having a certain foundation type & number of stories\n",
    "# Ideally, we would do this conditioned on prefirm but the\n",
    "# building year column is based on median year built from ACS\n",
    "# data\n",
    "# From the foundation type that is drawn from the multinomial in \n",
    "# the ensemble, we will get the FFE from the distribution \n",
    "# defined in the code for the Wing et al. 2022 paper\n",
    "# The point estimate version will just use default values\n",
    "\n",
    "# Start by retaining only relevant columns in nsi_struct\n",
    "# Then subset this and nsi_ref to the fd_id in nsi_depths\n",
    "# We do need sqft for elevation cost or floodproof estimates\n",
    "\n",
    "# Normally we would only keep the below, but I'm commenting those out\n",
    "# because we also want to keep found_ht\n",
    "# keep_cols = ['fd_id', 'occtype', 'val_struct']\n",
    "keep_cols = ['fd_id', 'occtype', 'val_struct', 'bldgtype',\n",
    "             'found_type', 'found_ht', 'sqft']\n",
    "nsi_res = nsi_struct[keep_cols]\n",
    "\n",
    "# Let's merge in refs into nsi_res\n",
    "nsi_res = nsi_res.merge(nsi_ref, on='fd_id')\n",
    "\n",
    "# Split occtype to get the number of stories and basement\n",
    "# We only need to keep stories for the purposes\n",
    "# of estimating the distribution that stories comes from\n",
    "# We will draw basement from the foundation type\n",
    "# distribution which also gives us first floor elevation\n",
    "\n",
    "## Can comment these out...\n",
    "\n",
    "# structs = nsi_res['occtype'].str.split('-').str[1]\n",
    "# basements = structs.str[2:]\n",
    "# stories = structs.str[:2]\n",
    "\n",
    "# nsi_res = nsi_res.assign(stories=stories)\n",
    "\n",
    "# Retain only the rows that correspond to structures\n",
    "# that are exposed to flood depths\n",
    "## For this case study, we don't need to merge depths in\n",
    "# at this stage\n",
    "full_df = nsi_res[nsi_res['fd_id'].isin(nsi_depths['fd_id'])]\n",
    "\n",
    "# Merge in the depths to the struct df you are working with\n",
    "# Also merge in the refs - there are inconsistencies\n",
    "# with the cbfips column from nsi directly and the\n",
    "# block data I downloaded from the census webpage\n",
    "# You retain more structures if you use the block data \n",
    "# full_df = nsi_res_f.merge(nsi_depths, on='fd_id')\n",
    "\n",
    "# This dataset can be directly used for estimating the \n",
    "# benchmark losses of using NSI as-is\n",
    "# Use the Hazus DDFs with no uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1a487-2c5b-45f2-9d51-0bec86293572",
   "metadata": {},
   "source": [
    "## Get parameters for structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b7511-969a-4853-8e2e-aa7b9eb805b7",
   "metadata": {},
   "source": [
    "## Load depth damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2b3e0d-7253-45b0-9eb0-88b5acd96bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:49:48.531513Z",
     "iopub.status.busy": "2024-03-07T17:49:48.530990Z",
     "iopub.status.idle": "2024-03-07T17:49:48.640109Z",
     "shell.execute_reply": "2024-03-07T17:49:48.638446Z",
     "shell.execute_reply.started": "2024-03-07T17:49:48.531468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load DDFs\n",
    "naccs_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'naccs_ddfs.pqt'))\n",
    "\n",
    "# Load helper dictionaries\n",
    "with open(join(VULN_DIR_I, 'physical', 'naccs.json'), 'r') as fp:\n",
    "    NACCS_MAX_DICT = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af95c2-be74-41f7-862e-df0fe0f0beca",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c0b6e3-68e3-447d-ba42-a70f1a5a0421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:50:03.433901Z",
     "iopub.status.busy": "2024-03-07T17:50:03.433712Z",
     "iopub.status.idle": "2024-03-07T17:50:03.454732Z",
     "shell.execute_reply": "2024-03-07T17:50:03.453035Z",
     "shell.execute_reply.started": "2024-03-07T17:50:03.433886Z"
    }
   },
   "outputs": [],
   "source": [
    "# We need a randon number generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28457a0-b693-4514-b38e-e2ee0a9f8742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:49:50.880825Z",
     "iopub.status.busy": "2024-03-07T17:49:50.880295Z",
     "iopub.status.idle": "2024-03-07T17:49:56.726917Z",
     "shell.execute_reply": "2024-03-07T17:49:56.726117Z",
     "shell.execute_reply.started": "2024-03-07T17:49:50.880782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Index for Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Need to create a dataframe w/ 10,000 rows for each fd_id\n",
    "# From full_df, keep fd_id, val_struct, bg_id, and the\n",
    "# depth columns. \n",
    "# The way I usually do this is with\n",
    "# df.loc[np.repeat(df.index, N)].reset_index(drop=True)\n",
    "# With this approach, we can do everything in a vectorized\n",
    "# form by passing array_like data of size N*len(df)\n",
    "# to different rng() calls to get all the draws from\n",
    "# distributions that we need\n",
    "\n",
    "## The below is what we normally would drop\n",
    "# But I have to change it for this case study\n",
    "# drop_cols = ['occtype', 'found_type', 'block_id', 'fld_zone',\n",
    "#              'stories']\n",
    "drop_cols = ['block_id']\n",
    "\n",
    "ens_df = full_df.drop(columns=drop_cols)\n",
    "ens_df = ens_df.loc[np.repeat(ens_df.index, N_SOW)].reset_index(drop=True)\n",
    "print('Created Index for Ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1b82c-188c-41d8-b638-69bc88db4ef9",
   "metadata": {},
   "source": [
    "## Sample structure characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9da0470a-5553-47bb-97ab-ac937aef3020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T17:50:06.238223Z",
     "iopub.status.busy": "2024-03-07T17:50:06.237697Z",
     "iopub.status.idle": "2024-03-07T17:50:58.926241Z",
     "shell.execute_reply": "2024-03-07T17:50:58.925158Z",
     "shell.execute_reply.started": "2024-03-07T17:50:06.238175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw values\n",
      "Generated Structure Characteristics\n"
     ]
    }
   ],
   "source": [
    "# Values\n",
    "# Draw from the structure value distribution for each property\n",
    "# normal(val_struct, val_struct*CF_DET) where these are array_like\n",
    "# Using 1 as an artificial, arbitrary lower bound on value\n",
    "# Very low probability of getting a negative number but we cannot\n",
    "# allow that because you cannot have negative risk\n",
    "vals = rng.normal(ens_df['val_struct'],\n",
    "                  ens_df['val_struct']*COEF_VARIATION)\n",
    "vals[vals < 1] = 1\n",
    "ens_df['val_s'] = vals\n",
    "\n",
    "print('Draw values')\n",
    "\n",
    "# For this case study, use the below code\n",
    "# This drops the \"RES1-\" part of the occtype column\n",
    "# and keeps 1SNB, 2SNB, etc.\n",
    "ens_df['bld_types'] = ens_df['occtype'].str.split('-').str[1]\n",
    "\n",
    "# In theory, bld_type is naccs_ddf_type. No need to \n",
    "# take this storage up in practice... just refer to bld_type\n",
    "# when needed\n",
    "\n",
    "# We are going to use the fnd_type to draw from the\n",
    "# FFE distribution\n",
    "# Need to use np.stack to get the array of floats\n",
    "tri_params = np.stack(ens_df['found_type'].map(FFE_DICT))\n",
    "\n",
    "# Can use [:] to access like a matrix and directly input to \n",
    "# rng.triangular\n",
    "# 0, 1, and 2 are column indices corresponding to left,\n",
    "# mode, and right\n",
    "# We round this to the nearest foot\n",
    "ffes = np.round(rng.triangular(tri_params[:,0],\n",
    "                               tri_params[:,1],\n",
    "                               tri_params[:,2]))\n",
    "ens_df['ffe'] = ffes\n",
    "\n",
    "print('Generated Structure Characteristics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d7659-03e2-4c2f-bed5-5d435df1233a",
   "metadata": {},
   "source": [
    "## Estimate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d124645c-368f-4b45-b114-df8731e83aa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:20:05.132453Z",
     "iopub.status.busy": "2024-02-29T20:20:05.132312Z",
     "iopub.status.idle": "2024-02-29T20:20:17.035932Z",
     "shell.execute_reply": "2024-02-29T20:20:17.035369Z",
     "shell.execute_reply.started": "2024-02-29T20:20:05.132438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: Lower\n",
      "Adjuted depths by FFE\n",
      "\n",
      "Scenario: Mid\n",
      "Adjuted depths by FFE\n",
      "\n",
      "Scenario: Upper\n",
      "Adjuted depths by FFE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## For this case study, we're using depths as scenarios\n",
    "# Loop through each of Lower, Mid, Upper in the depths_df\n",
    "# and merge this depth_df into ens_df\n",
    "# This is hard coded (i.e. Lower/Mid) and (500) which isn't ideal\n",
    "# and I should replace these with values in the config file\n",
    "# Store this in a dictionary - it's a little easier\n",
    "ens_dfs = {}\n",
    "# Also helps to have a dictionary for the depths adjusted\n",
    "# by first floor elevation\n",
    "depth_ffes = {}\n",
    "for scen in ['Mid']:\n",
    "    print('Scenario: ' + scen)\n",
    "    # We subset to the scenario\n",
    "    depth_df = depths_df[depths_df['scen'] == scen].drop(columns=['scen'])\n",
    "    # We only need to keep properties with depth[500] > 0\n",
    "    keep_rows = depth_df['500'] > 0\n",
    "    depth_df = depth_df.loc[keep_rows]\n",
    "    # Replace 0 values with na\n",
    "    depth_df[depth_df == 0] = np.nan\n",
    "    # Let's do an inner merge so that we don't have\n",
    "    # to keep the ensemble members that correspond to \n",
    "    # 0 losses under this scenario\n",
    "    ens_dfs[scen] = ens_df.merge(depth_df, how='inner', on='fd_id')\n",
    "    # Dataframe for adjusted depths\n",
    "    # depth_df and ens_dfs\n",
    "    depth_ffes[scen] = ens_dfs[scen][RET_PERS].subtract(ens_dfs[scen]['ffe'],\n",
    "                                                        axis=0).round(1) \n",
    "    print('Adjuted depths by FFE\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "003f5629-dbc4-4da1-b408-5a0f6b96351f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:20:17.037153Z",
     "iopub.status.busy": "2024-02-29T20:20:17.036867Z",
     "iopub.status.idle": "2024-02-29T20:30:47.021593Z",
     "shell.execute_reply": "2024-02-29T20:30:47.020750Z",
     "shell.execute_reply.started": "2024-02-29T20:20:17.037130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: Lower\n",
      "Estimate Losses for NACCS & Hazus, RP: 001\n",
      "Estimate Losses for NACCS & Hazus, RP: 002\n",
      "Estimate Losses for NACCS & Hazus, RP: 005\n",
      "Estimate Losses for NACCS & Hazus, RP: 010\n",
      "Estimate Losses for NACCS & Hazus, RP: 015\n",
      "Estimate Losses for NACCS & Hazus, RP: 020\n",
      "Estimate Losses for NACCS & Hazus, RP: 025\n",
      "Estimate Losses for NACCS & Hazus, RP: 050\n",
      "Estimate Losses for NACCS & Hazus, RP: 075\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 200\n",
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Obtained Full Ensemble\n",
      "Calculated EAL\n",
      "Stored in dictionary\n",
      "\n",
      "Scenario: Mid\n",
      "Estimate Losses for NACCS & Hazus, RP: 001\n",
      "Estimate Losses for NACCS & Hazus, RP: 002\n",
      "Estimate Losses for NACCS & Hazus, RP: 005\n",
      "Estimate Losses for NACCS & Hazus, RP: 010\n",
      "Estimate Losses for NACCS & Hazus, RP: 015\n",
      "Estimate Losses for NACCS & Hazus, RP: 020\n",
      "Estimate Losses for NACCS & Hazus, RP: 025\n",
      "Estimate Losses for NACCS & Hazus, RP: 050\n",
      "Estimate Losses for NACCS & Hazus, RP: 075\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 200\n",
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Obtained Full Ensemble\n",
      "Calculated EAL\n",
      "Stored in dictionary\n",
      "\n",
      "Scenario: Upper\n",
      "Estimate Losses for NACCS & Hazus, RP: 001\n",
      "Estimate Losses for NACCS & Hazus, RP: 002\n",
      "Estimate Losses for NACCS & Hazus, RP: 005\n",
      "Estimate Losses for NACCS & Hazus, RP: 010\n",
      "Estimate Losses for NACCS & Hazus, RP: 015\n",
      "Estimate Losses for NACCS & Hazus, RP: 020\n",
      "Estimate Losses for NACCS & Hazus, RP: 025\n",
      "Estimate Losses for NACCS & Hazus, RP: 050\n",
      "Estimate Losses for NACCS & Hazus, RP: 075\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 200\n",
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Obtained Full Ensemble\n",
      "Calculated EAL\n",
      "Stored in dictionary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting losses\n",
    "ens_dfs = {}\n",
    "# Also helps to have a dictionary for the depths adjusted\n",
    "# by first floor elevation\n",
    "depth_ffes = {}\n",
    "for scen in ['Mid']:\n",
    "    print('Scenario: ' + scen)\n",
    "    # We subset to the scenario\n",
    "    depth_df = depths_df[depths_df['scen'] == scen].drop(columns=['scen'])\n",
    "    # We only need to keep properties with depth[500] > 0\n",
    "    keep_rows = depth_df['500'] > 0\n",
    "    depth_df = depth_df.loc[keep_rows]\n",
    "    # Replace 0 values with na\n",
    "    depth_df[depth_df == 0] = np.nan\n",
    "    # Let's do an inner merge so that we don't have\n",
    "    # to keep the ensemble members that correspond to \n",
    "    # 0 losses under this scenario\n",
    "    ens_dfs[scen] = ens_df.merge(depth_df, how='inner', on='fd_id')\n",
    "    # Dataframe for adjusted depths\n",
    "    # depth_df and ens_dfs\n",
    "    depth_ffes[scen] = ens_dfs[scen][RET_PERS].subtract(ens_dfs[scen]['ffe'],\n",
    "                                                        axis=0).round(1) \n",
    "    print('Adjuted depths by FFE\\n')\n",
    "\n",
    "# Now, we are going to loop through each return period\n",
    "# and estimate losses for NACCS \n",
    "# We do this for each of the ens_df in ens_dfs\n",
    "for scen, ens_df in ens_dfs.items():\n",
    "    print('Scenario: ' + scen)\n",
    "    # Get the depth_ffe_df\n",
    "    depth_ffe_df = depth_ffes[scen]\n",
    "    \n",
    "    # We will store these in dictionaries with return period keys\n",
    "    hazus_loss = {}\n",
    "    naccs_loss = {}\n",
    "    \n",
    "    for rp in RET_PERS:\n",
    "        naccs_loss[rp] = est_naccs_loss(ens_df['bld_types'],\n",
    "                                        depth_ffe_df[rp],\n",
    "                                        naccs_ddfs,\n",
    "                                        NACCS_MAX_DICT)\n",
    "    \n",
    "        print('Estimate Losses for NACCS, RP: ' + rp)\n",
    "    \n",
    "    # Then, we convert this to a dataframe\n",
    "    losses_df = pd.DataFrame.from_dict(naccs_loss)\n",
    "\n",
    "    # Update column names\n",
    "    losses_df.columns = ['naccs_rel_dam_' + x for x in losses_df.columns]\n",
    "\n",
    "    # Now we concat these with ens_df, stories, fnd_type,\n",
    "    # ffe, structure value, and depth_ffe_df\n",
    "    depth_ffe = pd.DataFrame.from_dict(depth_ffe_df)\n",
    "    \n",
    "    # Add clearer column names\n",
    "    depth_ffe.columns = ['depth_ffe_' + x for x in depth_ffe.columns]\n",
    "\n",
    "    # For our case study, ens_df contains occtype & \n",
    "    # found_ht, so don't need to add structure characteristics\n",
    "    # back in\n",
    "    ens_df = pd.concat([ens_df, losses_df, depth_ffe],\n",
    "                       axis=1)\n",
    "    \n",
    "    # Get relative damage columns\n",
    "    rel_cols = [x for x in ens_df.columns if 'rel_dam' in x]\n",
    "    # For each relative damage column, scale by val_s, the structure\n",
    "    # value realization\n",
    "    # We need to do this for naccs & hazus prefixes\n",
    "    for col in rel_cols:\n",
    "        prefix = col.split('_')[0]\n",
    "        rp = col.split('_')[-1]\n",
    "        ens_df[prefix + '_loss_' + rp] = ens_df[col]*ens_df['val_s']\n",
    "    \n",
    "    print('Obtained Full Ensemble')\n",
    "\n",
    "    # Now we calculate EAL\n",
    "    # We will use trapezoidal approximation for this\n",
    "    # Using trapezoid method and adding bin of lowest probability\n",
    "    # events to obtain expected annual \n",
    "    \n",
    "    # We make a list of our loss columns\n",
    "    # This is easier to do splitting by prefix\n",
    "    naccs_loss_list = ['naccs_loss_' + x for x in RET_PERS]\n",
    "    # As well as the corresponding probabilities\n",
    "    p_rp_list = [round(1/int(x), 4) for x in RET_PERS]\n",
    "    \n",
    "    # Then we create an empty series\n",
    "    # Two, for hazus & naccs loss estimates\n",
    "    eal_naccs = pd.Series(index=ens_df.index).fillna(0)\n",
    "    \n",
    "    # We loop through our loss list and apply the \n",
    "    # trapezoidal approximation\n",
    "    for i in range(len(naccs_loss_list) - 1):\n",
    "        loss1_naccs = ens_df[naccs_loss_list[i]]\n",
    "        loss2_naccs = ens_df[naccs_loss_list[i+1]]\n",
    "        rp1 = p_rp_list[i]\n",
    "        rp2 = p_rp_list[i+1]\n",
    "        # We add each approximation\n",
    "        eal_naccs += (loss1_naccs + loss2_naccs)*(rp1-rp2)/2\n",
    "    # This is the final trapezoid to add in\n",
    "    final_eal_naccs = eal_naccs + ens_df[naccs_loss_list[-1]]*p_rp_list[-1]\n",
    "    print('Calculated EAL')\n",
    "    # Add eal columns to our dataframe\n",
    "    ens_df = pd.concat([ens_df, pd.Series(final_eal_naccs, name='naccs_eal')],\n",
    "                       axis=1)\n",
    "    \n",
    "    # Let's also get the SOW index - start at 0\n",
    "    sow_ind = np.arange(len(ens_df))%N_SOW\n",
    "    ens_df = pd.concat([ens_df, pd.Series(sow_ind, name='sow_ind')], axis=1)\n",
    "\n",
    "    # Put this back in ens_dfs[scen]\n",
    "    ens_dfs[scen] = ens_df\n",
    "    print('Stored in dictionary\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a51a9926-7c09-4d66-b60d-92a3ea946afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:30:47.022658Z",
     "iopub.status.busy": "2024-02-29T20:30:47.022515Z",
     "iopub.status.idle": "2024-02-29T20:31:55.447143Z",
     "shell.execute_reply": "2024-02-29T20:31:55.446005Z",
     "shell.execute_reply.started": "2024-02-29T20:30:47.022643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out our ensemble df\n",
    "ens_out_filep = join(FO, 'ensemble_Mid.pqt')\n",
    "prepare_saving(ens_out_filep)\n",
    "ens_dfs['Mid'].to_parquet(join(FO, 'ensemble_Mid.pqt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

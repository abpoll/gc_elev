{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2df130da-8ff7-481a-9f41-49220ebbb9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:20.841564Z",
     "iopub.status.busy": "2024-02-29T20:18:20.840337Z",
     "iopub.status.idle": "2024-02-29T20:18:20.875924Z",
     "shell.execute_reply": "2024-02-29T20:18:20.874733Z",
     "shell.execute_reply.started": "2024-02-29T20:18:20.841511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aea6c73e-9b4f-4bf1-a40c-0d4105e9eafb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:20.879045Z",
     "iopub.status.busy": "2024-02-29T20:18:20.878087Z",
     "iopub.status.idle": "2024-02-29T20:18:20.921403Z",
     "shell.execute_reply": "2024-02-29T20:18:20.920262Z",
     "shell.execute_reply.started": "2024-02-29T20:18:20.878995Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62a43d5d-4b14-43c7-80ec-eb029a6e5529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:20.923319Z",
     "iopub.status.busy": "2024-02-29T20:18:20.922874Z",
     "iopub.status.idle": "2024-02-29T20:18:20.961240Z",
     "shell.execute_reply": "2024-02-29T20:18:20.960139Z",
     "shell.execute_reply.started": "2024-02-29T20:18:20.923276Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6111f5a-c61f-469a-92b2-265c90f3cd6d",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "120b6681-df66-4e80-8f4b-d2e7495fe878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:20.965091Z",
     "iopub.status.busy": "2024-02-29T20:18:20.964204Z",
     "iopub.status.idle": "2024-02-29T20:18:21.000226Z",
     "shell.execute_reply": "2024-02-29T20:18:20.999096Z",
     "shell.execute_reply.started": "2024-02-29T20:18:20.965045Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate structure ensemble\n",
    "# Merge hazard data in\n",
    "# Sample from the depth grids\n",
    "# Add our vulnerability uncertainty\n",
    "# (it's conditioned on the depth value in \n",
    "# a particular state of the world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23370d86-7c25-4b42-afad-11395b3a819f",
   "metadata": {},
   "source": [
    "## Load and subset exposure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59719ccf-11fb-4507-bc0a-86cf020e58d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.002034Z",
     "iopub.status.busy": "2024-02-29T20:18:21.001597Z",
     "iopub.status.idle": "2024-02-29T20:18:21.534803Z",
     "shell.execute_reply": "2024-02-29T20:18:21.534053Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.001991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the single family homes,\n",
    "# the fd_id/reference file\n",
    "# the fd_id/depths file\n",
    "# the fd_id flood zone file\n",
    "nsi_struct = gpd.read_file(join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg'))\n",
    "nsi_ref = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_ref.pqt'))\n",
    "nsi_depths = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_depths.pqt'))\n",
    "nsi_fz = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_fz.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fcd531e-6d19-4f32-908e-04184401466e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.535531Z",
     "iopub.status.busy": "2024-02-29T20:18:21.535391Z",
     "iopub.status.idle": "2024-02-29T20:18:21.668860Z",
     "shell.execute_reply": "2024-02-29T20:18:21.668119Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.535518Z"
    }
   },
   "outputs": [],
   "source": [
    "# The point of the code below is to make it so that\n",
    "# we can draw from triangular distributions\n",
    "# This is a very case-study specific solution\n",
    "# Ultimately, we'd like to have a pdf of depths\n",
    "# to draw from - either a distribution & fitted parameters\n",
    "# or an empirical distribution\n",
    "# So, we are going to round depths to the nearest foot\n",
    "# and we're going to retain the rows that have non-zero\n",
    "# foot relative to grade depths\n",
    "# Don't want to bake this processing into the process_haz\n",
    "# scripts because it is case-study specific\n",
    "\n",
    "## Note - we're not doing triangular anymore, for now\n",
    "# Instead, we need to do processing that\n",
    "# let's us treat these as separate scenarios\n",
    "\n",
    "# Filter to properties with > 0 \n",
    "nsi_depths = nsi_depths[nsi_depths.iloc[:,1:].sum(axis=1) > 0]\n",
    "\n",
    "# We need to melt our dataframe\n",
    "# Split return periods and scenarios\n",
    "# then pivot with fd_id and scenarios as our id vars\n",
    "nsi_d_melt = nsi_depths.melt(id_vars='fd_id', value_name='depth_ft')\n",
    "nsi_d_melt['rp'] = nsi_d_melt['variable'].str.split('_').str[0]\n",
    "nsi_d_melt['scen'] = nsi_d_melt['variable'].str.split('_').str[1]\n",
    "depths_df = nsi_d_melt.pivot(index=['fd_id', 'scen'], columns=['rp'],\n",
    "                             values='depth_ft').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e57e6ce7-c2d3-4fa4-9613-a52c31e004a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.669657Z",
     "iopub.status.busy": "2024-02-29T20:18:21.669530Z",
     "iopub.status.idle": "2024-02-29T20:18:21.687333Z",
     "shell.execute_reply": "2024-02-29T20:18:21.686668Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.669645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need foundation type, number stories, structure value\n",
    "# for our ensemble. Structure value will be the center of \n",
    "# the distribution and will be passed to the loss estimation\n",
    "# function. Foundation type will be drawn from the implicit\n",
    "# distribution in the NSI data. For each census block, \n",
    "# we are going to get the multinomial probabilities of \n",
    "# a building having a certain foundation type & number of stories\n",
    "# Ideally, we would do this conditioned on prefirm but the\n",
    "# building year column is based on median year built from ACS\n",
    "# data\n",
    "# From the foundation type that is drawn from the multinomial in \n",
    "# the ensemble, we will get the FFE from the distribution \n",
    "# defined in the code for the Wing et al. 2022 paper\n",
    "# The point estimate version will just use default values\n",
    "\n",
    "# Start by retaining only relevant columns in nsi_struct\n",
    "# Then subset this and nsi_ref to the fd_id in nsi_depths\n",
    "# We do need sqft for elevation cost or floodproof estimates\n",
    "\n",
    "# Normally we would only keep the below, but I'm commenting those out\n",
    "# because we also want to keep found_ht\n",
    "# keep_cols = ['fd_id', 'occtype', 'val_struct']\n",
    "keep_cols = ['fd_id', 'occtype', 'val_struct', 'bldgtype',\n",
    "             'found_type', 'found_ht', 'sqft']\n",
    "nsi_res = nsi_struct[keep_cols]\n",
    "\n",
    "# Let's merge in refs into nsi_res\n",
    "nsi_res = nsi_res.merge(nsi_ref, on='fd_id')\n",
    "\n",
    "# We're also going to merge in fzs\n",
    "nsi_res = nsi_res.merge(nsi_fz[['fd_id', 'fld_zone']], on='fd_id')\n",
    "\n",
    "# Split occtype to get the number of stories and basement\n",
    "# We only need to keep stories for the purposes\n",
    "# of estimating the distribution that stories comes from\n",
    "# We will draw basement from the foundation type\n",
    "# distribution which also gives us first floor elevation\n",
    "\n",
    "## Can comment these out...\n",
    "\n",
    "# structs = nsi_res['occtype'].str.split('-').str[1]\n",
    "# basements = structs.str[2:]\n",
    "# stories = structs.str[:2]\n",
    "\n",
    "# nsi_res = nsi_res.assign(stories=stories)\n",
    "\n",
    "# Retain only the rows that correspond to structures\n",
    "# that are exposed to flood depths\n",
    "## For this case study, we don't need to merge depths in\n",
    "# at this stage\n",
    "full_df = nsi_res[nsi_res['fd_id'].isin(nsi_depths['fd_id'])]\n",
    "\n",
    "# Merge in the depths to the struct df you are working with\n",
    "# Also merge in the refs - there are inconsistencies\n",
    "# with the cbfips column from nsi directly and the\n",
    "# block data I downloaded from the census webpage\n",
    "# You retain more structures if you use the block data \n",
    "# full_df = nsi_res_f.merge(nsi_depths, on='fd_id')\n",
    "\n",
    "# This dataset can be directly used for estimating the \n",
    "# benchmark losses of using NSI as-is\n",
    "# Use the Hazus DDFs with no uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ada9027e-c015-4fec-bee8-cb05132e88b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.687898Z",
     "iopub.status.busy": "2024-02-29T20:18:21.687785Z",
     "iopub.status.idle": "2024-02-29T20:18:21.724852Z",
     "shell.execute_reply": "2024-02-29T20:18:21.724205Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.687887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the fld_zone column processed for the way it needs\n",
    "# to be done for using hazus ddfs\n",
    "# Get the first character of the flood zone and only retain it\n",
    "# if it's a V zone. We are going to use A zone for A and outside\n",
    "# (if any) flood zone depth exposures\n",
    "ve_zone = np.where(full_df['fld_zone'].str[0] == 'V',\n",
    "                   'V',\n",
    "                   'A')\n",
    "full_df = full_df.assign(fz_ddf = ve_zone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1a487-2c5b-45f2-9d51-0bec86293572",
   "metadata": {},
   "source": [
    "## Get parameters for structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "941ef1b8-8c14-4ba0-9ba4-879fe974dbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.725551Z",
     "iopub.status.busy": "2024-02-29T20:18:21.725428Z",
     "iopub.status.idle": "2024-02-29T20:18:21.761935Z",
     "shell.execute_reply": "2024-02-29T20:18:21.761214Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.725539Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are also going to use nsi_struct merged with refs\n",
    "# to determine the multinomial probabilities of basement\n",
    "# and number stories (binomial) from block level which matches\n",
    "# up with NSI tech reference on where data is randomly assigned\n",
    "# from. While there are maps from parcel data, where available, \n",
    "# it's not clear which entries have this non-random assignment. \n",
    "# In addition, it is known that parcel aggregation datasets like\n",
    "# ZTRAX may have data errors. The sources the NSI used\n",
    "# have unknown validation/accuracy so we can treat these as\n",
    "# part of estimating the distribution to draw from\n",
    "\n",
    "# The method for estimating number of stories is based on assignment\n",
    "# from parcel data. Where missing, square footage is divided by the \n",
    "# structure's footprint (when sq. ft. is missing, they take 86% of\n",
    "# the structure's footprint as sq. ft). If > 1.25,\n",
    "# a second floor is assumed\n",
    "# If no footprint is available, \n",
    "# stories is randomly assigned from a distribution that varies by\n",
    "# year built and census region. So, we can use census block again\n",
    "# here\n",
    "\n",
    "# The methodology for the structure valuation is obscure\n",
    "# and there is no reporting on how accurate it is to some\n",
    "# observed data on market values\n",
    "# In a conservative thrust, we can take the reported\n",
    "# coefficient of determination from Philadelphia Assesor's \n",
    "# methodology for estimating market values. This COD can be\n",
    "# multiplied by the estimated value from NSI for a presumably\n",
    "# conservative estimate of the standard deviation surrounding\n",
    "# structure value estimates to observed market values\n",
    "# We can also show in a representative example what would\n",
    "# happen to the loss estimate distribution\n",
    "# if the NSI COD is a factor of 2 larger. We still don't know\n",
    "# if this is a reasonable representation since we assume\n",
    "# there is no bias in the NSI structure valuation by\n",
    "# centering the noise distribution at their estimated value. \n",
    "# In reality, the Philly assessor office reports their estimates\n",
    "# are slightly biased which allows us to use a bias correction\n",
    "# factor if we used that data. Down the line, comparing\n",
    "# what the structure inventory distributions are using different\n",
    "# data sources could be very interesting, including accounting\n",
    "# for different # of RES1 buildings based on more detailed\n",
    "# and survye-based methods in the city assessor data\n",
    "# From the Nolte et al. (2023) large-scale parcel data good\n",
    "# practices data, we know that there are many issues in using parcel\n",
    "# data to fill in important data fields (even occupancy type)\n",
    "# It is not the panacea it appears framed as in the NSI technical\n",
    "# documentation\n",
    "\n",
    "# There are not nearly enough observations at the block level\n",
    "# to reliably estimate the parameter for binomial # stories\n",
    "# or multinomial foundation type. Sometimes just one observation\n",
    "# in general. Tract appears to have enough\n",
    "# This check is based on the subset of tracts (or other ref)\n",
    "# in nsi_res that are also in full_df (these are the ones) we need\n",
    "# the probabilities for\n",
    "# I guess STRUCT_REF should either be a config, \n",
    "# or something identified on-the-fly based on \n",
    "# whether we have enough observations at block or blockgroup\n",
    "# before \"rolling back\" to coarser resolution to get\n",
    "# proportions\n",
    "\n",
    "# I'm commenting this out because while this is part of the UNSAFE\n",
    "# framework, this is not easy to implement\n",
    "# for the decision analysis. \n",
    "\n",
    "# STRUCT_REF = 'tract_id'\n",
    "# struct_tot = nsi_res[nsi_res[STRUCT_REF].isin(full_df[STRUCT_REF])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04bc15-8355-4697-b864-9b77af2a1d25",
   "metadata": {},
   "source": [
    "### Number of stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f63184d1-481b-411b-a2d6-7f192d8bb8c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.763908Z",
     "iopub.status.busy": "2024-02-29T20:18:21.763724Z",
     "iopub.status.idle": "2024-02-29T20:18:21.802798Z",
     "shell.execute_reply": "2024-02-29T20:18:21.802017Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.763892Z"
    }
   },
   "outputs": [],
   "source": [
    "## Going to comment this out\n",
    "\n",
    "# # Get the total number of structures w/ number of stories \n",
    "# # in each block gruop\n",
    "# stories_sum = struct_tot.groupby([STRUCT_REF, 'stories']).size()\n",
    "# # Then get the proportion\n",
    "# stories_prop = stories_sum/struct_tot.groupby([STRUCT_REF]).size()\n",
    "# # Our parameters can be drawn from this table based on the bg_id\n",
    "# # of a structure we are estimating losses for\n",
    "# stories_param = stories_prop.reset_index().pivot(index=STRUCT_REF,\n",
    "#                                                  columns='stories',\n",
    "#                                                  values=0).fillna(0)\n",
    "# # Since it's a binomial distribution, we only need to specify\n",
    "# # one param. Arbitrarily choose 1S\n",
    "# # Round the param to the hundredth place\n",
    "# # Store in a dict\n",
    "# stories_param = stories_param['1S'].round(2)\n",
    "# STRY_DICT = dict(stories_param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e298834-f43d-4d33-9146-2dfc53f20097",
   "metadata": {},
   "source": [
    "### Foundation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc9cbc33-bd3c-4379-9dc8-edabb73545f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.803640Z",
     "iopub.status.busy": "2024-02-29T20:18:21.803453Z",
     "iopub.status.idle": "2024-02-29T20:18:21.851955Z",
     "shell.execute_reply": "2024-02-29T20:18:21.850896Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.803621Z"
    }
   },
   "outputs": [],
   "source": [
    "## Going to comment this out\n",
    "\n",
    "# # Repeat procedure above\n",
    "# found_sum = struct_tot.groupby([STRUCT_REF, 'found_type']).size()\n",
    "# found_prop = found_sum/struct_tot.groupby([STRUCT_REF]).size()\n",
    "# found_param = found_prop.reset_index().pivot(index=STRUCT_REF,\n",
    "#                                              columns='found_type',\n",
    "#                                              values=0).fillna(0)\n",
    "\n",
    "# # We want a dictionary of bg_id to a list of B, C, S\n",
    "# # for direct use in our multinomial distribution draw\n",
    "# # Store params in a list (each row is bg_id and corresponds to\n",
    "# # its own probabilities of each foundation type)\n",
    "# params = found_param.values.round(2)\n",
    "# # Then create our dictionary\n",
    "# FND_DICT = dict(zip(found_param.index, params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b7511-969a-4853-8e2e-aa7b9eb805b7",
   "metadata": {},
   "source": [
    "## Load depth damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db2b3e0d-7253-45b0-9eb0-88b5acd96bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.853733Z",
     "iopub.status.busy": "2024-02-29T20:18:21.853255Z",
     "iopub.status.idle": "2024-02-29T20:18:21.917526Z",
     "shell.execute_reply": "2024-02-29T20:18:21.916282Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.853695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load DDFs\n",
    "naccs_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'naccs_ddfs.pqt'))\n",
    "hazus_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs.pqt'))\n",
    "hazus_nounc = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs_nounc.pqt'))\n",
    "\n",
    "# Load helper dictionaries\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus.json'), 'r') as fp:\n",
    "    HAZUS_MAX_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus_nounc.json'), 'r') as fp:\n",
    "    HAZUS_MAX_NOUNC_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'naccs.json'), 'r') as fp:\n",
    "    NACCS_MAX_DICT = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af95c2-be74-41f7-862e-df0fe0f0beca",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73c0b6e3-68e3-447d-ba42-a70f1a5a0421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.919615Z",
     "iopub.status.busy": "2024-02-29T20:18:21.919079Z",
     "iopub.status.idle": "2024-02-29T20:18:21.945564Z",
     "shell.execute_reply": "2024-02-29T20:18:21.944545Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.919574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reminder of the dataframes/dictionaries we have to help generate\n",
    "# our ensemble members efficiently\n",
    "# STRY_DICT\n",
    "# FND_DICT\n",
    "# FFE_DICT\n",
    "# hazus\n",
    "# naccs\n",
    "# HAZUS_MAX_DICT\n",
    "# NACCS_MAX_DICT\n",
    "# HAZUS_MAX_NOUNC_DICT\n",
    "\n",
    "# And some constants\n",
    "# COEF_VARIATION\n",
    "# N_SOW\n",
    "# RET_PERS\n",
    "\n",
    "# We need a randon number generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d28457a0-b693-4514-b38e-e2ee0a9f8742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:21.947258Z",
     "iopub.status.busy": "2024-02-29T20:18:21.946810Z",
     "iopub.status.idle": "2024-02-29T20:18:28.504612Z",
     "shell.execute_reply": "2024-02-29T20:18:28.503941Z",
     "shell.execute_reply.started": "2024-02-29T20:18:21.947224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Index for Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Need to create a dataframe w/ 10,000 rows for each fd_id\n",
    "# From full_df, keep fd_id, val_struct, bg_id, and the\n",
    "# depth columns. \n",
    "# The way I usually do this is with\n",
    "# df.loc[np.repeat(df.index, N)].reset_index(drop=True)\n",
    "# With this approach, we can do everything in a vectorized\n",
    "# form by passing array_like data of size N*len(df)\n",
    "# to different rng() calls to get all the draws from\n",
    "# distributions that we need\n",
    "\n",
    "## The below is what we normally would drop\n",
    "# But I have to change it for this case study\n",
    "# drop_cols = ['occtype', 'found_type', 'block_id', 'fld_zone',\n",
    "#              'stories']\n",
    "drop_cols = ['block_id', 'fld_zone']\n",
    "\n",
    "ens_df = full_df.drop(columns=drop_cols)\n",
    "ens_df = ens_df.loc[np.repeat(ens_df.index, N_SOW)].reset_index(drop=True)\n",
    "print('Created Index for Ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0718d-ebdb-44eb-ab6f-c036548a3062",
   "metadata": {},
   "source": [
    "## Sample depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c4ec57b-c9d2-42d0-b7a1-6cc809b52561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:28.505812Z",
     "iopub.status.busy": "2024-02-29T20:18:28.505479Z",
     "iopub.status.idle": "2024-02-29T20:18:28.525677Z",
     "shell.execute_reply": "2024-02-29T20:18:28.525058Z",
     "shell.execute_reply.started": "2024-02-29T20:18:28.505786Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## I want to keep the code below in case we decide\n",
    "# to go back to triangular... but this does not seem\n",
    "# like the appropriate approach\n",
    "# The more appropriate thing is to treat the\n",
    "# depths as scenarios and generate SOWs around\n",
    "# these -- i.e. the DDF uncertainty importance\n",
    "# for the different scenarios\n",
    "# The way this works is there will be 3 ens_df \n",
    "# for each scenario, and we merge the corresponding\n",
    "# depth_df from depths_df to the ensemble\n",
    "# we've generated (for this case study, just values)\n",
    "\n",
    "# # We want to store our sampled depths for each \n",
    "# # return period in a list\n",
    "# # And will put these in a dataframe later\n",
    "# depths_l = []\n",
    "# # Let's loop through each return period\n",
    "# for rp in RET_PERS:\n",
    "#     # Let's store rp + scen as strings for easy reference\n",
    "#     left = rp + '_Lower'\n",
    "#     mid = rp + '_Mid'\n",
    "#     right = rp + '_Upper'\n",
    "    \n",
    "#     # Let's create a temporary dataframe with the same index\n",
    "#     # as ens_df and for just the three columns we want to use\n",
    "#     # to draw from a triangular distribution\n",
    "#     draw_df = ens_df[[left, mid, right]].copy()\n",
    "\n",
    "#     # We need a mask for when the left and right are the same\n",
    "#     # In such a case we do not need (and cannot) draw from \n",
    "#     # a triangular distribution and just need to take one of \n",
    "#     # the values\n",
    "#     no_tri_mask = draw_df[left] == draw_df[right]\n",
    "\n",
    "#     # Now, we either take the left value, or we draw from\n",
    "#     # the triangular distribution\n",
    "#     draw_df.loc[no_tri_mask, rp] = draw_df.loc[no_tri_mask][left]\n",
    "    \n",
    "#     draws = rng.triangular(draw_df.loc[~no_tri_mask][left],\n",
    "#                            draw_df.loc[~no_tri_mask][mid],\n",
    "#                            draw_df.loc[~no_tri_mask][right])\n",
    "#     draw_df.loc[~no_tri_mask, rp] = draws\n",
    "\n",
    "#     # Replace 0 values with na\n",
    "#     draw_df.loc[draw_df[rp] == 0, rp] = np.nan\n",
    "    \n",
    "#     # Now we just want to store the series for our rp\n",
    "#     # in a list which we will concat after all this\n",
    "#     # looping is done\n",
    "#     # This will be our final depth dataframe\n",
    "#     depths_l.append(draw_df[rp])\n",
    "    \n",
    "#     # Helpful logging message\n",
    "#     print('Got sampled depth values: ' + rp)\n",
    "\n",
    "# \n",
    "# depth_df = pd.concat(depths_l, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1b82c-188c-41d8-b638-69bc88db4ef9",
   "metadata": {},
   "source": [
    "## Sample structure characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9da0470a-5553-47bb-97ab-ac937aef3020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:18:28.527196Z",
     "iopub.status.busy": "2024-02-29T20:18:28.526607Z",
     "iopub.status.idle": "2024-02-29T20:20:05.131455Z",
     "shell.execute_reply": "2024-02-29T20:20:05.130605Z",
     "shell.execute_reply.started": "2024-02-29T20:18:28.527167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw values\n",
      "Generated Structure Characteristics\n"
     ]
    }
   ],
   "source": [
    "# Values\n",
    "# Draw from the structure value distribution for each property\n",
    "# normal(val_struct, val_struct*CF_DET) where these are array_like\n",
    "# I also want to treat this as truncated\n",
    "# on the lower end since there is a risk of drawing impossibly\n",
    "# low numbers (like negative) with this approach\n",
    "# https://github.com/kieranrcampbell/blog-notebooks/blob/master/\n",
    "# Fast%20vectorized%20sampling%20from%20truncated%\n",
    "# 20normal%20distributions%20in%20python.ipynb\n",
    "# outlines an approach to use numpy to do a truncated sample\n",
    "# TODO move this to a util file\n",
    "def truncnorm_rvs_recursive(x, sigma, lower_clip):\n",
    "    rng = np.random.default_rng()\n",
    "    q = rng.normal(x, sigma)\n",
    "    if np.any(q < lower_clip):\n",
    "        # Adjustment to the code provided to index the sigma vector\n",
    "        q[q < lower_clip] = truncnorm_rvs_recursive(x[q < lower_clip],\n",
    "                                                    sigma[q < lower_clip],\n",
    "                                                    lower_clip)\n",
    "\n",
    "    return q\n",
    "# Using 20000 as an artificial, arbitrary lower bound on value\n",
    "ens_df['val_s'] = truncnorm_rvs_recursive(ens_df['val_struct'],\n",
    "                                          ens_df['val_struct']*COEF_VARIATION,\n",
    "                                          20000)\n",
    "\n",
    "print('Draw values')\n",
    "\n",
    "## Commenting out the below for this case study\n",
    "\n",
    "# # Draw from the #stories distribution\n",
    "# # We do this by mapping ens_df values with STRY_DICT\n",
    "# # and passing this parameter to rng.binomial()\n",
    "# # We also need to create an array of 1s with length\n",
    "# # N_SOW * len(full_df) - i.e. len(ens_df)\n",
    "# # full_df['bg_id'].map(STRY_DICT)\n",
    "# bin_n = np.ones(len(ens_df), dtype=np.int8)\n",
    "# bin_p = ens_df[STRUCT_REF].map(STRY_DICT).values\n",
    "# # This gives us an array of 0s and 1s\n",
    "# # Based on how STRY_DICT is defined, the probability of\n",
    "# # success parameter corresponds to 1S, so we need to\n",
    "# # swap out 1 with 1S and 0 with 2S\n",
    "# stories = rng.binomial(bin_n, bin_p)\n",
    "# stories = np.where(stories == 1,\n",
    "#                    '1S',\n",
    "#                    '2S')\n",
    "\n",
    "# print('Draw stories')\n",
    "\n",
    "# # Draw from the fnd_type distribution\n",
    "# # We do the same thing as above but with\n",
    "# # the FND_DICT. This is a multinomial distribution\n",
    "# # and 0, 1, 2 correspond to B, C, S\n",
    "# # We get an array returned of the form \n",
    "# # [0, 0, 1] (if we have Slab foundation, for example)\n",
    "# # so we need to transform this into the corresponding\n",
    "# # foundation type array\n",
    "# # Can do this with fnds[fnds[0] == 1] = 'B'\n",
    "# # fnds[fnds[1]] == 1] = 'C' & fnds[fnds[2] == 1] = 'S'\n",
    "# # One way to do the mapping is by treating each\n",
    "# # row-array as a binary string and converting it\n",
    "# # to an int\n",
    "# # So you get [a, b, c] => a*2^2 + b*2^1 + c*2^0\n",
    "# # This uniquely maps to 4, 2, and 1\n",
    "# # So we can create a dict for 4: 'B', 2: 'C', and 1: 'S'\n",
    "# # and make it a pd.Series() (I think this is useful because\n",
    "# # pandas can combine this with the 1S and 2S string easily\n",
    "# # into a series and we'll need to use that bld_type\n",
    "# # for the other dicts we have)\n",
    "\n",
    "# # This is our ens_df index aligned multinomial\n",
    "# # probabilities array\n",
    "# # np.stack makes sure the dtype is correct\n",
    "# # Not sure why it is cast to object dtype if\n",
    "# # I call .values, but this works...\n",
    "\n",
    "# mult_p = np.stack(ens_df[STRUCT_REF].map(FND_DICT))\n",
    "# # This is our map of binary string/int\n",
    "# # conversions to the foundation type\n",
    "# bin_str_map = {4: 'B', 2: 'C', 1: 'S'}\n",
    "# # We need our np.ones array \n",
    "# mult_n = np.ones(len(ens_df), dtype=np.int8)\n",
    "# # Draw from mult_p\n",
    "# fnds = rng.multinomial(mult_n, mult_p)\n",
    "# # Create a series of 4, 2, and 1 from the binary strings\n",
    "# # This code accomplishes the conversion outlined in the\n",
    "# # note above and comes from this stackoverflow post\n",
    "# # https://stackoverflow.com/questions/41069825/\n",
    "# # convert-binary-01-numpy-to-integer-or-binary-string\n",
    "# fnds_ints = pd.Series(fnds.dot(2**np.arange(fnds.shape[1])[::-1]))\n",
    "# # Replace these values with the fnd_type\n",
    "# fnd_types = fnds_ints.map(bin_str_map)\n",
    "## For this case study, we use \n",
    "\n",
    "# print('Draw foundation type')\n",
    "\n",
    "# # We take fnd_types for two tasks now\n",
    "# # First, if B, it's WB type home and we\n",
    "# # combine this with stories to get the bld_type\n",
    "# # This is naccs_ddf_type \n",
    "# # We combine bld_type with fz_ddf to get hazus_ddf_type\n",
    "# # For our case study, it turns out we will use the same hazus\n",
    "# # ddf for the basement houses (_A) since no V zone houses\n",
    "# # For no basement, hazus_ddf_type does not add the _fz\n",
    "\n",
    "# # Let's get bld_type\n",
    "# # Basement type from fnd_types\n",
    "# base_types = np.where(fnd_types == 'B',\n",
    "#                       'WB',\n",
    "#                       'NB')\n",
    "\n",
    "# # Combine 1S and this\n",
    "# bld_types = pd.Series(stories) + pd.Series(base_types)\n",
    "\n",
    "# For this case study, use the below code\n",
    "# This drops the \"RES1-\" part of the occtype column\n",
    "# and keeps 1SNB, 2SNB, etc.\n",
    "ens_df['bld_types'] = ens_df['occtype'].str.split('-').str[1]\n",
    "\n",
    "# In theory, bld_type is naccs_ddf_type. No need to \n",
    "# take this storage up in practice... just refer to bld_type\n",
    "# when needed\n",
    "# For WB homes, hazus_ddf_type is bld_types + '_' + ens_df['fz_ddf']\n",
    "# For NB homes, it's bld_types\n",
    "# It makes practical sense to create a new series for this\n",
    "ens_df['hazus_types'] = np.where(ens_df['bld_types'].str[-2:] == 'WB',\n",
    "                                 ens_df['bld_types'] + '_' + ens_df['fz_ddf'],\n",
    "                                 ens_df['bld_types'])\n",
    "\n",
    "\n",
    "# We are going to use the fnd_type to draw from the\n",
    "# FFE distribution\n",
    "# Need to use np.stack to get the array of floats\n",
    "tri_params = np.stack(ens_df['found_type'].map(FFE_DICT))\n",
    "\n",
    "# Can use [:] to access like a matrix and directly input to \n",
    "# rng.triangular\n",
    "# 0, 1, and 2 are column indices corresponding to left,\n",
    "# mode, and right\n",
    "# We round this to the nearest foot\n",
    "ffes = np.round(rng.triangular(tri_params[:,0],\n",
    "                               tri_params[:,1],\n",
    "                               tri_params[:,2]))\n",
    "ens_df['ffe'] = ffes\n",
    "\n",
    "print('Generated Structure Characteristics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d7659-03e2-4c2f-bed5-5d435df1233a",
   "metadata": {},
   "source": [
    "## Estimate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d124645c-368f-4b45-b114-df8731e83aa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:20:05.132453Z",
     "iopub.status.busy": "2024-02-29T20:20:05.132312Z",
     "iopub.status.idle": "2024-02-29T20:20:17.035932Z",
     "shell.execute_reply": "2024-02-29T20:20:17.035369Z",
     "shell.execute_reply.started": "2024-02-29T20:20:05.132438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: Lower\n",
      "Adjuted depths by FFE\n",
      "\n",
      "Scenario: Mid\n",
      "Adjuted depths by FFE\n",
      "\n",
      "Scenario: Upper\n",
      "Adjuted depths by FFE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## For this case study, we're using depths as scenarios\n",
    "# Loop through each of Lower, Mid, Upper in the depths_df\n",
    "# and merge this depth_df into ens_df\n",
    "# This is hard coded (i.e. Lower/Mid) and (500) which isn't ideal\n",
    "# and I should replace these with values in the config file\n",
    "# Store this in a dictionary - it's a little easier\n",
    "ens_dfs = {}\n",
    "# Also helps to have a dictionary for the depths adjusted\n",
    "# by first floor elevation\n",
    "depth_ffes = {}\n",
    "for scen in ['Lower', 'Mid', 'Upper']:\n",
    "    print('Scenario: ' + scen)\n",
    "    # We subset to the scenario\n",
    "    depth_df = depths_df[depths_df['scen'] == scen].drop(columns=['scen'])\n",
    "    # We only need to keep properties with depth[500] > 0\n",
    "    keep_rows = depth_df['500'] > 0\n",
    "    depth_df = depth_df.loc[keep_rows]\n",
    "    # Replace 0 values with na\n",
    "    depth_df[depth_df == 0] = np.nan\n",
    "    # Let's do an inner merge so that we don't have\n",
    "    # to keep the ensemble members that correspond to \n",
    "    # 0 losses under this scenario\n",
    "    ens_dfs[scen] = ens_df.merge(depth_df, how='inner', on='fd_id')\n",
    "    # Dataframe for adjusted depths\n",
    "    # depth_df and ens_dfs\n",
    "    depth_ffes[scen] = ens_dfs[scen][RET_PERS].subtract(ens_dfs[scen]['ffe'],\n",
    "                                                        axis=0).round(1) \n",
    "    print('Adjuted depths by FFE\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "003f5629-dbc4-4da1-b408-5a0f6b96351f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:20:17.037153Z",
     "iopub.status.busy": "2024-02-29T20:20:17.036867Z",
     "iopub.status.idle": "2024-02-29T20:30:47.021593Z",
     "shell.execute_reply": "2024-02-29T20:30:47.020750Z",
     "shell.execute_reply.started": "2024-02-29T20:20:17.037130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: Lower\n",
      "Estimate Losses for NACCS & Hazus, RP: 001\n",
      "Estimate Losses for NACCS & Hazus, RP: 002\n",
      "Estimate Losses for NACCS & Hazus, RP: 005\n",
      "Estimate Losses for NACCS & Hazus, RP: 010\n",
      "Estimate Losses for NACCS & Hazus, RP: 015\n",
      "Estimate Losses for NACCS & Hazus, RP: 020\n",
      "Estimate Losses for NACCS & Hazus, RP: 025\n",
      "Estimate Losses for NACCS & Hazus, RP: 050\n",
      "Estimate Losses for NACCS & Hazus, RP: 075\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 200\n",
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Obtained Full Ensemble\n",
      "Calculated EAL\n",
      "Stored in dictionary\n",
      "\n",
      "Scenario: Mid\n",
      "Estimate Losses for NACCS & Hazus, RP: 001\n",
      "Estimate Losses for NACCS & Hazus, RP: 002\n",
      "Estimate Losses for NACCS & Hazus, RP: 005\n",
      "Estimate Losses for NACCS & Hazus, RP: 010\n",
      "Estimate Losses for NACCS & Hazus, RP: 015\n",
      "Estimate Losses for NACCS & Hazus, RP: 020\n",
      "Estimate Losses for NACCS & Hazus, RP: 025\n",
      "Estimate Losses for NACCS & Hazus, RP: 050\n",
      "Estimate Losses for NACCS & Hazus, RP: 075\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 200\n",
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Obtained Full Ensemble\n",
      "Calculated EAL\n",
      "Stored in dictionary\n",
      "\n",
      "Scenario: Upper\n",
      "Estimate Losses for NACCS & Hazus, RP: 001\n",
      "Estimate Losses for NACCS & Hazus, RP: 002\n",
      "Estimate Losses for NACCS & Hazus, RP: 005\n",
      "Estimate Losses for NACCS & Hazus, RP: 010\n",
      "Estimate Losses for NACCS & Hazus, RP: 015\n",
      "Estimate Losses for NACCS & Hazus, RP: 020\n",
      "Estimate Losses for NACCS & Hazus, RP: 025\n",
      "Estimate Losses for NACCS & Hazus, RP: 050\n",
      "Estimate Losses for NACCS & Hazus, RP: 075\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 200\n",
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Obtained Full Ensemble\n",
      "Calculated EAL\n",
      "Stored in dictionary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, we are going to loop through each return period\n",
    "# and estimate losses for NACCS and HAZUS using our helper\n",
    "# functions for each of these\n",
    "\n",
    "# We do this for each of the ens_df in ens_dfs\n",
    "for scen, ens_df in ens_dfs.items():\n",
    "    print('Scenario: ' + scen)\n",
    "    # Get the depth_ffe_df\n",
    "    depth_ffe_df = depth_ffes[scen]\n",
    "    \n",
    "    # We will store these in dictionaries with return period keys\n",
    "    hazus_loss = {}\n",
    "    naccs_loss = {}\n",
    "    \n",
    "    for rp in RET_PERS:\n",
    "        naccs_loss[rp] = est_naccs_loss(ens_df['bld_types'],\n",
    "                                        depth_ffe_df[rp],\n",
    "                                        naccs_ddfs,\n",
    "                                        NACCS_MAX_DICT)\n",
    "        hazus_loss[rp] = est_hazus_loss(ens_df['hazus_types'],\n",
    "                                        depth_ffe_df[rp],\n",
    "                                        hazus_ddfs,\n",
    "                                        HAZUS_MAX_DICT)\n",
    "    \n",
    "        print('Estimate Losses for NACCS & Hazus, RP: ' + rp)\n",
    "    \n",
    "    # Then, we convert these to dataframes\n",
    "    hazus_df = pd.DataFrame.from_dict(hazus_loss)\n",
    "    naccs_df = pd.DataFrame.from_dict(naccs_loss)\n",
    "\n",
    "    ## Normally, UNSAFE will randomly assign loss from\n",
    "    # one of the DDFs. For this case study, we are not accounting\n",
    "    # for deep uncertainty so we want to store\n",
    "    # losses from both DDFs under uncertainty\n",
    "    # The NACCS DDFs are expert defined uncertainty DDFs\n",
    "    # which is more defensible for use in the case study (plus\n",
    "    # it covers our case study region). We will repeat all the\n",
    "    # analysis from the main results w/ the FEMA DDFs under\n",
    "    # uncertainty, which are heuristically defined. \n",
    "    \n",
    "    # # And we use a binomial rv to determine if we use the hazus or naccs\n",
    "    # # loss estimate for a particular return period\n",
    "    # # Binomial\n",
    "    # random_loss = rng.binomial(1, .5, size=len(ens_df))\n",
    "    \n",
    "    # # Get indices to take from each df\n",
    "    # hazus_ind = (random_loss == 0)\n",
    "    # naccs_ind = (random_loss == 1)\n",
    "    \n",
    "    # # Concat subsetted dataframes\n",
    "    # losses_df = pd.concat([hazus_df.loc[hazus_ind],\n",
    "    #                        naccs_df.loc[naccs_ind]], axis=0).sort_index()\n",
    "    # # Rename columns to make it more clear what this is\n",
    "    # losses_df.columns = ['rel_dam_' + x for x in losses_df.columns]\n",
    "    \n",
    "    # # Add a column indicating NACCS or Hazus ddf\n",
    "    # losses_df.loc[hazus_ind, 'ddf'] = 'HAZUS'\n",
    "    # losses_df.loc[naccs_ind, 'ddf'] = 'NACCS'\n",
    "    \n",
    "    # print('Randomly assigned NACCS or HAZUS Loss')\n",
    "\n",
    "    ## So, we define the losses_df by concatenating the hazus & naccs\n",
    "    # data frames along their columns, after fixing their column\n",
    "    # names\n",
    "    hazus_df.columns = ['hazus_rel_dam_' + x for x in hazus_df.columns]\n",
    "    naccs_df.columns = ['naccs_rel_dam_' + x for x in naccs_df.columns]\n",
    "    losses_df = pd.concat([hazus_df, naccs_df], axis=1)\n",
    "\n",
    "    # Now we concat these with ens_df, stories, fnd_type,\n",
    "    # ffe, structure value, and depth_ffe_df\n",
    "    depth_ffe = pd.DataFrame.from_dict(depth_ffe_df)\n",
    "    \n",
    "    # Add clearer column names\n",
    "    depth_ffe.columns = ['depth_ffe_' + x for x in depth_ffe.columns]\n",
    "    \n",
    "    ## Slight modification for our case study\n",
    "    # Concat for our full ensemble\n",
    "    # ens_df = pd.concat([ens_df, losses_df, depth_ffe,\n",
    "    #                     pd.Series(stories, name='stories'),\n",
    "    #                     pd.Series(fnd_types, name='fnd_type'),\n",
    "    #                     pd.Series(ffes, name='ffe'),\n",
    "    #                     pd.Series(values, name='val_s')],\n",
    "    #                    axis=1)\n",
    "    ## For our case study, ens_df contains occtype & \n",
    "    # found_ht, so don't need to add structure characteristics\n",
    "    # back in\n",
    "    ens_df = pd.concat([ens_df, losses_df, depth_ffe],\n",
    "                       axis=1)\n",
    "    \n",
    "    # Get relative damage columns\n",
    "    rel_cols = [x for x in ens_df.columns if 'rel_dam' in x]\n",
    "    # For each relative damage column, scale by val_s, the structure\n",
    "    # value realization\n",
    "    # We need to do this for naccs & hazus prefixes\n",
    "    for col in rel_cols:\n",
    "        prefix = col.split('_')[0]\n",
    "        rp = col.split('_')[-1]\n",
    "        ens_df[prefix + '_loss_' + rp] = ens_df[col]*ens_df['val_s']\n",
    "    \n",
    "    print('Obtained Full Ensemble')\n",
    "\n",
    "    # Now we calculate EAL\n",
    "    # We will use trapezoidal approximation for this\n",
    "    # Using trapezoid method and adding bin of lowest probability\n",
    "    # events to obtain expected annual \n",
    "    \n",
    "    # We make a list of our loss columns\n",
    "    # This is easier to do splitting by prefix\n",
    "    hazus_loss_list = ['hazus_loss_' + x for x in RET_PERS]\n",
    "    naccs_loss_list = ['naccs_loss_' + x for x in RET_PERS]\n",
    "    # As well as the corresponding probabilities\n",
    "    p_rp_list = [round(1/int(x), 4) for x in RET_PERS]\n",
    "    \n",
    "    # Then we create an empty series\n",
    "    # Two, for hazus & naccs loss estimates\n",
    "    eal_hazus = pd.Series(index=ens_df.index).fillna(0)\n",
    "    eal_naccs = pd.Series(index=ens_df.index).fillna(0)\n",
    "    \n",
    "    # We loop through our loss list and apply the \n",
    "    # trapezoidal approximation\n",
    "    for i in range(len(hazus_loss_list) - 1):\n",
    "        loss1_hazus = ens_df[hazus_loss_list[i]]\n",
    "        loss2_hazus = ens_df[hazus_loss_list[i+1]]\n",
    "        loss1_naccs = ens_df[naccs_loss_list[i]]\n",
    "        loss2_naccs = ens_df[naccs_loss_list[i+1]]\n",
    "        rp1 = p_rp_list[i]\n",
    "        rp2 = p_rp_list[i+1]\n",
    "        # We add each approximation\n",
    "        eal_hazus += (loss1_hazus + loss2_hazus)*(rp1-rp2)/2\n",
    "        eal_naccs += (loss1_naccs + loss2_naccs)*(rp1-rp2)/2\n",
    "    # This is the final trapezoid to add in\n",
    "    final_eal_hazus = eal_hazus + ens_df[hazus_loss_list[-1]]*p_rp_list[-1]\n",
    "    final_eal_naccs = eal_naccs + ens_df[naccs_loss_list[-1]]*p_rp_list[-1]\n",
    "    print('Calculated EAL')\n",
    "    # Add eal columns to our dataframe\n",
    "    ens_df = pd.concat([ens_df, pd.Series(final_eal_hazus, name='hazus_eal')],\n",
    "                       axis=1)\n",
    "    ens_df = pd.concat([ens_df, pd.Series(final_eal_naccs, name='naccs_eal')],\n",
    "                       axis=1)\n",
    "    \n",
    "    # Let's also get the SOW index - start at 0\n",
    "    sow_ind = np.arange(len(ens_df))%N_SOW\n",
    "    ens_df = pd.concat([ens_df, pd.Series(sow_ind, name='sow_ind')], axis=1)\n",
    "\n",
    "    # Put this back in ens_dfs[scen]\n",
    "    ens_dfs[scen] = ens_df\n",
    "    print('Stored in dictionary\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a51a9926-7c09-4d66-b60d-92a3ea946afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:30:47.022658Z",
     "iopub.status.busy": "2024-02-29T20:30:47.022515Z",
     "iopub.status.idle": "2024-02-29T20:31:55.447143Z",
     "shell.execute_reply": "2024-02-29T20:31:55.446005Z",
     "shell.execute_reply.started": "2024-02-29T20:30:47.022643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out our ensemble dfs\n",
    "ens_out_filep = join(FO, 'ensemble.pqt')\n",
    "prepare_saving(ens_out_filep)\n",
    "ens_dfs['Lower'].to_parquet(join(FO, 'ensemble_Lower.pqt'))\n",
    "ens_dfs['Mid'].to_parquet(join(FO, 'ensemble_Mid.pqt'))\n",
    "ens_dfs['Upper'].to_parquet(join(FO, 'ensemble_Upper.pqt'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd01cab0-e55e-4fe4-aa81-88afbb80a320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T19:57:33.667724Z",
     "iopub.status.busy": "2023-12-27T19:57:33.665880Z",
     "iopub.status.idle": "2023-12-27T19:57:33.708146Z",
     "shell.execute_reply": "2023-12-27T19:57:33.706686Z",
     "shell.execute_reply.started": "2023-12-27T19:57:33.667656Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6922fd9-d241-449c-a0c2-297b4cf56827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T19:57:33.946548Z",
     "iopub.status.busy": "2023-12-27T19:57:33.946062Z",
     "iopub.status.idle": "2023-12-27T19:57:35.417551Z",
     "shell.execute_reply": "2023-12-27T19:57:35.416448Z",
     "shell.execute_reply.started": "2023-12-27T19:57:33.946500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c48690-c6db-4531-b975-4c0f77a2c578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T19:57:35.418634Z",
     "iopub.status.busy": "2023-12-27T19:57:35.418356Z",
     "iopub.status.idle": "2023-12-27T19:57:35.434141Z",
     "shell.execute_reply": "2023-12-27T19:57:35.433024Z",
     "shell.execute_reply.started": "2023-12-27T19:57:35.418617Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'\n",
    "\n",
    "# I think it also could make sense to pass in scenario and\n",
    "# ddf type as arguments. For main results\n",
    "# we're using 'mid' and 'naccs' but for generating\n",
    "# our sensitivity analysis results we will need to pass\n",
    "# in the other scenarios and 'hazus'\n",
    "# Very well suited for snakemake :) \n",
    "SCENARIO = 'Mid'\n",
    "DDF_TYPE = 'naccs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90042fbe-f2e4-4c34-bc71-c156fab78c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T19:57:35.434815Z",
     "iopub.status.busy": "2023-12-27T19:57:35.434661Z",
     "iopub.status.idle": "2023-12-27T19:57:35.474472Z",
     "shell.execute_reply": "2023-12-27T19:57:35.473031Z",
     "shell.execute_reply.started": "2023-12-27T19:57:35.434800Z"
    }
   },
   "outputs": [],
   "source": [
    "# This notebook evaluates optimal elevation \n",
    "# per house across an ensemble\n",
    "# Optimal elevation height for each house is returned based on\n",
    "# the ensemble generated in benchmark_ensemble\n",
    "# This is estimated w/o uncertainty (the benchmark estimates)\n",
    "# as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3319852-a49d-47d9-9d18-a95c8768c53f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T19:57:35.475816Z",
     "iopub.status.busy": "2023-12-27T19:57:35.475639Z",
     "iopub.status.idle": "2023-12-27T19:57:35.512282Z",
     "shell.execute_reply": "2023-12-27T19:57:35.511376Z",
     "shell.execute_reply.started": "2023-12-27T19:57:35.475800Z"
    }
   },
   "outputs": [],
   "source": [
    "# The gist of this is to evaluate costs & benefits for each increment\n",
    "# of elevation from 3 to 10 feet (I think it's up to 10 feet - will\n",
    "# double check)\n",
    "# There are values for costs from the FEMA report \n",
    "# that go in the config file\n",
    "# This gets multipled by sq ft of the building\n",
    "# There is uncertainty in the inflation costs (relative to 2009 when\n",
    "# the report was published) and the fixed costs (CLARA and\n",
    "# NACCS reports provide us with a range)\n",
    "# All of these values are in the config.yaml, and \n",
    "# the data for these estimates is in the resources/\n",
    "# directory of the project repository\n",
    "# For benefits, we'll do it \"through\" the DDF by moving along the\n",
    "# x-axis of the DDF. We do this by changing the FFE, which we are\n",
    "# treating under uncertainty. We are not treating the elevation\n",
    "# as an uncertain amount for this case study. We need\n",
    "# to estimate the losses with the new first-floor elevation adjusted\n",
    "# height under uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879497b-4844-47b6-b950-8bdfb9dd1bd6",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73e91616-ed1b-4ffc-b21a-a01991161cbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T20:34:40.273822Z",
     "iopub.status.busy": "2023-12-27T20:34:40.273270Z",
     "iopub.status.idle": "2023-12-27T20:34:42.537555Z",
     "shell.execute_reply": "2023-12-27T20:34:42.535952Z",
     "shell.execute_reply.started": "2023-12-27T20:34:40.273774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the mid scenario data\n",
    "# There are only a few columns we need\n",
    "ens_filep = join(FO, 'ensemble_' + SCENARIO + '.pqt')\n",
    "ens_df = pd.read_parquet(ens_filep)\n",
    "\n",
    "# Also load the benchmark data\n",
    "# There are only a few columns we need\n",
    "bench_df = pd.read_parquet(join(FO, 'benchmark_loss.pqt'))\n",
    "\n",
    "# Everything that we do here is based on the ensemble values\n",
    "# That means we take the ffe variable in our ensemble df\n",
    "# and adjust it by the heightening amount, re-estimate losses\n",
    "# across all return periods, and re-estimate eal\n",
    "# In fact, since depths are \"fixed\" in our case study\n",
    "# we don't have to adjust the ffe variable, and can instead\n",
    "# adjust the depth_ffe_* columns\n",
    "\n",
    "# These are shared columns for subsetting\n",
    "# We need found_type because it is used in\n",
    "# elevation cost estimation\n",
    "# We need sqft because it's a key variable for\n",
    "# elevation cost estimation\n",
    "# We need bldgtype for elevation cost estimation, too\n",
    "sub_cols = ['fd_id', 'found_type', 'sqft', 'bldgtype']\n",
    "# We need to add depth_ffe_* columns \n",
    "depth_ffe_cols = ['depth_ffe_' + x for x in RET_PERS]\n",
    "sub_cols = sub_cols + depth_ffe_cols\n",
    "\n",
    "# These are columns for the benchmark df\n",
    "# We need fz_ddf for DDF estimation\n",
    "# We need eal for estimating avoided losses\n",
    "bench_cols = ['fz_ddf', 'eal'] + sub_cols\n",
    "\n",
    "# We need to use DDF_TYPE argument to add either bld_types (naccs)\n",
    "# or hazus_types to our ens_cols list\n",
    "ddf_col = 'bld_types' if DDF_TYPE == 'naccs' else 'hazus_types'\n",
    "\n",
    "# We also need the eal col for comparing against eals w/ elevation\n",
    "eal_col = 'naccs_eal' if DDF_TYPE == 'naccs' else 'haz_eal'\n",
    "\n",
    "# These are general columns for the ensemble df\n",
    "# We need to keep track of SOW, and val_s is needed for\n",
    "# estimating avoided losses in a particular SOW\n",
    "ens_cols = ['val_s', 'sow_ind', ddf_col, eal_col] + sub_cols\n",
    "\n",
    "# Subset ens_df and bench_df on their column lists\n",
    "ens_df = ens_df.loc[:,ens_cols]\n",
    "bench_df = bench_df.loc[:,bench_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "372f66d7-aef5-4dd2-945a-13f7f33037e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T20:34:47.087118Z",
     "iopub.status.busy": "2023-12-27T20:34:47.086838Z",
     "iopub.status.idle": "2023-12-27T20:34:47.150841Z",
     "shell.execute_reply": "2023-12-27T20:34:47.148919Z",
     "shell.execute_reply.started": "2023-12-27T20:34:47.087097Z"
    }
   },
   "outputs": [],
   "source": [
    "# We'll need DDFs for estimating benefits\n",
    "# Load DDFs\n",
    "naccs_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'naccs_ddfs.pqt'))\n",
    "hazus_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs.pqt'))\n",
    "hazus_nounc = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs_nounc.pqt'))\n",
    "\n",
    "# Load helper dictionaries\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus.json'), 'r') as fp:\n",
    "    HAZUS_MAX_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus_nounc.json'), 'r') as fp:\n",
    "    HAZUS_MAX_NOUNC_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'naccs.json'), 'r') as fp:\n",
    "    NACCS_MAX_DICT = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1f8874e-27d5-48d8-9389-e1d7ae36e506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T20:34:55.504740Z",
     "iopub.status.busy": "2023-12-27T20:34:55.504176Z",
     "iopub.status.idle": "2023-12-27T20:34:55.601232Z",
     "shell.execute_reply": "2023-12-27T20:34:55.599563Z",
     "shell.execute_reply.started": "2023-12-27T20:34:55.504691Z"
    }
   },
   "outputs": [],
   "source": [
    "# From the config file take the inflation values, heightening values, \n",
    "# and fixed cost values. For heightening, we need to linearly interpolate\n",
    "# for our structure specific heightening cost estimates. For the others, \n",
    "# we need to generate N_SOW length realizations. We can pre-populate\n",
    "# a cost dataframe with this information and for each \n",
    "# foundation type, heightening combo, we will have\n",
    "# the SOW specific cost estimate to apply to the structure. \n",
    "# The costs are applied against the expected annual losses\n",
    "# to figure out the optimal heightening. We do not need to include\n",
    "# discount rates at this step since these are uniformly applied\n",
    "# in our case study since all the elevations are assumed to occur\n",
    "# at the same time. Time-based elevations that account for changing\n",
    "# cost estimates and discount rates is an extension of this work. \n",
    "\n",
    "# Do the interpolation on the elev costs\n",
    "# To get basement/bldgtype as multiindex from our dict\n",
    "elev_cost_df = pd.DataFrame.from_dict(ELEV_COST_DICT).stack().to_frame()\n",
    "# To break out the lists into columns\n",
    "# Column names as the foot value (as int) \n",
    "e_c_df = pd.DataFrame(elev_cost_df[0].values.tolist(),\n",
    "                      index=elev_cost_df.index,\n",
    "                      columns=[2, 4, 8]).reset_index()\n",
    "\n",
    "# Melt and rename to get ready for linear interpolation between feet\n",
    "e_c_df = e_c_df.melt(id_vars=['level_0', 'level_1'], value_vars=[2, 4, 8])\n",
    "e_c_df.columns = ['fnd_type', 'bldgtype', 'elev_ht', 'cost_sqft']\n",
    "\n",
    "# Loop through fnd_type, bldgtype groups\n",
    "# Add missing foot values and interpolate using\n",
    "# spline of order 1 to get values filled\n",
    "# past the 8 foot value and up to 10\n",
    "# Store each interpolated dataframe in a list and concat at the end\n",
    "elev_dfs = []\n",
    "for fnd_bldg, df_sub in e_c_df.groupby(['fnd_type', 'bldgtype']):\n",
    "    # keep track of foundation type and bldgtype\n",
    "    fnd = fnd_bldg[0]\n",
    "    bld = fnd_bldg[1]\n",
    "\n",
    "    # use elev ht as index and get a series of costs\n",
    "    elevs = df_sub.set_index('elev_ht')['cost_sqft']\n",
    "    # get the elevations from 2 to 3 feet that we are missing\n",
    "    missing_elevs = [x for x in np.arange(2, 11) if x not in elevs.index]\n",
    "    # combine elevs and missing elevs\n",
    "    elevs_f = pd.concat([elevs, pd.DataFrame(index=pd.Index(missing_elevs))])\n",
    "    # sort index and interpolate\n",
    "    elevs_f = elevs_f.sort_index().interpolate('spline', order=1).round(1)\n",
    "\n",
    "    # We consider elevation from 3 to 10 feet only\n",
    "    elevs_f = elevs_f.loc[3:10]\n",
    "\n",
    "    # Reset index and rename columns\n",
    "    elevs_f = elevs_f.reset_index()\n",
    "    elevs_f.columns = ['elev_ht', 'cost_sqft']\n",
    "    # Add back fnd_type and bldgtype\n",
    "    # using first character as capital letter\n",
    "    elevs_f['fnd_type'] = fnd[0].upper()\n",
    "    elevs_f['bldgtype'] = bld[0].upper()\n",
    "\n",
    "    elev_dfs.append(elevs_f)\n",
    "# Final cost per sqft dataframe\n",
    "elev_costs = pd.concat(elev_dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Sample N_SOW from uniform(CPI_LOW, CPI_HIGH)\n",
    "# Sample N_SOW from uniform (ELEV_FIX_LOW, ELEV_FIX_HIGH)\n",
    "rng = np.random.default_rng()\n",
    "construction_infl = rng.uniform(CPI_LOW, CPI_HIGH, N_SOW)\n",
    "fixed = rng.uniform(ELEV_FIX_LOW, ELEV_FIX_HIGH, N_SOW)\n",
    "\n",
    "# Get the cost dataframe for each sow_ind \n",
    "# Columns are sow_ind, bldgtype, heightening, cost\n",
    "# We need to multiply each cost_sqft value in elev_costs\n",
    "# by each element of construction_infl and make sure this is\n",
    "# indexed by the sow. Then, we need to add the fixed cost\n",
    "# corresponding to that sow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17053fb2-3b3d-456b-926f-b3b62ce2cb39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T20:35:04.127760Z",
     "iopub.status.busy": "2023-12-27T20:35:04.127243Z",
     "iopub.status.idle": "2023-12-27T20:35:04.382483Z",
     "shell.execute_reply": "2023-12-27T20:35:04.380950Z",
     "shell.execute_reply.started": "2023-12-27T20:35:04.127715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Repeat the elev_costs df so that each entry (ht, cst, types) has\n",
    "# N_SOW rows\n",
    "e_c_ens = elev_costs.loc[np.repeat(elev_costs.index, N_SOW)]\n",
    "e_c_ens = e_c_ens.reset_index(drop=True)\n",
    "# Then repeat the construction_infl and fixed series len(elev_costs)\n",
    "# times. Do this via tiling (i.e. repeat the whole array not \n",
    "# the elements) \n",
    "c_infl_full = np.tile(construction_infl, len(elev_costs))\n",
    "fixed_full = np.tile(fixed, len(elev_costs))\n",
    "\n",
    "# Now create new column in e_c_ens for\n",
    "# cost_sqft*c_infl_full and fixed_full\n",
    "e_c_ens['cost_sqft_unc'] = e_c_ens['cost_sqft']*c_infl_full\n",
    "e_c_ens['cost_fix_unc'] = fixed_full\n",
    "\n",
    "# Then get the sow_ind for the e_c_ens dataframe\n",
    "sow_ind = np.arange(len(e_c_ens))%N_SOW\n",
    "e_c_ens = pd.concat([e_c_ens, pd.Series(sow_ind, name='sow_ind')], axis=1)\n",
    "\n",
    "# Write out the elevation cost ensemble\n",
    "elev_ens_filep = join(EXP_DIR_I, FIPS, 'elev_ens.pqt')\n",
    "prepare_saving(elev_ens_filep)\n",
    "e_c_ens.to_parquet(elev_ens_filep)\n",
    "\n",
    "# We will also do the cost_sqft_unc*sqft + cost_fix_unc, indexed\n",
    "# on sow_ind for each structure across SOWs, but will only write out\n",
    "# the costs associated with the optimal elevation\n",
    "# Since the elevation cost ensemble is written out, it's always\n",
    "# accessible to inspect elevation costs for any \n",
    "# eligible heightening for homes in/across SOWs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb11a9-ce60-4edb-9ff1-c3043b0905d1",
   "metadata": {},
   "source": [
    "# Optimal Elevation Under Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa6f43-0d17-4881-9d4f-81f58d0f92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do this, we loop through each of the possible heightenings\n",
    "# This effectively creates a new first floor elevation for each home\n",
    "# and then we repeat the loss estimation from benchmark_ensemble\n",
    "# for each structure\n",
    "# In addition to this, we estimate the costs for each heightening\n",
    "# This is done on a state of the world basis since it's informed\n",
    "# by macroeconomic conditions. What this means is we generate\n",
    "# a N_SOW length vector of the inflation value & the fixed cost\n",
    "# value that is used for the elevation cost. The per sq. ft. costs\n",
    "# are not changing across SOWs\n",
    "# It seems like because DDFs are monotonic increasing with depth\n",
    "# and costs are monotonic increasing with heightening (due to\n",
    "# linear interpolation), you could use an approach like binary\n",
    "# sort on the heightening possibilities to save some search time\n",
    "# but it won't take that long to loop through the 8 possible\n",
    "# heightenings we're considering. Because the cost data is less than\n",
    "# ideal, I don't want to introduce an approach like binary sort that\n",
    "# works only on the cost data we have (in reality, cost estimates\n",
    "# may not be well suited to this approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8dc4f8c5-b288-4675-9a0c-cbaf54036312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T20:35:06.241346Z",
     "iopub.status.busy": "2023-12-27T20:35:06.241061Z",
     "iopub.status.idle": "2023-12-27T20:35:06.442971Z",
     "shell.execute_reply": "2023-12-27T20:35:06.441384Z",
     "shell.execute_reply.started": "2023-12-27T20:35:06.241322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get fnd_type variable for ens_df that corresponds to B and S\n",
    "# where crawl space (C) from found_type gets classified as B\n",
    "# This is needed for a future step\n",
    "ens_df['fnd_type'] = np.where(ens_df['found_type'] == 'S',\n",
    "                              'S', \n",
    "                              'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "85d2ca71-514e-4b78-b4d7-9d0ddbb3996a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T21:32:07.295623Z",
     "iopub.status.busy": "2023-12-27T21:32:07.295399Z",
     "iopub.status.idle": "2023-12-27T21:35:50.745433Z",
     "shell.execute_reply": "2023-12-27T21:35:50.744606Z",
     "shell.execute_reply.started": "2023-12-27T21:32:07.295606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 3 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 4 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 5 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 6 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 7 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 8 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 9 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 10 feet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through possible heightenings of 3 through 10 feet, inclusive\n",
    "# For each of these, add that value to each column in depth_ffe_*\n",
    "# Then, go through the procedures from benchmark_ensemble\n",
    "# to calculate losses per return period and ultimately the eal\n",
    "# Then, we compare this eal to the non-elevated eal, which is \n",
    "# stored in the reference \"eal_col\"\n",
    "# We then take the subset of \n",
    "# fd_id, sow_ind, fnd_type, bldgtype, sqft, reduced_eal\n",
    "# and merge it with e_c_ens. (sow_ind, fnd_type, bldgtype)\n",
    "# Take reduced_eal/(sqft*cost_sqft_unc + cost_fix_unc) and store\n",
    "# it as bcr. Groupby on fd_id and calculate the mean bcr.\n",
    "# Store this as a series with name corresponding to the amount\n",
    "# of heightening and index corresponding to fd_id,\n",
    "# and store that series in a list\n",
    "# I think we also want to store the reduced_eal and\n",
    "# the costs for each sow. \n",
    "# After looping through all of these, we can concat our list\n",
    "# into a dataframe and figure out which column corresponds\n",
    "# to the highest bcr for each fd_id. I think we can do this\n",
    "# using df.idxmax(axis=\"columns\") if we concat on columns\n",
    "# Finally, we use the corresponding\n",
    "# heightening value to match up each fd_id to its\n",
    "# avoided loss, heightening, and expected costs. The BCR needs to \n",
    "# be adjusted in the allocate funding procedure later on by different\n",
    "# discount rate projections. I think we need to discount the \n",
    "# reduced_eal in each SOW AND divide each of those by costs again\n",
    "# to get the correct expected BCR. But you don't need to do \n",
    "# discounting to find the optimal elevation. \n",
    "\n",
    "# \n",
    "\n",
    "# List for series of mean bcr at each heightening\n",
    "h_list = []\n",
    "\n",
    "for h in np.arange(3, 11):\n",
    "    # Adjust depth_ffe_* columns by h\n",
    "    # We substract h because these are depths relative to first floor\n",
    "    # and now the first floor is higher\n",
    "    depth_ffe_df = ens_df.loc[:,depth_ffe_cols] - h\n",
    "    # Remove 'depth_ffe_' part from the column\n",
    "    depth_ffe_df.columns = [x.split('_')[-1] for x in depth_ffe_df.columns]\n",
    "    \n",
    "    # We will store losses in dictionaries with return period keys\n",
    "    elev_losses = {}\n",
    "    \n",
    "    for rp in RET_PERS:\n",
    "        if DDF_TYPE == 'naccs':\n",
    "            elev_losses[rp] = est_naccs_loss(ens_df['bld_types'],\n",
    "                                             depth_ffe_df[rp],\n",
    "                                             naccs_ddfs,\n",
    "                                             NACCS_MAX_DICT)\n",
    "        else:\n",
    "            elev_losses[rp] = est_hazus_loss(ens_df['hazus_types'],\n",
    "                                             depth_ffe_df[rp],\n",
    "                                             hazus_ddfs,\n",
    "                                             HAZUS_MAX_DICT)\n",
    "    \n",
    "        print('Estimate Losses for Elevated Home, RP: ' + rp)\n",
    "    \n",
    "    # Then, we convert these to dataframes\n",
    "    loss_df = pd.DataFrame.from_dict(elev_losses)\n",
    "    \n",
    "    # For each relative damage column, scale by val_s, the structure\n",
    "    # value realization\n",
    "    # loss_df and ens_df are index aligned, so this works\n",
    "    for col in loss_df.columns:\n",
    "        loss_df['loss_' + col] = loss_df[col]*ens_df['val_s']\n",
    "    \n",
    "    # We make a list of our loss columns\n",
    "    loss_list = ['loss_' + x for x in RET_PERS]\n",
    "    # As well as the corresponding probabilities\n",
    "    p_rp_list = [round(1/int(x), 4) for x in RET_PERS]\n",
    "    \n",
    "    # Then we create an empty series\n",
    "    eal_elev = pd.Series(index=loss_df.index).fillna(0)\n",
    "    \n",
    "    # We loop through our loss list and apply the \n",
    "    # trapezoidal approximation\n",
    "    for i in range(len(loss_list) - 1):\n",
    "        loss1 = loss_df[loss_list[i]]\n",
    "        loss2 = loss_df[loss_list[i+1]]\n",
    "        rp1 = p_rp_list[i]\n",
    "        rp2 = p_rp_list[i+1]\n",
    "        # We add each approximation\n",
    "        eal_elev += (loss1 + loss2)*(rp1-rp2)/2\n",
    "    # This is the final trapezoid to add in\n",
    "    final_eal = eal_elev + loss_df[loss_list[-1]]*p_rp_list[-1]\n",
    "    print('Calculated EAL')\n",
    "\n",
    "    # Calculate avoided losses and add to ens_df\n",
    "    ens_df['eal_avoid_' + str(h)] = ens_df[eal_col] - final_eal\n",
    "    \n",
    "    # Merge e_c_ens on subset of ens_df columns to figure out\n",
    "    # the elevation cost and get this into ens_df\n",
    "    ens_sub = ens_df[['fd_id', 'sow_ind', 'fnd_type',\n",
    "                      'bldgtype', 'sqft']].copy()\n",
    "    \n",
    "    # Also subset e_c_ens for the correct heightening\n",
    "    # Don't need cost_sqft for this either\n",
    "    e_c_ens_sub = e_c_ens[e_c_ens['elev_ht'] == h].drop(columns=['elev_ht',\n",
    "                                                                 'cost_sqft'])\n",
    "    \n",
    "    # Merge on sow, fnd_type, bldgtype\n",
    "    e_c_merge = ens_sub.merge(e_c_ens_sub,\n",
    "                              on=['fnd_type', 'bldgtype', 'sow_ind'])\n",
    "    \n",
    "    # Get costs\n",
    "    costs = (e_c_merge['sqft']*e_c_merge['cost_sqft_unc']\n",
    "             + e_c_merge['cost_fix_unc'])\n",
    "    ens_df['elev_cost_' + str(h)] = costs\n",
    "    \n",
    "    # Now we have the avoided loss and elev cost for this level of\n",
    "    # heightening stored in ens_df\n",
    "    # It also helps to do some side calculations to save some time\n",
    "    # later in obtaining the optimal level of heightening\n",
    "    # Get the ratio of eal_avoid_str(h) to elev_cost_str(h)\n",
    "    # Groupby on fd_id and take the mean\n",
    "    # Store this as a series with name corresponding to the amount\n",
    "    # of heightening and index corresponding to fd_id,\n",
    "    # and store that series in a list\n",
    "    ens_df['bcr_part_' + str(h)] = (ens_df['eal_avoid_' + str(h)]\n",
    "                                    /ens_df['elev_cost_' + str(h)])\n",
    "    \n",
    "    bcrs = ens_df.groupby(['fd_id'])['bcr_part_' + str(h)].mean()\n",
    "    h_list.append(bcrs)\n",
    "\n",
    "    print('Calculations done for heightening by ' + str(h) + ' feet\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f399f4f7-b1d0-4b8d-a392-6f849c7e31e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T21:43:35.312688Z",
     "iopub.status.busy": "2023-12-27T21:43:35.312165Z",
     "iopub.status.idle": "2023-12-27T21:43:35.372837Z",
     "shell.execute_reply": "2023-12-27T21:43:35.371415Z",
     "shell.execute_reply.started": "2023-12-27T21:43:35.312641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the dataframe of mean bcr across SOWs for each structure\n",
    "# for each heightening. Find the heightening for each structure\n",
    "# that leads to the max mean bcr, and write this out to a file. \n",
    "# When we do the full bcr estimation later, we will loop through\n",
    "# each value in this series, subset the ens_df based on\n",
    "# fd_id with that bcr_part_h as their max mean bcr, and then\n",
    "# do the full discounting and recalculation of BCR. You need\n",
    "# to use the discount rate chain that corresponds to each SOW\n",
    "# to discount the avoided losses correctly\n",
    "# and you need to divide that by the costs in that SOW\n",
    "# THEN you can calculate our objectives like net benefits and\n",
    "# check conditions like BCR > 1. \n",
    "opt_elev = pd.concat(h_list, axis=1)\n",
    "opt_elev['opt_elev'] = bcr_parts.idxmax('columns')\n",
    "bcr_out_filep = join(FO, 'opt_height.pqt')\n",
    "prepare_saving(bcr_out_filep)\n",
    "opt_elev.to_parquet(bcr_out_filep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

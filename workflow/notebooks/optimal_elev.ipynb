{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd01cab0-e55e-4fe4-aa81-88afbb80a320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T17:14:16.272316Z",
     "iopub.status.busy": "2023-12-27T17:14:16.271758Z",
     "iopub.status.idle": "2023-12-27T17:14:16.834471Z",
     "shell.execute_reply": "2023-12-27T17:14:16.832894Z",
     "shell.execute_reply.started": "2023-12-27T17:14:16.272262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6922fd9-d241-449c-a0c2-297b4cf56827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T17:14:16.837051Z",
     "iopub.status.busy": "2023-12-27T17:14:16.836581Z",
     "iopub.status.idle": "2023-12-27T17:14:16.874856Z",
     "shell.execute_reply": "2023-12-27T17:14:16.873419Z",
     "shell.execute_reply.started": "2023-12-27T17:14:16.837002Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c48690-c6db-4531-b975-4c0f77a2c578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T17:14:16.877853Z",
     "iopub.status.busy": "2023-12-27T17:14:16.877363Z",
     "iopub.status.idle": "2023-12-27T17:14:16.912361Z",
     "shell.execute_reply": "2023-12-27T17:14:16.910978Z",
     "shell.execute_reply.started": "2023-12-27T17:14:16.877805Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'\n",
    "\n",
    "# I think it also could make sense to pass in scenario and\n",
    "# ddf type as arguments. For main results\n",
    "# we're using 'mid' and 'naccs' but for generating\n",
    "# our sensitivity analysis results we will need to pass\n",
    "# in the other scenarios and 'hazus'\n",
    "# Very well suited for snakemake :) \n",
    "SCENARIO = 'Mid'\n",
    "DDF_TYPE = 'naccs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90042fbe-f2e4-4c34-bc71-c156fab78c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T17:14:17.607004Z",
     "iopub.status.busy": "2023-12-27T17:14:17.606470Z",
     "iopub.status.idle": "2023-12-27T17:14:17.638247Z",
     "shell.execute_reply": "2023-12-27T17:14:17.636468Z",
     "shell.execute_reply.started": "2023-12-27T17:14:17.606956Z"
    }
   },
   "outputs": [],
   "source": [
    "# This notebook evaluates optimal elevation \n",
    "# per house across an ensemble\n",
    "# Optimal elevation height for each house is returned based on\n",
    "# the ensemble generated in benchmark_ensemble\n",
    "# This is estimated w/o uncertainty (the benchmark estimates)\n",
    "# as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3319852-a49d-47d9-9d18-a95c8768c53f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T17:14:18.023272Z",
     "iopub.status.busy": "2023-12-27T17:14:18.022088Z",
     "iopub.status.idle": "2023-12-27T17:14:18.054168Z",
     "shell.execute_reply": "2023-12-27T17:14:18.052405Z",
     "shell.execute_reply.started": "2023-12-27T17:14:18.023201Z"
    }
   },
   "outputs": [],
   "source": [
    "# The gist of this is to evaluate costs & benefits for each increment\n",
    "# of elevation from 3 to 10 feet (I think it's up to 10 feet - will\n",
    "# double check)\n",
    "# There are values for costs from the FEMA report \n",
    "# that go in the config file\n",
    "# This gets multipled by sq ft of the building\n",
    "# There is uncertainty in the inflation costs (relative to 2009 when\n",
    "# the report was published) and the fixed costs (CLARA and\n",
    "# NACCS reports provide us with a range)\n",
    "# All of these values are in the config.yaml, and \n",
    "# the data for these estimates is in the resources/\n",
    "# directory of the project repository\n",
    "# For benefits, we'll do it \"through\" the DDF by moving along the\n",
    "# x-axis of the DDF. We do this by changing the FFE, which we are\n",
    "# treating under uncertainty. We are not treating the elevation\n",
    "# as an uncertain amount for this case study. We need\n",
    "# to estimate the losses with the new first-floor elevation adjusted\n",
    "# height under uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879497b-4844-47b6-b950-8bdfb9dd1bd6",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73e91616-ed1b-4ffc-b21a-a01991161cbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T17:31:04.773247Z",
     "iopub.status.busy": "2023-12-27T17:31:04.772687Z",
     "iopub.status.idle": "2023-12-27T17:31:06.324969Z",
     "shell.execute_reply": "2023-12-27T17:31:06.323454Z",
     "shell.execute_reply.started": "2023-12-27T17:31:04.773195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the mid scenario data\n",
    "# There are only a few columns we need\n",
    "ens_filep = join(FO, 'ensemble_' + SCENARIO + '.pqt')\n",
    "ens_df = pd.read_parquet(ens_filep)\n",
    "\n",
    "# Also load the benchmark data\n",
    "# There are only a few columns we need\n",
    "bench_df = pd.read_parquet(join(FO, 'benchmark_loss.pqt'))\n",
    "\n",
    "# Everything that we do here is based on the ensemble values\n",
    "# That means we take the ffe variable in our ensemble df\n",
    "# and adjust it by the heightening amount, re-estimate losses\n",
    "# across all return periods, and re-estimate eal\n",
    "# In fact, since depths are \"fixed\" in our case study\n",
    "# we don't have to adjust the ffe variable, and can instead\n",
    "# adjust the depth_ffe_* columns\n",
    "\n",
    "# These are shared columns for subsetting\n",
    "# We need found_type because it is used in\n",
    "# elevation cost estimation\n",
    "# We need sqft because it's a key variable for\n",
    "# elevation cost estimation\n",
    "# We need bldgtype for elevation cost estimation, too\n",
    "sub_cols = ['fd_id', 'found_type', 'sqft', 'bldgtype']\n",
    "# We need to add depth_ffe_* columns \n",
    "depth_ffe_cols = ['depth_ffe_' + x for x in RET_PERS]\n",
    "sub_cols = sub_cols + depth_ffe_cols\n",
    "\n",
    "# These are columns for the benchmark df\n",
    "# We need fz_ddf for DDF estimation\n",
    "bench_cols = ['fz_ddf'] + sub_cols\n",
    "\n",
    "# We need to use DDF_TYPE argument to add either bld_types (naccs)\n",
    "# or hazus_types to our ens_cols list\n",
    "ddf_col = 'bld_types' if DDF_TYPE == 'naccs' else 'hazus_types'\n",
    "# These are general columns for the ensemble df\n",
    "# We need to keep track of SOW, and val_s is needed for\n",
    "# estimating avoided losses in a particular SOW\n",
    "ens_cols = ['val_s', 'sow_ind', ddf_col] + sub_cols\n",
    "\n",
    "# Subset ens_df and bench_df on their column lists\n",
    "ens_df = ens_df.loc[ens_cols]\n",
    "bench_df = bench_df.loc[bench_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f66d7-aef5-4dd2-945a-13f7f33037e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need DDFs for estimating benefits\n",
    "# Load DDFs\n",
    "naccs_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'naccs_ddfs.pqt'))\n",
    "hazus_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs.pqt'))\n",
    "hazus_nounc = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs_nounc.pqt'))\n",
    "\n",
    "# Load helper dictionaries\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus.json'), 'r') as fp:\n",
    "    HAZUS_MAX_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus_nounc.json'), 'r') as fp:\n",
    "    HAZUS_MAX_NOUNC_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'naccs.json'), 'r') as fp:\n",
    "    NACCS_MAX_DICT = json.load(fp)\n",
    "\n",
    "# Use DDF_TYPE argument for storing the correct ddfs\n",
    "benefit_ddfs = naccs_ddfs if DDF_TYPE == 'naccs' else hazus_ddfs\n",
    "MAX_DICT = NACCS_MAX_DICT if DDF_TYPE == 'naccs' else HAZUS_MAX_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8874e-27d5-48d8-9389-e1d7ae36e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the config file take the inflation values, heightening values, \n",
    "# and fixed cost values. For heightening, we need to linearly interpolate\n",
    "# for our structure specific heightening cost estimates. For the others, \n",
    "# we need to generate N_SOW length realizations. We can pre-populate\n",
    "# a cost dataframe with this information and for each \n",
    "# foundation type, heightening combo, we will have\n",
    "# the SOW specific cost estimate to apply to the structure. \n",
    "# The costs are applied against the expected annual losses\n",
    "# to figure out the optimal heightening. We do not need to include\n",
    "# discount rates at this step since these are uniformly applied\n",
    "# in our case study since all the elevations are assumed to occur\n",
    "# at the same time. Time-based elevations that account for changing\n",
    "# cost estimates and discount rates is an extension of this work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb11a9-ce60-4edb-9ff1-c3043b0905d1",
   "metadata": {},
   "source": [
    "# Optimal Elevation Under Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa6f43-0d17-4881-9d4f-81f58d0f92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do this, we loop through each of the possible heightenings\n",
    "# This effectively creates a new first floor elevation for each home\n",
    "# and then we repeat the loss estimation from benchmark_ensemble\n",
    "# for each structure\n",
    "# In addition to this, we estimate the costs for each heightening\n",
    "# This is done on a state of the world basis since it's informed\n",
    "# by macroeconomic conditions. What this means is we generate\n",
    "# a N_SOW length vector of the inflation value & the fixed cost\n",
    "# value that is used for the elevation cost. The per sq. ft. costs\n",
    "# are not changing across SOWs\n",
    "# It seems like because DDFs are monotonic increasing with depth\n",
    "# and costs are monotonic increasing with heightening (due to\n",
    "# linear interpolation), you could use an approach like binary\n",
    "# sort on the heightening possibilities to save some search time\n",
    "# but it won't take that long to loop through the 8 possible\n",
    "# heightenings we're considering. Because the cost data is less than\n",
    "# ideal, I don't want to introduce an approach like binary sort that\n",
    "# works only on the cost data we have (in reality, cost estimates\n",
    "# may not be well suited to this approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4f8c5-b288-4675-9a0c-cbaf54036312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2ca71-514e-4b78-b4d7-9d0ddbb3996a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

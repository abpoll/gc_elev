{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd01cab0-e55e-4fe4-aa81-88afbb80a320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:32.312420Z",
     "iopub.status.busy": "2024-01-02T14:29:32.311228Z",
     "iopub.status.idle": "2024-01-02T14:29:32.491388Z",
     "shell.execute_reply": "2024-01-02T14:29:32.490239Z",
     "shell.execute_reply.started": "2024-01-02T14:29:32.312367Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6922fd9-d241-449c-a0c2-297b4cf56827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:32.494891Z",
     "iopub.status.busy": "2024-01-02T14:29:32.493813Z",
     "iopub.status.idle": "2024-01-02T14:29:45.297795Z",
     "shell.execute_reply": "2024-01-02T14:29:45.296476Z",
     "shell.execute_reply.started": "2024-01-02T14:29:32.494843Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c48690-c6db-4531-b975-4c0f77a2c578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:45.302810Z",
     "iopub.status.busy": "2024-01-02T14:29:45.301907Z",
     "iopub.status.idle": "2024-01-02T14:29:45.428347Z",
     "shell.execute_reply": "2024-01-02T14:29:45.427209Z",
     "shell.execute_reply.started": "2024-01-02T14:29:45.302755Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'\n",
    "\n",
    "# I think it also could make sense to pass in scenario and\n",
    "# ddf type as arguments. For main results\n",
    "# we're using 'mid' and 'naccs' but for generating\n",
    "# our sensitivity analysis results we will need to pass\n",
    "# in the other scenarios and 'hazus'\n",
    "# Very well suited for snakemake :) \n",
    "SCENARIO = 'Mid'\n",
    "DDF_TYPE = 'naccs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90042fbe-f2e4-4c34-bc71-c156fab78c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:45.430216Z",
     "iopub.status.busy": "2024-01-02T14:29:45.429763Z",
     "iopub.status.idle": "2024-01-02T14:29:45.471006Z",
     "shell.execute_reply": "2024-01-02T14:29:45.469938Z",
     "shell.execute_reply.started": "2024-01-02T14:29:45.430167Z"
    }
   },
   "outputs": [],
   "source": [
    "# This notebook evaluates optimal elevation \n",
    "# per house across an ensemble\n",
    "# Optimal elevation height for each house is returned based on\n",
    "# the ensemble generated in benchmark_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3319852-a49d-47d9-9d18-a95c8768c53f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:45.475026Z",
     "iopub.status.busy": "2024-01-02T14:29:45.474243Z",
     "iopub.status.idle": "2024-01-02T14:29:45.506984Z",
     "shell.execute_reply": "2024-01-02T14:29:45.505876Z",
     "shell.execute_reply.started": "2024-01-02T14:29:45.474981Z"
    }
   },
   "outputs": [],
   "source": [
    "# The gist of this is to evaluate costs & benefits for each increment\n",
    "# of elevation from 3 to 10 feet \n",
    "# There are values for costs from the FEMA report \n",
    "# that go in the config file\n",
    "# This gets multipled by sq ft of the building\n",
    "# There is uncertainty in the inflation costs (relative to 2009 when\n",
    "# the report was published) and the fixed costs (CLARA and\n",
    "# NACCS reports provide us with a range)\n",
    "# All of these values are in the config.yaml, and \n",
    "# the data for these estimates is in the resources/\n",
    "# directory of the project repository\n",
    "# For benefits, we'll do it \"through\" the DDF by moving along the\n",
    "# x-axis of the DDF. We do this by changing the FFE, which we are\n",
    "# treating under uncertainty. We are not treating the elevation\n",
    "# as an uncertain amount for this case study. We need\n",
    "# to estimate the losses with the new first-floor elevation adjusted\n",
    "# height under uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879497b-4844-47b6-b950-8bdfb9dd1bd6",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e91616-ed1b-4ffc-b21a-a01991161cbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:45.509116Z",
     "iopub.status.busy": "2024-01-02T14:29:45.508643Z",
     "iopub.status.idle": "2024-01-02T14:29:52.957523Z",
     "shell.execute_reply": "2024-01-02T14:29:52.956760Z",
     "shell.execute_reply.started": "2024-01-02T14:29:45.509072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the mid scenario data\n",
    "# There are only a few columns we need\n",
    "ens_filep = join(FO, 'ensemble_' + SCENARIO + '.pqt')\n",
    "ens_df = pd.read_parquet(ens_filep)\n",
    "\n",
    "# Also load the benchmark data\n",
    "# There are only a few columns we need\n",
    "bench_df = pd.read_parquet(join(FO, 'benchmark_loss.pqt'))\n",
    "\n",
    "# We also need the design flood elevations\n",
    "dfe_filep = join(EXP_DIR_I, FIPS, 'nsi_dfe.pqt')\n",
    "dfe_df = pd.read_parquet(dfe_filep)\n",
    "\n",
    "# Everything that we do here is based on the ensemble values\n",
    "# That means we take the ffe variable in our ensemble df\n",
    "# and adjust it by the heightening amount, re-estimate losses\n",
    "# across all return periods, and re-estimate eal\n",
    "# In fact, since depths are \"fixed\" in our case study\n",
    "# we don't have to adjust the ffe variable, and can instead\n",
    "# adjust the depth_ffe_* columns\n",
    "\n",
    "# These are shared columns for subsetting\n",
    "# We need found_type because it is used in\n",
    "# elevation cost estimation\n",
    "# We need sqft because it's a key variable for\n",
    "# elevation cost estimation\n",
    "# We need bldgtype for elevation cost estimation, too\n",
    "sub_cols = ['fd_id', 'found_type', 'sqft', 'bldgtype']\n",
    "# We need to add depth_ffe_* columns \n",
    "depth_ffe_cols = ['depth_ffe_' + x for x in RET_PERS]\n",
    "sub_cols = sub_cols + depth_ffe_cols\n",
    "\n",
    "# These are columns for the benchmark df\n",
    "# We need fz_ddf for DDF estimation\n",
    "# We need eal for estimating avoided losses\n",
    "bench_cols = ['fz_ddf', 'eal'] + sub_cols\n",
    "\n",
    "# We need to use DDF_TYPE argument to add either bld_types (naccs)\n",
    "# or hazus_types to our ens_cols list\n",
    "ddf_col = 'bld_types' if DDF_TYPE == 'naccs' else 'hazus_types'\n",
    "\n",
    "# We also need the eal col for comparing against eals w/ elevation\n",
    "eal_col = 'naccs_eal' if DDF_TYPE == 'naccs' else 'haz_eal'\n",
    "\n",
    "# These are general columns for the ensemble df\n",
    "# We need to keep track of SOW, and val_s is needed for\n",
    "# estimating avoided losses in a particular SOW\n",
    "ens_cols = ['val_s', 'sow_ind', ddf_col, eal_col] + sub_cols\n",
    "\n",
    "# Subset ens_df and bench_df on their column lists\n",
    "ens_df = ens_df.loc[:,ens_cols]\n",
    "bench_df = bench_df.loc[:,bench_cols]\n",
    "\n",
    "# Add design flood elevation. When missing, fill in with 3\n",
    "dfe_map = dict(zip(dfe_df['fd_id'], dfe_df['dfe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "372f66d7-aef5-4dd2-945a-13f7f33037e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:52.958292Z",
     "iopub.status.busy": "2024-01-02T14:29:52.958152Z",
     "iopub.status.idle": "2024-01-02T14:29:53.132109Z",
     "shell.execute_reply": "2024-01-02T14:29:53.131107Z",
     "shell.execute_reply.started": "2024-01-02T14:29:52.958278Z"
    }
   },
   "outputs": [],
   "source": [
    "# We'll need DDFs for estimating benefits\n",
    "# Load DDFs\n",
    "naccs_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'naccs_ddfs.pqt'))\n",
    "hazus_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs.pqt'))\n",
    "hazus_nounc = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs_nounc.pqt'))\n",
    "\n",
    "# Load helper dictionaries\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus.json'), 'r') as fp:\n",
    "    HAZUS_MAX_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus_nounc.json'), 'r') as fp:\n",
    "    HAZUS_MAX_NOUNC_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'naccs.json'), 'r') as fp:\n",
    "    NACCS_MAX_DICT = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc27e55-cadc-4fc3-9c17-6ca1ccc267fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:53.133808Z",
     "iopub.status.busy": "2024-01-02T14:29:53.133460Z",
     "iopub.status.idle": "2024-01-02T14:29:54.196677Z",
     "shell.execute_reply": "2024-01-02T14:29:54.195942Z",
     "shell.execute_reply.started": "2024-01-02T14:29:53.133772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and prepare discount rates and house lifetime\n",
    "\n",
    "# Download discount rate chains from external source\n",
    "# The rows correspond to house lifetime, indexed at 0\n",
    "# The columns correspond to states of the world, indexed at 0\n",
    "dr_chains = pd.read_csv(join(FE, 'dr_chains.csv'),\n",
    "                        header=None)\n",
    "\n",
    "# Following https://www.journals.uchicago.edu/doi/10.1086/718145\n",
    "# and https://doi.org/10.1162/rest_a_01109, replace values < 0 with 0\n",
    "# The economic argument is that descriptive discount rates will not\n",
    "# be less than 0 for long. Bauer and Rudebusch (the latter link)\n",
    "# have ~ 3 paragraphs addressing the mechanisms behind this\n",
    "# which I have paraphrased badly here. Some of the intuition is that\n",
    "# when nominal rates are low and inflation is high, households\n",
    "# can hold cash and reduce spending, bringing inflation down\n",
    "# and real rates back up. They offer a more complex, comprehsneive,\n",
    "# and convincing argument. I have had some conversations about\n",
    "# scrutinizing this assumption in a future paper. \n",
    "dr_chains[dr_chains < 0] = 0\n",
    "\n",
    "# Need to turn these into discount factors, following\n",
    "# Maggie's code\n",
    "# We need the rates as percentages, then we take the cumulative\n",
    "# sum of these such that the discount factor in year t\n",
    "# is the sum of all rates leading to that\n",
    "# Then we take e^- of that value\n",
    "dr_factors = np.exp(-(dr_chains/100).cumsum())\n",
    "\n",
    "# Generate house lifetime draws from the weibull distribution\n",
    "# following https://www.nature.com/articles/s41467-020-19188-9\n",
    "# Weibull with shape and scale parameters of 2.8 and 73.5\n",
    "# In numpy, you generate draws from a 1 parameter Weibull\n",
    "# using the shape parameter, and multiply these draws from\n",
    "# the scale parameter\n",
    "# It's likely that house lifetime distributions are different for\n",
    "# elevated and non-elevated properties exposed to flooding, but\n",
    "# we don't have this information. While the method can be improved,\n",
    "# Maggie's paper demonstrates the importance of accounting for\n",
    "# house lifetime uncertainty in estimating project benefits. In\n",
    "# particular, when discount rates in the future are low and project\n",
    "# lifetimes are long, the net benefits will be higher than\n",
    "# under the standard procedure (moderate discount rate and\n",
    "# 30 year lifetime). The standard procedure -- which also requires\n",
    "# that the project BCR > 1 -- will tend to under-prefer\n",
    "# investment in lower valued structures which need a longer time\n",
    "# to exceed the BCR > 1 threshold. So again, while future work\n",
    "# should improve on this, accounting for house lifetime uncertainty\n",
    "# matters when you're dealing with a BCR > 1 rule, as we are. \n",
    "# Since this is about the interaction with the discount rate, and\n",
    "# we don't have the ability to associate the parameters with\n",
    "# housing characteristics, I will use the same draws for each house\n",
    "# and interpret this as uncertainty in the house lifetime parameter\n",
    "# whose benchmark value is 30. \n",
    "\n",
    "rng = np.random.default_rng()\n",
    "lifetime = rng.weibull(W_SHAPE, N_SOW)*W_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f8874e-27d5-48d8-9389-e1d7ae36e506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:54.197522Z",
     "iopub.status.busy": "2024-01-02T14:29:54.197385Z",
     "iopub.status.idle": "2024-01-02T14:29:58.798699Z",
     "shell.execute_reply": "2024-01-02T14:29:58.797462Z",
     "shell.execute_reply.started": "2024-01-02T14:29:54.197509Z"
    }
   },
   "outputs": [],
   "source": [
    "# From the config file take the inflation values, heightening values, \n",
    "# and fixed cost values. For heightening, we need to linearly interpolate\n",
    "# for our structure specific heightening cost estimates. For the others, \n",
    "# we need to generate N_SOW length realizations. We can pre-populate\n",
    "# a cost dataframe with this information and for each \n",
    "# foundation type, heightening combo, we will have\n",
    "# the SOW specific cost estimate to apply to the structure. \n",
    "# The costs are applied against the expected annual losses\n",
    "# to figure out the optimal heightening. We do not need to include\n",
    "# discount rates at this step since these are uniformly applied\n",
    "# in our case study since all the elevations are assumed to occur\n",
    "# at the same time. Time-based elevations that account for changing\n",
    "# cost estimates and discount rates is an extension of this work. \n",
    "\n",
    "# Do the interpolation on the elev costs\n",
    "# To get basement/bldgtype as multiindex from our dict\n",
    "elev_cost_df = pd.DataFrame.from_dict(ELEV_COST_DICT).stack().to_frame()\n",
    "# To break out the lists into columns\n",
    "# Column names as the foot value (as int) \n",
    "e_c_df = pd.DataFrame(elev_cost_df[0].values.tolist(),\n",
    "                      index=elev_cost_df.index,\n",
    "                      columns=[2, 4, 8]).reset_index()\n",
    "\n",
    "# Melt and rename to get ready for linear interpolation between feet\n",
    "e_c_df = e_c_df.melt(id_vars=['level_0', 'level_1'], value_vars=[2, 4, 8])\n",
    "e_c_df.columns = ['fnd_type', 'bldgtype', 'elev_ht', 'cost_sqft']\n",
    "\n",
    "# Loop through fnd_type, bldgtype groups\n",
    "# Add missing foot values and interpolate using\n",
    "# spline of order 1 to get values filled\n",
    "# past the 8 foot value and up to 10\n",
    "# Store each interpolated dataframe in a list and concat at the end\n",
    "elev_dfs = []\n",
    "for fnd_bldg, df_sub in e_c_df.groupby(['fnd_type', 'bldgtype']):\n",
    "    # keep track of foundation type and bldgtype\n",
    "    fnd = fnd_bldg[0]\n",
    "    bld = fnd_bldg[1]\n",
    "\n",
    "    # use elev ht as index and get a series of costs\n",
    "    elevs = df_sub.set_index('elev_ht')['cost_sqft']\n",
    "    # get the elevations from 2 to 3 feet that we are missing\n",
    "    missing_elevs = [x for x in np.arange(2, 11) if x not in elevs.index]\n",
    "    # combine elevs and missing elevs\n",
    "    elevs_f = pd.concat([elevs, pd.DataFrame(index=pd.Index(missing_elevs))])\n",
    "    # sort index and interpolate\n",
    "    elevs_f = elevs_f.sort_index().interpolate('spline', order=1).round(1)\n",
    "\n",
    "    # We consider elevation from 3 to 10 feet only\n",
    "    elevs_f = elevs_f.loc[3:10]\n",
    "\n",
    "    # Reset index and rename columns\n",
    "    elevs_f = elevs_f.reset_index()\n",
    "    elevs_f.columns = ['elev_ht', 'cost_sqft']\n",
    "    # Add back fnd_type and bldgtype\n",
    "    # using first character as capital letter\n",
    "    elevs_f['fnd_type'] = fnd[0].upper()\n",
    "    elevs_f['bldgtype'] = bld[0].upper()\n",
    "\n",
    "    elev_dfs.append(elevs_f)\n",
    "# Final cost per sqft dataframe\n",
    "elev_costs = pd.concat(elev_dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Sample N_SOW from uniform(CPI_LOW, CPI_HIGH)\n",
    "# Sample N_SOW from uniform (ELEV_FIX_LOW, ELEV_FIX_HIGH)\n",
    "rng = np.random.default_rng()\n",
    "construction_infl = rng.uniform(CPI_LOW, CPI_HIGH, N_SOW)\n",
    "fixed = rng.uniform(ELEV_FIX_LOW, ELEV_FIX_HIGH, N_SOW)\n",
    "\n",
    "# Get the cost dataframe for each sow_ind \n",
    "# Columns are sow_ind, bldgtype, heightening, cost\n",
    "# We need to multiply each cost_sqft value in elev_costs\n",
    "# by each element of construction_infl and make sure this is\n",
    "# indexed by the sow. Then, we need to add the fixed cost\n",
    "# corresponding to that sow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17053fb2-3b3d-456b-926f-b3b62ce2cb39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:58.800793Z",
     "iopub.status.busy": "2024-01-02T14:29:58.800226Z",
     "iopub.status.idle": "2024-01-02T14:29:59.158708Z",
     "shell.execute_reply": "2024-01-02T14:29:59.157697Z",
     "shell.execute_reply.started": "2024-01-02T14:29:58.800747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Repeat the elev_costs df so that each entry (ht, cst, types) has\n",
    "# N_SOW rows\n",
    "e_c_ens = elev_costs.loc[np.repeat(elev_costs.index, N_SOW)]\n",
    "e_c_ens = e_c_ens.reset_index(drop=True)\n",
    "# Then repeat the construction_infl and fixed series len(elev_costs)\n",
    "# times. Do this via tiling (i.e. repeat the whole array not \n",
    "# the elements) \n",
    "c_infl_full = np.tile(construction_infl, len(elev_costs))\n",
    "fixed_full = np.tile(fixed, len(elev_costs))\n",
    "\n",
    "# Now create new column in e_c_ens for\n",
    "# cost_sqft*c_infl_full and fixed_full\n",
    "e_c_ens['cost_sqft_unc'] = e_c_ens['cost_sqft']*c_infl_full\n",
    "e_c_ens['cost_fix_unc'] = fixed_full\n",
    "\n",
    "# Then get the sow_ind for the e_c_ens dataframe\n",
    "sow_ind = np.arange(len(e_c_ens))%N_SOW\n",
    "e_c_ens = pd.concat([e_c_ens, pd.Series(sow_ind, name='sow_ind')], axis=1)\n",
    "\n",
    "# Write out the elevation cost ensemble\n",
    "elev_ens_filep = join(EXP_DIR_I, FIPS, 'elev_ens.pqt')\n",
    "prepare_saving(elev_ens_filep)\n",
    "e_c_ens.to_parquet(elev_ens_filep)\n",
    "\n",
    "# We will also do the cost_sqft_unc*sqft + cost_fix_unc, indexed\n",
    "# on sow_ind for each structure across SOWs, but will only write out\n",
    "# the costs associated with the optimal elevation\n",
    "# Since the elevation cost ensemble is written out, it's always\n",
    "# accessible to inspect elevation costs for any \n",
    "# eligible heightening for homes in/across SOWs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb11a9-ce60-4edb-9ff1-c3043b0905d1",
   "metadata": {},
   "source": [
    "# Optimal Elevation Under Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cfa6f43-0d17-4881-9d4f-81f58d0f92a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:59.160394Z",
     "iopub.status.busy": "2024-01-02T14:29:59.159874Z",
     "iopub.status.idle": "2024-01-02T14:29:59.185594Z",
     "shell.execute_reply": "2024-01-02T14:29:59.184687Z",
     "shell.execute_reply.started": "2024-01-02T14:29:59.160363Z"
    }
   },
   "outputs": [],
   "source": [
    "# To do this, we loop through each of the possible heightenings\n",
    "# This effectively creates a new first floor elevation for each home\n",
    "# and then we repeat the loss estimation from benchmark_ensemble\n",
    "# for each structure\n",
    "# In addition to this, we estimate the costs for each heightening\n",
    "# This is done on a state of the world basis since it's informed\n",
    "# by macroeconomic conditions. What this means is we generate\n",
    "# a N_SOW length vector of the inflation value & the fixed cost\n",
    "# value that is used for the elevation cost. The per sq. ft. costs\n",
    "# are not changing across SOWs\n",
    "# It seems like because DDFs are monotonic increasing with depth\n",
    "# and costs are monotonic increasing with heightening (due to\n",
    "# linear interpolation), you could use an approach like binary\n",
    "# sort on the heightening possibilities to save some search time\n",
    "# but it won't take that long to loop through the 8 possible\n",
    "# heightenings we're considering. Because the cost data is less than\n",
    "# ideal, I don't want to introduce an approach like binary sort that\n",
    "# works only on the cost data we have (in reality, cost estimates\n",
    "# may not be well suited to this approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc4f8c5-b288-4675-9a0c-cbaf54036312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:59.186939Z",
     "iopub.status.busy": "2024-01-02T14:29:59.186649Z",
     "iopub.status.idle": "2024-01-02T14:29:59.386116Z",
     "shell.execute_reply": "2024-01-02T14:29:59.385397Z",
     "shell.execute_reply.started": "2024-01-02T14:29:59.186910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get fnd_type variable for ens_df that corresponds to B and S\n",
    "# where crawl space (C) from found_type gets classified as B\n",
    "# This is needed for a future step\n",
    "ens_df['fnd_type'] = np.where(ens_df['found_type'] == 'S',\n",
    "                              'S', \n",
    "                              'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae425b1-762b-4e13-bf91-fd4d7422a723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:59.386876Z",
     "iopub.status.busy": "2024-01-02T14:29:59.386738Z",
     "iopub.status.idle": "2024-01-02T14:29:59.873437Z",
     "shell.execute_reply": "2024-01-02T14:29:59.872687Z",
     "shell.execute_reply.started": "2024-01-02T14:29:59.386863Z"
    }
   },
   "outputs": [],
   "source": [
    "# We're going to make a lifetime_mask and dr_matrix \n",
    "# to calculate present values of potential heightenings - the avoided\n",
    "# losses as well as the residual risk\n",
    "# Prepare lifetime mask and matrix for discount factors\n",
    "\n",
    "# Use the lifetime series to create a mask\n",
    "# Can adapt this code\n",
    "# https://stackoverflow.com/questions/55190295/\n",
    "# create-a-2-d-mask-from-a-1-d-numpy-array\n",
    "# This code is complex, so I want to explain what is happening. You\n",
    "# can also look at the stackoverflow link which provides helpful\n",
    "# information. \n",
    "# So, let's start from the inside out. The first command \n",
    "# is np.less.outer(lifetime, np.arange(100))\n",
    "# This takes the lifetime array, which is N_SOW in length\n",
    "# and broadcasts that with outer into a N_SOW*100 shape 2d array\n",
    "# 100 is the max lifetime we consider since discount rates are\n",
    "# projected through 2100. We're comparing the values in lifetime\n",
    "# to the values in the np.arange(100) array, and when the lifetime\n",
    "# value is less, the element is assigned True. This creates\n",
    "# a mask of True/False values which we need to match up to\n",
    "# our 100x2390000 matrix of eal_avoid values. We do this first by\n",
    "# transposing, then tiling along the columns the same number\n",
    "# of times as we have structures in our sample. \n",
    "lifetime_mask = np.tile(np.less.outer(lifetime, np.arange(100)).T,\n",
    "                        (1, len(ens_df['fd_id'].unique())))\n",
    "\n",
    "# discount factor matrix\n",
    "dr_matrix = np.tile(dr_factors, (1, len(ens_df['fd_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d2ca71-514e-4b78-b4d7-9d0ddbb3996a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:29:59.875199Z",
     "iopub.status.busy": "2024-01-02T14:29:59.875049Z",
     "iopub.status.idle": "2024-01-02T14:34:21.996858Z",
     "shell.execute_reply": "2024-01-02T14:34:21.996037Z",
     "shell.execute_reply.started": "2024-01-02T14:29:59.875186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 3 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 4 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 5 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 6 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 7 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 8 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 9 feet\n",
      "\n",
      "Estimate Losses for Elevated Home, RP: 001\n",
      "Estimate Losses for Elevated Home, RP: 002\n",
      "Estimate Losses for Elevated Home, RP: 005\n",
      "Estimate Losses for Elevated Home, RP: 010\n",
      "Estimate Losses for Elevated Home, RP: 015\n",
      "Estimate Losses for Elevated Home, RP: 020\n",
      "Estimate Losses for Elevated Home, RP: 025\n",
      "Estimate Losses for Elevated Home, RP: 050\n",
      "Estimate Losses for Elevated Home, RP: 075\n",
      "Estimate Losses for Elevated Home, RP: 100\n",
      "Estimate Losses for Elevated Home, RP: 200\n",
      "Estimate Losses for Elevated Home, RP: 500\n",
      "Calculated EAL\n",
      "Calculations done for heightening by 10 feet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through possible heightenings of 3 through 10 feet, inclusive\n",
    "# For each of these, add that value to each column in depth_ffe_*\n",
    "# Then, go through the procedures from benchmark_ensemble\n",
    "# to calculate losses per return period and ultimately the eal\n",
    "# Then, we compare this eal to the non-elevated eal, which is \n",
    "# stored in the reference \"eal_col\"\n",
    "# We then take the subset of \n",
    "# fd_id, sow_ind, fnd_type, bldgtype, sqft, reduced_eal\n",
    "# and merge it with e_c_ens. (sow_ind, fnd_type, bldgtype)\n",
    "# Take reduced_eal/(sqft*cost_sqft_unc + cost_fix_unc) and store\n",
    "# it as bcr. Groupby on fd_id and calculate the mean bcr.\n",
    "# Store this as a series with name corresponding to the amount\n",
    "# of heightening and index corresponding to fd_id,\n",
    "# and store that series in a list\n",
    "# I think we also want to store the reduced_eal and\n",
    "# the costs for each sow. \n",
    "# After looping through all of these, we can concat our list\n",
    "# into a dataframe and figure out which column corresponds\n",
    "# to the highest bcr for each fd_id. I think we can do this\n",
    "# using df.idxmax(axis=\"columns\") if we concat on columns\n",
    "# Finally, we use the corresponding\n",
    "# heightening value to match up each fd_id to its\n",
    "# avoided loss, heightening, and expected costs. The BCR needs to \n",
    "# be adjusted in the allocate funding procedure later on by different\n",
    "# discount rate projections. I think we need to discount the \n",
    "# reduced_eal in each SOW AND divide each of those by costs again\n",
    "# to get the correct expected BCR. But you don't need to do \n",
    "# discounting to find the optimal elevation. \n",
    "\n",
    "# List for series of mean bcr at each heightening\n",
    "h_list = []\n",
    "\n",
    "for h in np.arange(3, 11):\n",
    "    # Adjust depth_ffe_* columns by h\n",
    "    # We substract h because these are depths relative to first floor\n",
    "    # and now the first floor is higher\n",
    "    depth_ffe_df = ens_df.loc[:,depth_ffe_cols] - h\n",
    "    # Remove 'depth_ffe_' part from the column\n",
    "    depth_ffe_df.columns = [x.split('_')[-1] for x in depth_ffe_df.columns]\n",
    "    \n",
    "    # We will store losses in dictionaries with return period keys\n",
    "    elev_losses = {}\n",
    "    \n",
    "    for rp in RET_PERS:\n",
    "        if DDF_TYPE == 'naccs':\n",
    "            elev_losses[rp] = est_naccs_loss(ens_df['bld_types'],\n",
    "                                             depth_ffe_df[rp],\n",
    "                                             naccs_ddfs,\n",
    "                                             NACCS_MAX_DICT)\n",
    "        else:\n",
    "            elev_losses[rp] = est_hazus_loss(ens_df['hazus_types'],\n",
    "                                             depth_ffe_df[rp],\n",
    "                                             hazus_ddfs,\n",
    "                                             HAZUS_MAX_DICT)\n",
    "    \n",
    "        print('Estimate Losses for Elevated Home, RP: ' + rp)\n",
    "    \n",
    "    # Then, we convert these to dataframes\n",
    "    loss_df = pd.DataFrame.from_dict(elev_losses)\n",
    "    \n",
    "    # For each relative damage column, scale by val_s, the structure\n",
    "    # value realization\n",
    "    # loss_df and ens_df are index aligned, so this works\n",
    "    for col in loss_df.columns:\n",
    "        loss_df['loss_' + col] = loss_df[col]*ens_df['val_s']\n",
    "    \n",
    "    # We make a list of our loss columns\n",
    "    loss_list = ['loss_' + x for x in RET_PERS]\n",
    "    # As well as the corresponding probabilities\n",
    "    p_rp_list = [round(1/int(x), 4) for x in RET_PERS]\n",
    "    \n",
    "    # Then we create an empty series\n",
    "    eal_elev = pd.Series(index=loss_df.index).fillna(0)\n",
    "    \n",
    "    # We loop through our loss list and apply the \n",
    "    # trapezoidal approximation\n",
    "    for i in range(len(loss_list) - 1):\n",
    "        loss1 = loss_df[loss_list[i]]\n",
    "        loss2 = loss_df[loss_list[i+1]]\n",
    "        rp1 = p_rp_list[i]\n",
    "        rp2 = p_rp_list[i+1]\n",
    "        # We add each approximation\n",
    "        eal_elev += (loss1 + loss2)*(rp1-rp2)/2\n",
    "    # This is the final trapezoid to add in\n",
    "    final_eal = eal_elev + loss_df[loss_list[-1]]*p_rp_list[-1]\n",
    "    print('Calculated EAL')\n",
    "\n",
    "    # Calculate avoided losses and add to ens_df\n",
    "    # Cannot be less than 0\n",
    "    eal_avoid_temp = ens_df[eal_col] - final_eal\n",
    "    eal_avoid_temp[eal_avoid_temp < 0] = 0\n",
    "    ens_df['eal_avoid_' + str(h)] = eal_avoid_temp\n",
    "    \n",
    "    # Present value - avoided losses\n",
    "    eal_avoid = np.tile(ens_df['eal_avoid_' + str(h)], (100, 1))\n",
    "    # Apply the lifetime_mask to eal_avoid\n",
    "    eal_av_life = ma.masked_array(eal_avoid,\n",
    "                                  mask=lifetime_mask,\n",
    "                                  fill_value=0)\n",
    "    # present value \n",
    "    pv_avoided = (eal_av_life*dr_matrix).sum(axis=0)\n",
    "    # Add back into ens_df\n",
    "    ens_df['pv_avoid_' + str(h)] = pv_avoided.data\n",
    "    # Also get the relative avoided\n",
    "    ens_df['avoid_rel_eal_' + str(h)] = (ens_df['eal_avoid_' + str(h)]\n",
    "                                         /ens_df['val_s'])\n",
    "    \n",
    "    # Merge e_c_ens on subset of ens_df columns to figure out\n",
    "    # the elevation cost and get this into ens_df\n",
    "    ens_sub = ens_df[['fd_id', 'sow_ind', 'fnd_type',\n",
    "                      'bldgtype', 'sqft']].copy()\n",
    "    \n",
    "    # Also subset e_c_ens for the correct heightening\n",
    "    # Don't need cost_sqft for this either\n",
    "    e_c_ens_sub = e_c_ens[e_c_ens['elev_ht'] == h].drop(columns=['elev_ht',\n",
    "                                                                 'cost_sqft'])\n",
    "    \n",
    "    # Merge on sow, fnd_type, bldgtype\n",
    "    e_c_merge = ens_sub.merge(e_c_ens_sub,\n",
    "                              on=['fnd_type', 'bldgtype', 'sow_ind'])\n",
    "    \n",
    "    # Get upfront costs\n",
    "    invsts = (e_c_merge['sqft']*e_c_merge['cost_sqft_unc']\n",
    "             + e_c_merge['cost_fix_unc'])\n",
    "    ens_df['elev_invst_' + str(h)] = invsts\n",
    "\n",
    "    # Present value - residual risk (our final_eal column)\n",
    "    eal_resid = np.tile(final_eal, (100, 1))\n",
    "    # Apply the lifetime_mask to eal_avoid\n",
    "    eal_r_life = ma.masked_array(eal_resid,\n",
    "                                 mask=lifetime_mask,\n",
    "                                 fill_value=0)\n",
    "    # present value \n",
    "    pv_resid = (eal_r_life*dr_matrix).sum(axis=0)\n",
    "    # Add back into ens_df\n",
    "    ens_df['pv_resid_' + str(h)] = pv_resid.data\n",
    "    # Also get the relative resid\n",
    "    ens_df['resid_rel_eal_' + str(h)] = final_eal/ens_df['val_s']\n",
    "\n",
    "    # Get costs\n",
    "    # Add present value of residual risk to upfront cost\n",
    "    ens_df['elev_cost_' + str(h)] = (ens_df['pv_resid_' + str(h)]\n",
    "                                     + ens_df['elev_invst_' + str(h)])\n",
    "    \n",
    "    # Now we have the avoided loss and elev cost for this level of\n",
    "    # heightening stored in ens_df\n",
    "    # It also helps to do some side calculations to save some time\n",
    "    # later in obtaining the optimal level of heightening\n",
    "    # Get the ratio of eal_avoid_str(h) to elev_cost_str(h)\n",
    "    # Groupby on fd_id and take the mean\n",
    "    # Store this as a series with name corresponding to the amount\n",
    "    # of heightening and index corresponding to fd_id,\n",
    "    # and store that series in a list\n",
    "    ens_df['npv_' + str(h)] = (ens_df['pv_avoid_' + str(h)]\n",
    "                               - ens_df['elev_cost_' + str(h)])\n",
    "    \n",
    "    npvs = ens_df.groupby(['fd_id'])['npv_' + str(h)].mean()\n",
    "    h_list.append(npvs)\n",
    "\n",
    "    print('Calculations done for heightening by ' + str(h) + ' feet\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f399f4f7-b1d0-4b8d-a392-6f849c7e31e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:34:21.997788Z",
     "iopub.status.busy": "2024-01-02T14:34:21.997653Z",
     "iopub.status.idle": "2024-01-02T14:34:22.253276Z",
     "shell.execute_reply": "2024-01-02T14:34:22.252089Z",
     "shell.execute_reply.started": "2024-01-02T14:34:21.997776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the dataframe of mean bcr across SOWs for each structure\n",
    "# for each heightening. Find the heightening for each structure\n",
    "# that leads to the max mean bcr, and write this out to a file. \n",
    "# When we do the full bcr estimation later, we will loop through\n",
    "# each value in this series, subset the ens_df based on\n",
    "# fd_id with that bcr_part_h as their max mean bcr, and then\n",
    "# do the full discounting and recalculation of BCR. You need\n",
    "# to use the discount rate chain that corresponds to each SOW\n",
    "# to discount the avoided losses correctly\n",
    "# and you need to divide that by the costs in that SOW\n",
    "# THEN you can calculate our objectives like net benefits and\n",
    "# check conditions like BCR > 1. \n",
    "opt_elev = pd.concat(h_list, axis=1)\n",
    "opt_elev['opt_elev'] = opt_elev.idxmax('columns')\n",
    "npv_out_filename = 'opt_height_' + DDF_TYPE + '_' + SCENARIO + '.pqt'\n",
    "npv_out_filep = join(EXP_DIR_I, FIPS, npv_out_filename)\n",
    "prepare_saving(npv_out_filep)\n",
    "opt_elev.to_parquet(npv_out_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08af061a-a1c3-42ad-a71e-5dbd2ca931f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:34:22.255879Z",
     "iopub.status.busy": "2024-01-02T14:34:22.255009Z",
     "iopub.status.idle": "2024-01-02T14:34:25.681356Z",
     "shell.execute_reply": "2024-01-02T14:34:25.680907Z",
     "shell.execute_reply.started": "2024-01-02T14:34:22.255834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed rows with optimal elevation height of 5\n",
      "Processed rows with optimal elevation height of 6\n",
      "Processed rows with optimal elevation height of 3\n",
      "Processed rows with optimal elevation height of 7\n",
      "Processed rows with optimal elevation height of 4\n",
      "Processed rows with optimal elevation height of 8\n",
      "Processed rows with optimal elevation height of 10\n"
     ]
    }
   ],
   "source": [
    "# Write out ens_df columns related to optimal elevation\n",
    "# eal_avoid_h and elev_cost_h\n",
    "# Only need to do this for the heightening that corresponds\n",
    "# to the optimal level\n",
    "# Subset ens_df based on the information in opt_elev\n",
    "# Do this by looping through the values in opt_elev, getting the list\n",
    "# of fd_id that correspond to this, and then storing the ens_df\n",
    "# rows & columns (eal_avoid_h and elev_cost_h) in a list along\n",
    "# with the heightening amount\n",
    "# You will end up concatenating a dataframe that is\n",
    "# sow_ind, fd_id, eal_avoid_opt, elev_cost_opt, elev_h\n",
    "\n",
    "elev_df_l = []\n",
    "for elev_h in opt_elev['opt_elev'].unique():\n",
    "    # Subset of fd_id that have this optimal heightening\n",
    "    struct_sub = opt_elev[opt_elev['opt_elev'] == elev_h].index\n",
    "    # elev value\n",
    "    h = elev_h.split('_')[-1]\n",
    "    # Corresponding columns\n",
    "    ens_col_sub = ['pv_avoid_' + str(h), 'elev_cost_' + str(h),\n",
    "                   'elev_invst_' + str(h), 'pv_resid_' + str(h),\n",
    "                   'fd_id', 'sow_ind', \n",
    "                   'avoid_rel_eal_' + str(h), 'resid_rel_eal_' + str(h),\n",
    "                    eal_col]\n",
    "    # Corresponding rows and columns\n",
    "    ens_sub = ens_df.loc[ens_df['fd_id'].isin(struct_sub),\n",
    "                         ens_col_sub]\n",
    "    # Rename columns\n",
    "    ens_sub.columns = ['pv_avoid', 'pv_cost', \n",
    "                       'elev_invst', 'pv_resid',\n",
    "                       'fd_id', 'sow_ind', \n",
    "                       'avoid_rel_eal', 'resid_rel_eal', 'base_eal']\n",
    "    # Add the heightening amount back in\n",
    "    ens_sub['opt_elev'] = h\n",
    "    \n",
    "    elev_df_l.append(ens_sub)\n",
    "    print('Processed rows with optimal elevation height of ' + str(h))\n",
    "\n",
    "elev_df_f = pd.concat(elev_df_l, axis=0).sort_index()\n",
    "\n",
    "# Should also calculate present value of the 'base' eal\n",
    "# and write out the lifetime data that was generated\n",
    "eal_base = np.tile(elev_df_f['base_eal'], (100, 1))\n",
    "# Apply the lifetime_mask to eal_avoid\n",
    "eal_life = ma.masked_array(eal_base,\n",
    "                           mask=lifetime_mask,\n",
    "                           fill_value=0)\n",
    "# present value \n",
    "pv_base = (eal_life*dr_matrix).sum(axis=0)\n",
    "# Add back into ens_df\n",
    "elev_df_f['pv_base'] = pv_base.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "684489ca-5a3a-45b2-8a89-8df765be62af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:34:25.681985Z",
     "iopub.status.busy": "2024-01-02T14:34:25.681864Z",
     "iopub.status.idle": "2024-01-02T14:34:28.988875Z",
     "shell.execute_reply": "2024-01-02T14:34:28.987956Z",
     "shell.execute_reply.started": "2024-01-02T14:34:25.681974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out the lifetime mask\n",
    "lifetime_filename = 'lifetime_mask.npy'\n",
    "lifetime_filep = join(EXP_DIR_I, FIPS, lifetime_filename)\n",
    "with open(lifetime_filep, 'wb') as f:\n",
    "    np.save(f, lifetime_mask)\n",
    "\n",
    "# Write file in FIPS specific exp/ directory\n",
    "opt_elev_filename = 'ens_opt_elev_' + DDF_TYPE + '_' + SCENARIO + '.pqt'\n",
    "opt_elev_filep = join(EXP_DIR_I, FIPS, opt_elev_filename)\n",
    "elev_df_f.to_parquet(opt_elev_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d75f8-8521-410c-a3d3-6717ec709967",
   "metadata": {},
   "source": [
    "# Design Flood Elevation Estimates Under Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "007de45b-907e-4065-95f4-f8e2a3ec5fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:34:28.990196Z",
     "iopub.status.busy": "2024-01-02T14:34:28.989927Z",
     "iopub.status.idle": "2024-01-02T14:34:29.019275Z",
     "shell.execute_reply": "2024-01-02T14:34:29.018428Z",
     "shell.execute_reply.started": "2024-01-02T14:34:28.990171Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Like we did before, we need to loop through the \n",
    "# # design flood elevations associated with each property\n",
    "# # and get the costs and avoided eal associated with\n",
    "# # those heightenings\n",
    "# # To do this, we will add a column to the opt_elev dataframe\n",
    "# # based on the 'dfe' map we made earlier, filling in 3 feet\n",
    "# # as the value for non-BFE mapped places\n",
    "# dfe_elev = opt_elev.reset_index()\n",
    "# dfe_elev['dfe'] = dfe_elev['fd_id'].map(dfe_map).fillna(3)\n",
    "# # Set index to fd_id again\n",
    "# dfe_elev = dfe_elev.set_index('fd_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f04c4e9c-78ab-46d8-9839-009817be271e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T14:34:29.020403Z",
     "iopub.status.busy": "2024-01-02T14:34:29.020156Z",
     "iopub.status.idle": "2024-01-02T14:34:29.051300Z",
     "shell.execute_reply": "2024-01-02T14:34:29.050301Z",
     "shell.execute_reply.started": "2024-01-02T14:34:29.020377Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Similar loop as last time\n",
    "# elev_df_l = []\n",
    "# for h in dfe_elev['dfe'].unique():\n",
    "#     # Subset of fd_id that have this dfe\n",
    "#     struct_sub = dfe_elev[dfe_elev['dfe'] == h].index\n",
    "#     # We want h as an int for the next steps\n",
    "#     h = int(h)\n",
    "#     # Corresponding columns\n",
    "#     ens_col_sub = ['pv_avoid_' + str(h), 'elev_cost_' + str(h),\n",
    "#                    'elev_invst_' + str(h), 'pv_resid_' + str(h),\n",
    "#                    'fd_id', 'sow_ind', eal_col]\n",
    "#     # Corresponding rows and columns\n",
    "#     ens_sub = ens_df.loc[ens_df['fd_id'].isin(struct_sub),\n",
    "#                          ens_col_sub]\n",
    "#     # Rename columns\n",
    "#     ens_sub.columns = ['pv_avoid', 'pv_cost', \n",
    "#                        'elev_invst', 'pv_resid',\n",
    "#                        'fd_id', 'sow_ind', 'base_eal']\n",
    "#     # Add the heightening amount back in\n",
    "#     ens_sub['dfe_elev'] = h\n",
    "\n",
    "#     elev_df_l.append(ens_sub)\n",
    "#     print('Processed rows with dfe height of ' + str(h))\n",
    "\n",
    "# elev_df_f = pd.concat(elev_df_l, axis=0).reset_index(drop=True)\n",
    "\n",
    "# # Write file in FIPS specific exp/ directory\n",
    "# dfe_elev_filename = 'ens_dfe_elev_' + DDF_TYPE + '_' + SCENARIO + '.pqt'\n",
    "# dfe_elev_filep = join(EXP_DIR_I, FIPS, dfe_elev_filename)\n",
    "# elev_df_f.to_parquet(dfe_elev_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195e103-39f1-4f43-af4f-25a5ff37e218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

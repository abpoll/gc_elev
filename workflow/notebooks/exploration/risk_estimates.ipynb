{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08595ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T19:45:18.213660Z",
     "start_time": "2023-03-14T19:45:17.952081Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747a5025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T19:45:23.269780Z",
     "start_time": "2023-03-14T19:45:18.301985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656689/863041077.py:6: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import glob\n",
    "from os.path import join\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2bc1ee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T19:45:23.310791Z",
     "start_time": "2023-03-14T19:45:23.274950Z"
    }
   },
   "outputs": [],
   "source": [
    "# It could make sense to have a lib/ style directory\n",
    "# like PLACES has for common functionality\n",
    "# and this code block would be useful there for getting\n",
    "# a fr() path\n",
    "\n",
    "# Get the absolute path to the precal_hazard directory\n",
    "# Which is two directories above notebooks/exploration/\n",
    "abs_dir = os.path.abspath(Path(os.getcwd()).parents[1])\n",
    "# Get raw data directory\n",
    "fr = join(abs_dir, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "fi = join(abs_dir, 'data', 'interim')\n",
    "# Get processed data directory\n",
    "fp = join(abs_dir, 'data', 'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9821b6",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a2f01f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:38:17.592540Z",
     "start_time": "2023-03-14T20:38:14.783801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load structures (linked to hazard)\n",
    "# Probably more reasonable to load hazard as well\n",
    "# Especially as we go on to take in more hazard grids\n",
    "EXP_DIR = join(fp, 'exposure')\n",
    "EXP_FILEP = join(EXP_DIR, 'nsi_res.gpkg')\n",
    "nsi_res = gpd.read_file(EXP_FILEP)\n",
    "\n",
    "# Load depths\n",
    "DEPTH_FILEP = join(EXP_DIR, 'depths.pqt')\n",
    "depths = pd.read_parquet(DEPTH_FILEP)\n",
    "\n",
    "# Load damage functions\n",
    "# Filepath to NACCS depth damage functions\n",
    "VUL_DIR = join(fp, 'vulnerability')\n",
    "# Read ddfs\n",
    "naccs = pd.read_csv(join(VUL_DIR, 'naccs_ddfs.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0320a",
   "metadata": {},
   "source": [
    "# Subset to 1/2 story residences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1d0669a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:38:17.673709Z",
     "start_time": "2023-03-14T20:38:17.594360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add #story and wb or nb to RES3 homes\n",
    "# Store NB or WB indexed to RES3 homes based on B,C and N found_type\n",
    "# Get num_story + 'S' \n",
    "# Merge these and then add to occtype for RES3 homes\n",
    "\n",
    "# Start with index of res3 homes\n",
    "res3_ind = nsi_res['occtype'].str[:4] == 'RES3'\n",
    "# Get subsetted df\n",
    "res3 = nsi_res.loc[res3_ind]\n",
    "\n",
    "# For this subset\n",
    "# If found_type == B, then WB\n",
    "# Else then NB\n",
    "res3b = np.where(res3['found_type'] == 'B',\n",
    "                 'WB',\n",
    "                 'NB')\n",
    "# For this subset\n",
    "# Get num_story + 'S'\n",
    "res3s = res3['num_story'].astype(str) + 'S'\n",
    "\n",
    "# Adjust occtype column for these homes in nsi_res\n",
    "nsi_res.loc[res3_ind, 'occtype'] = res3['occtype'] + '-' + res3s + res3b\n",
    "\n",
    "# For this case-study, don't use multifamily residences\n",
    "# Drop any RES3 buildings\n",
    "nsi_res_f = nsi_res.loc[~res3_ind]\n",
    "\n",
    "# For this case-study, don't use any building with more \n",
    "# than 2 stories\n",
    "res1_3s_ind = nsi_res_f['num_story'] > 2\n",
    "# Final exposure data\n",
    "res_f = nsi_res_f.loc[~res1_3s_ind]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64beac72",
   "metadata": {},
   "source": [
    "# Calculate losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85eac39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T20:14:29.993305Z",
     "start_time": "2023-02-07T20:14:29.938234Z"
    }
   },
   "source": [
    "## Get inundation depth relative to FFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d032209",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:38:21.438510Z",
     "start_time": "2023-03-14T20:38:21.393020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset for dataframe of columns needed for loss estimates and merging\n",
    "res_loss_cols = ['fd_id', 'occtype', 'found_ht', 'found_type', 'val_struct']\n",
    "res_loss_temp = res_f.loc[:,res_loss_cols].set_index('fd_id')\n",
    "\n",
    "# Merge res_loss with depths\n",
    "res_loss = res_loss_temp.merge(depths, on='fd_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7929d4ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:38:28.132084Z",
     "start_time": "2023-03-14T20:38:28.012944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Columns with depths\n",
    "depth_cols = [x for x in res_loss.columns if 'depth' in x]\n",
    "\n",
    "# Subtract foundation height from depth relative to grade\n",
    "res_loss.loc[:, depth_cols] = res_loss.loc[:, depth_cols].sub(res_loss['found_ht'], axis=0)\n",
    "\n",
    "# Round depth to nearest 10th of a foot\n",
    "res_loss.loc[:, depth_cols] = res_loss.loc[:, depth_cols].round(1)\n",
    "\n",
    "# For basement homes, we need to adjust to the depth of the basement\n",
    "# In the NACCS region, basements are assumed to have\n",
    "# -8 ft relative to FFE as where damage can be incurred\n",
    "# So we'll adjust the depth by 8\n",
    "bhomes = res_loss['found_type'] == 'B'\n",
    "res_loss.loc[bhomes, depth_cols] = res_loss.loc[bhomes, depth_cols].sub(8, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d6db7",
   "metadata": {},
   "source": [
    "## Get the reldam from triangular distribution linked to structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b914740",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:38:30.373253Z",
     "start_time": "2023-03-14T20:38:30.248282Z"
    }
   },
   "outputs": [],
   "source": [
    "# For each occtype and damcat combo we need to \n",
    "# linearly interpolate depths at 10th of foot increments\n",
    "# and get the reldam\n",
    "\n",
    "# Store this in a dataframe with the following shape\n",
    "# occtype & depth_ft index, columns reldam_min/ml/max\n",
    "# This way we can merge on occtype and depth_ft and \n",
    "# Add the correct reldam_min/ml/max as columns to the structures dataframe\n",
    "# You then can call the triangular random generator on those columns\n",
    "# using apply and automatically get the reldam associated with \n",
    "# the structure\n",
    "\n",
    "# Get RES1 DDFs\n",
    "naccs_r = naccs.loc[naccs['occtype'].str[:4] == 'RES1']\n",
    "\n",
    "# Loop through occtype, damcat pairs (groupby)\n",
    "# Subset the naccs_r ddfs\n",
    "# Get a full index\n",
    "# Interpolate reldam between the 10th of foot increments\n",
    "# Create a new fully interpolated dataframe\n",
    "# Append to a list of ddfs\n",
    "# After loop, concat these ddfs\n",
    "\n",
    "# List for interpolated ddfs\n",
    "ddf_list = []\n",
    "\n",
    "# Loop through occtype, damcat pairs\n",
    "for n, gb in naccs_r.groupby(['occtype', 'damcat']):\n",
    "    # Get occtype and damcat\n",
    "    occtype = n[0]\n",
    "    damcat = n[1]\n",
    "    \n",
    "    # Get ddf\n",
    "    ddf = naccs_r.loc[(naccs_r['occtype'] == occtype) &\n",
    "                      (naccs_r['damcat'] == damcat)]\n",
    "    \n",
    "    # Get range\n",
    "    min_d = ddf['depth_ft'].min()\n",
    "    max_d = ddf['depth_ft'].max()\n",
    "    \n",
    "    # Get full index\n",
    "    # Round to nearest 10th of foot\n",
    "    # Add extra 10th of foot for inclusive range\n",
    "    full_ind = pd.Index(np.arange(min_d, max_d + .1, .1).round(1))\n",
    "    \n",
    "    # Subset ddf to depth_ft and reldam\n",
    "    # Set index to depth_ft, reindex on full_ind\n",
    "    # Interpolate, reset & rename index, then add back occtype and damcat\n",
    "    keepcols = ['depth_ft', 'reldam']\n",
    "    ddf = ddf[keepcols].set_index('depth_ft')\n",
    "    ddf_new = ddf.reindex(full_ind).interpolate().reset_index()\n",
    "    # Update DDF depth_ft name to match naming convention in structure data\n",
    "    updatecols = ['depth_ffe', 'reldam']\n",
    "    ddf_new.columns = updatecols\n",
    "    ddf_new['occtype'] = occtype\n",
    "    ddf_new['damcat'] = damcat\n",
    "    ddf_list.append(ddf_new)\n",
    "# Concat for dataframe of ddfs\n",
    "naccs_r_f = pd.concat(ddf_list, axis=0)\n",
    "# pivot the dataframe so that you get reldamMin, \n",
    "# reldamML and reldamMax cols\n",
    "naccs_r_f = naccs_r_f.pivot(index=['depth_ffe', 'occtype'],\n",
    "                            columns=['damcat'],\n",
    "                            values='reldam').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0694b825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:43:05.096485Z",
     "start_time": "2023-03-14T20:43:03.488191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Melt dataframe so that we can easily apply damage curves\n",
    "# Retain occtype, val, depths, and reset index\n",
    "res_loss_f = res_loss.drop(columns=['found_ht', 'found_type']).reset_index()\n",
    "res_loss_f = res_loss_f.melt(id_vars=['fd_id', 'occtype', 'val_struct'],\n",
    "                             var_name='param',\n",
    "                             value_name='depth')\n",
    "# Update parameterization to just str after depth_\n",
    "res_loss_f['param'] = res_loss_f['param'].str.split('_').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f030b07e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:43:05.118144Z",
     "start_time": "2023-03-14T20:43:05.098776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fd_id</th>\n",
       "      <th>occtype</th>\n",
       "      <th>val_struct</th>\n",
       "      <th>param</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570201393</td>\n",
       "      <td>RES1-1SWB</td>\n",
       "      <td>438922.821</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570201400</td>\n",
       "      <td>RES1-2SWB</td>\n",
       "      <td>558333.600</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570201401</td>\n",
       "      <td>RES1-2SWB</td>\n",
       "      <td>411230.358</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570201402</td>\n",
       "      <td>RES1-2SWB</td>\n",
       "      <td>340946.179</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570201403</td>\n",
       "      <td>RES1-2SWB</td>\n",
       "      <td>447718.209</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fd_id    occtype  val_struct   param  depth\n",
       "0  570201393  RES1-1SWB  438922.821  0.0375  -10.0\n",
       "1  570201400  RES1-2SWB  558333.600  0.0375  -10.0\n",
       "2  570201401  RES1-2SWB  411230.358  0.0375  -10.0\n",
       "3  570201402  RES1-2SWB  340946.179  0.0375  -10.0\n",
       "4  570201403  RES1-2SWB  447718.209  0.0375  -10.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_loss_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c00604e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:43:14.390003Z",
     "start_time": "2023-03-14T20:43:06.930446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge DDFs into the structure data\n",
    "res_loss_f = res_loss_f.merge(naccs_r_f,\n",
    "                              left_on=['depth', 'occtype'],\n",
    "                              right_on=['depth_ffe', 'occtype'],\n",
    "                              how='left')\n",
    "# Drop depth_ffe\n",
    "# Fill na with 0\n",
    "res_loss_f = res_loss_f.drop(columns=['depth_ffe'])\n",
    "res_loss_f.loc[:,['ML', 'Max', 'Min']] = res_loss_f.loc[:,['ML', 'Max', 'Min']].fillna(0)\n",
    "\n",
    "\n",
    "# Helper function for drawing triangular distribution from\n",
    "# Min, ML, Max\n",
    "def tri_rd(mindam, mldam, maxdam):\n",
    "    # Function will throw value error if left & right are equal\n",
    "    # In this case, there is no distribution to draw from anyway\n",
    "    # So you can just return any of the values\n",
    "    if mindam == maxdam:\n",
    "        return mindam\n",
    "    return np.random.default_rng().triangular(left=mindam,\n",
    "                                              mode=mldam,\n",
    "                                              right=maxdam)\n",
    "\n",
    "# Get reldam from triangular distribution from (Min, ML, Max)\n",
    "res_loss_f['reldam'] = res_loss_f.apply(lambda x: tri_rd(x['Min'],\n",
    "                                                         x['ML'],\n",
    "                                                         x['Max']),\n",
    "                                        axis=1)\n",
    "\n",
    "# Get structure damage\n",
    "res_loss_f['structdam'] = res_loss_f['reldam'] * res_loss_f['val_struct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15658e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T20:43:46.862148Z",
     "start_time": "2023-03-14T20:43:46.744010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group by parameterizations\n",
    "# Sum over damages to structures\n",
    "# Look at distribution of overall damages\n",
    "res_loss_f.groupby(['param'])['structdam'].sum().describe().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2202539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As of now, not writing out any of the damages to file\n",
    "# The damages are calculated in memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icom_risk",
   "language": "python",
   "name": "icom_risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "167.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

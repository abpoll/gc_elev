{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07ae3738-6ce1-4625-9777-cb42c218896b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:35:57.433728Z",
     "iopub.status.busy": "2024-03-15T17:35:57.433413Z",
     "iopub.status.idle": "2024-03-15T17:35:57.635180Z",
     "shell.execute_reply": "2024-03-15T17:35:57.633572Z",
     "shell.execute_reply.started": "2024-03-15T17:35:57.433703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cf7aaad-086f-40a9-92e4-374d1efb7495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:35:57.637936Z",
     "iopub.status.busy": "2024-03-15T17:35:57.637463Z",
     "iopub.status.idle": "2024-03-15T17:35:57.684206Z",
     "shell.execute_reply": "2024-03-15T17:35:57.682438Z",
     "shell.execute_reply.started": "2024-03-15T17:35:57.637891Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import shape\n",
    "import rasterio \n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ae98b55-a0e0-4e75-b3f3-b9dcc294234c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:35:57.686366Z",
     "iopub.status.busy": "2024-03-15T17:35:57.685878Z",
     "iopub.status.idle": "2024-03-15T17:35:57.725422Z",
     "shell.execute_reply": "2024-03-15T17:35:57.723976Z",
     "shell.execute_reply.started": "2024-03-15T17:35:57.686323Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec963f-fac9-4bb2-ac41-364fde7dbcb8",
   "metadata": {},
   "source": [
    "# Process - everything ends up at county level and clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879462d-4490-4f69-88bf-15efa5de8d78",
   "metadata": {},
   "source": [
    "## Process clip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "462240d3-88c1-461a-bcbd-22f0585387f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:35:57.729183Z",
     "iopub.status.busy": "2024-03-15T17:35:57.728693Z",
     "iopub.status.idle": "2024-03-15T17:35:57.765945Z",
     "shell.execute_reply": "2024-03-15T17:35:57.764598Z",
     "shell.execute_reply.started": "2024-03-15T17:35:57.729137Z"
    }
   },
   "outputs": [],
   "source": [
    "# For our case study, we are going to focus on Gloucester City, NJ\n",
    "# Our config.yaml loads in a county indexed clip file\n",
    "# so that we can restrict all our data to the GC boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4108629a-beb2-44ba-94a2-634f69a44dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:35:57.767648Z",
     "iopub.status.busy": "2024-03-15T17:35:57.767319Z",
     "iopub.status.idle": "2024-03-15T17:35:57.809520Z",
     "shell.execute_reply": "2024-03-15T17:35:57.808235Z",
     "shell.execute_reply.started": "2024-03-15T17:35:57.767615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data we downloaded from the county's REST API server\n",
    "clip_filep = join(REF_DIR_R, FIPS, 'clip.json')\n",
    "with open(clip_filep) as f:\n",
    "    clip_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b08a969c-fd99-4dae-ae15-041f5be159f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:35:57.811161Z",
     "iopub.status.busy": "2024-03-15T17:35:57.810854Z",
     "iopub.status.idle": "2024-03-15T17:35:58.032567Z",
     "shell.execute_reply": "2024-03-15T17:35:58.031563Z",
     "shell.execute_reply.started": "2024-03-15T17:35:57.811131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use pandas to get the data in a form that is easier\n",
    "# to turn into a geodataframe for clipping\n",
    "clip_df = pd.json_normalize(clip_data['features'])\n",
    "# We want to make a polygon out of the geometry coordinates\n",
    "# We can access that from the original json object\n",
    "clip_geo = [shape(i['geometry']) for i in clip_data['features']]\n",
    "# We can create a geodataframe of clip_df by adding clip_geo\n",
    "# as its geometry column\n",
    "clip_gdf = gpd.GeoDataFrame(clip_df,\n",
    "                            crs=CLIP_CRS,\n",
    "                            geometry=clip_geo)\n",
    "\n",
    "# We can clean up the gdf by removing the\n",
    "# type, id, geometry.type and geometry.coordinates columns\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "clip_gdf = clip_gdf.drop(columns=drop_col)\n",
    "\n",
    "# Write the file out to interim\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "prepare_saving(clip_out_filep)\n",
    "clip_gdf.to_file(clip_out_filep,\n",
    "                 driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb2e91-abce-44c5-8a49-14efa3cae4e4",
   "metadata": {},
   "source": [
    "## Process NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b30dc5b6-24f2-4552-a274-465aab776350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:35:58.033204Z",
     "iopub.status.busy": "2024-03-15T17:35:58.033049Z",
     "iopub.status.idle": "2024-03-15T17:35:58.048153Z",
     "shell.execute_reply": "2024-03-15T17:35:58.047146Z",
     "shell.execute_reply.started": "2024-03-15T17:35:58.033190Z"
    }
   },
   "outputs": [],
   "source": [
    "# The NSI comes with all the data necessary for performing a standard \n",
    "# flood risk assessment. It is still useful to process the raw data.\n",
    "# Here, we subset to residential properties with 1 to 2 stories\n",
    "# and save as a geodataframe. These are the types of residences we have\n",
    "# multiple depth-damage functions for and a literature base to draw \n",
    "# from to introduce uncertainty in these loss estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80d179e9-26c9-4f13-88c8-c54d7c1188c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:35:58.048710Z",
     "iopub.status.busy": "2024-03-15T17:35:58.048571Z",
     "iopub.status.idle": "2024-03-15T17:36:03.922072Z",
     "shell.execute_reply": "2024-03-15T17:36:03.921331Z",
     "shell.execute_reply.started": "2024-03-15T17:35:58.048697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read NSI\n",
    "nsi_filep = join(EXP_DIR_R, FIPS, 'nsi.json')\n",
    "with open(nsi_filep, 'r') as fp:\n",
    "    nsi_full = json.load(fp)\n",
    "\n",
    "# json normalize \n",
    "nsi_df = pd.json_normalize(nsi_full['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b268cd77-1947-4d88-8e34-b6010345b208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:03.922805Z",
     "iopub.status.busy": "2024-03-15T17:36:03.922690Z",
     "iopub.status.idle": "2024-03-15T17:36:04.291212Z",
     "shell.execute_reply": "2024-03-15T17:36:04.290726Z",
     "shell.execute_reply.started": "2024-03-15T17:36:03.922794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to gdf\n",
    "# This is useful for some spatial joins we need to perform\n",
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_df['properties.x'],\n",
    "                              nsi_df['properties.y'])\n",
    "nsi_gdf = gpd.GeoDataFrame(nsi_df, geometry=geometry,\n",
    "                           crs=NSI_CRS)\n",
    "\n",
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dacc2ed-62c5-481c-8b0c-9c2b173d8604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:04.293007Z",
     "iopub.status.busy": "2024-03-15T17:36:04.292838Z",
     "iopub.status.idle": "2024-03-15T17:36:04.471453Z",
     "shell.execute_reply": "2024-03-15T17:36:04.470962Z",
     "shell.execute_reply.started": "2024-03-15T17:36:04.292992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to residential properties and update\n",
    "# RES 1 - single family\n",
    "# RES 2 - manufactured home\n",
    "# RES 3 - multifamily (but could fit into a depth-damage function\n",
    "# archetype depending on # stories)\n",
    "# We are going to use RES1 for this case-study\n",
    "# It is the only occtype with hazus and naccs\n",
    "# DDFs and has less ambiguous classification\n",
    "\n",
    "# occtype category for easier use in loss estimation steps\n",
    "\n",
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['occtype'].str[:4] == 'RES1']\n",
    "\n",
    "# For this case-study, don't use any building with more \n",
    "# than 2 stories\n",
    "res1_3s_ind = nsi_res['num_story'] > 2\n",
    "# Final residential dataframe\n",
    "res_f = nsi_res.loc[~res1_3s_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dca1d9e1-e9f1-4c5b-b55a-6c80e1f18b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:04.472225Z",
     "iopub.status.busy": "2024-03-15T17:36:04.472061Z",
     "iopub.status.idle": "2024-03-15T17:36:04.582259Z",
     "shell.execute_reply": "2024-03-15T17:36:04.581768Z",
     "shell.execute_reply.started": "2024-03-15T17:36:04.472211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to relevant columns\n",
    "cols = ['fd_id', 'occtype', 'found_type', 'cbfips', 'bldgtype',\n",
    "        'ftprntsrc', 'found_ht', 'val_struct', 'sqft',\n",
    "        'val_cont', 'source', 'firmzone', 'ground_elv_m',\n",
    "        'geometry']\n",
    "\n",
    "res_out = res_f.loc[:,cols]\n",
    "\n",
    "# Clip to our clip boundary\n",
    "# They are in the same CRS\n",
    "nsi_clip_out = gpd.clip(res_out, clip_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5fffede-d15f-4a04-8bc0-7865247078a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:04.583050Z",
     "iopub.status.busy": "2024-03-15T17:36:04.582883Z",
     "iopub.status.idle": "2024-03-15T17:36:05.178791Z",
     "shell.execute_reply": "2024-03-15T17:36:05.178074Z",
     "shell.execute_reply.started": "2024-03-15T17:36:04.583035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out to interim/exposure/FIPS/\n",
    "# Single family homes -- sf\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "prepare_saving(EXP_OUT_FILEP)\n",
    "# Limit to sqft <= 99th percentile\n",
    "# Arbitrary cutoff. The max value from the steps above\n",
    "# is 400858 which is way too large\n",
    "# There are other large values that are dropped with this\n",
    "# arbitrary cutoff\n",
    "# For GC case study, this value is 2696.41999\n",
    "sqft_clip = nsi_clip_out['sqft'].quantile(.99)\n",
    "nsi_clip_out[nsi_clip_out['sqft'] <= sqft_clip].to_file(EXP_OUT_FILEP,\n",
    "                                                        driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6fa61-56df-40a0-9c3f-219d09d52597",
   "metadata": {},
   "source": [
    "## Process Depth-Damage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2558d82-b45e-4249-8bd8-c416d0d5ca37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:05.179419Z",
     "iopub.status.busy": "2024-03-15T17:36:05.179299Z",
     "iopub.status.idle": "2024-03-15T17:36:05.194332Z",
     "shell.execute_reply": "2024-03-15T17:36:05.193686Z",
     "shell.execute_reply.started": "2024-03-15T17:36:05.179407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read depth damage functions\n",
    "ddf_filedir = join(VULN_DIR_UZ, \"physical\", NATION)\n",
    "naccs = pd.read_csv(join(ddf_filedir, \"naccs_ddfs.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e508c-7c50-4a49-8665-56267f8de4e6",
   "metadata": {},
   "source": [
    "### NACCS DDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f838a2d4-2491-449b-92dc-fc345ceea1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:05.194885Z",
     "iopub.status.busy": "2024-03-15T17:36:05.194776Z",
     "iopub.status.idle": "2024-03-15T17:36:05.228938Z",
     "shell.execute_reply": "2024-03-15T17:36:05.228246Z",
     "shell.execute_reply.started": "2024-03-15T17:36:05.194874Z"
    }
   },
   "outputs": [],
   "source": [
    "# For NACCS, we have the RES 1 DDFs\n",
    "# First, subset to the relevant Occupancy types\n",
    "# We want to end up with ddf ids 1swb, open, etc.\n",
    "# don't need to keep the RES1- part for this case study\n",
    "naccs['res_type'] = naccs['Occupancy'].str.split('-').str[0]\n",
    "naccs['bld_type'] = naccs['Occupancy'].str.split('-').str[1]\n",
    "occ_types = ['1SWB', '2SWB', '1SNB', '2SNB']\n",
    "naccs_res = naccs.loc[(naccs['bld_type'].isin(occ_types)) &\n",
    "                      ((naccs['res_type'] == 'RES1') |\n",
    "                       (naccs['res_type'] == 'RES'))]\n",
    "\n",
    "# Next, drop columns we don't need\n",
    "drop_cols = ['Description', 'Source', 'Occupancy', 'res_type']\n",
    "naccs_res = naccs_res.drop(columns=drop_cols)\n",
    "\n",
    "# Rename DamageCategory\n",
    "naccs_res = naccs_res.rename(columns={'DamageCategory': 'dam_cat',\n",
    "                                      'bld_type': 'ddf_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50cd8f69-6b40-47a5-a9e0-ce956e07f956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:05.229543Z",
     "iopub.status.busy": "2024-03-15T17:36:05.229432Z",
     "iopub.status.idle": "2024-03-15T17:36:05.292678Z",
     "shell.execute_reply": "2024-03-15T17:36:05.291969Z",
     "shell.execute_reply.started": "2024-03-15T17:36:05.229533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now get the melted dataframe\n",
    "idvars = ['ddf_id', 'dam_cat']\n",
    "naccs_melt = tidy_ddfs(naccs_res, idvars)\n",
    "\n",
    "# Drop columns we don't need\n",
    "drop_cols = ['depth_str', 'pct_dam']\n",
    "naccs_f = naccs_melt.drop(columns=drop_cols)\n",
    "\n",
    "# We want to pivot the dataframe so that Min/ML/Max are our columns\n",
    "naccs_piv = naccs_f.pivot(index=['ddf_id', 'depth_ft'],\n",
    "                          columns='dam_cat')['rel_dam'].reset_index()\n",
    "\n",
    "\n",
    "# We do the interpolation again\n",
    "df_int_list = []\n",
    "for ddf_id, df in naccs_piv.groupby('ddf_id'):\n",
    "    # This creates the duplicate rows\n",
    "    ddf_int = df.loc[np.repeat(df.index, 10)].reset_index(drop=True)\n",
    "    # Now we have to make them nulls by finding\n",
    "    # the \"original\" indexed rows\n",
    "    ddf_int.loc[ddf_int.index % 10 != 0,\n",
    "                ['depth_ft', 'ML', 'Max', 'Min']] = np.nan\n",
    "    # Now we interpolate\n",
    "    ddf_int = ddf_int.interpolate().round(2)\n",
    "    # Drop duplicate rows (this happens for the max depth values)\n",
    "    ddf_int = ddf_int.drop_duplicates()\n",
    "    # And append\n",
    "    df_int_list.append(ddf_int)\n",
    "naccs_ddfs = pd.concat(df_int_list, axis=0)\n",
    "\n",
    "# We want to obtain our 'params' column\n",
    "# same as above\n",
    "p_cols = ['Min', 'ML', 'Max']\n",
    "tri_params = naccs_ddfs[p_cols].values\n",
    "# Drop the p_cols\n",
    "naccs_out = naccs_ddfs.drop(columns=p_cols)\n",
    "naccs_out = naccs_out.assign(params=tri_params.tolist())\n",
    "\n",
    "# Get out dict of max depths\n",
    "NACCS_MAX_DICT = ddf_max_depth_dict(naccs_out.reset_index(drop=True),\n",
    "                                    'params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84161e-2cd2-4bbf-aeb2-96d68386c666",
   "metadata": {},
   "source": [
    "### Save our processed ddfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ffbc854-d3f2-4e3d-ae8b-554426645f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:05.293392Z",
     "iopub.status.busy": "2024-03-15T17:36:05.293237Z",
     "iopub.status.idle": "2024-03-15T17:36:05.315672Z",
     "shell.execute_reply": "2024-03-15T17:36:05.314948Z",
     "shell.execute_reply.started": "2024-03-15T17:36:05.293377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main directory\n",
    "ddf_out_dir = join(VULN_DIR_I, 'physical')\n",
    "# Main ddf files\n",
    "naccs_out_filep = join(ddf_out_dir, 'naccs_ddfs.pqt')\n",
    "# Dictionaries - save as .json for simplicity\n",
    "naccs_max_filep = join(ddf_out_dir, 'naccs.json')\n",
    "\n",
    "# Only need to call this for one of the files\n",
    "# since they share the same parent directory\n",
    "prepare_saving(naccs_out_filep)\n",
    "\n",
    "# Save as parquet files since\n",
    "# these will directly read in the\n",
    "# DDF params as a list, not as a string\n",
    "naccs_out.to_parquet(naccs_out_filep)\n",
    "\n",
    "# Save the json files\n",
    "with open(naccs_max_filep, 'w') as fp:\n",
    "    json.dump(NACCS_MAX_DICT, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77a02a-d329-479f-8355-93f0a34ded2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T17:55:19.418873Z",
     "iopub.status.busy": "2023-10-20T17:55:19.418001Z",
     "iopub.status.idle": "2023-10-20T17:55:21.014322Z",
     "shell.execute_reply": "2023-10-20T17:55:21.012336Z",
     "shell.execute_reply.started": "2023-10-20T17:55:19.418819Z"
    }
   },
   "source": [
    "## Process Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5c51a0c-0455-4a05-8e62-6708bd75e67d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:05.316475Z",
     "iopub.status.busy": "2024-03-15T17:36:05.316313Z",
     "iopub.status.idle": "2024-03-15T17:36:05.381618Z",
     "shell.execute_reply": "2024-03-15T17:36:05.380992Z",
     "shell.execute_reply.started": "2024-03-15T17:36:05.316460Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "clip_gdf = gpd.read_file(clip_out_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "881d0eee-bee3-45fe-97a8-f4f2eeda0bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:36:05.382168Z",
     "iopub.status.busy": "2024-03-15T17:36:05.382058Z",
     "iopub.status.idle": "2024-03-15T17:37:52.264024Z",
     "shell.execute_reply": "2024-03-15T17:37:52.263304Z",
     "shell.execute_reply.started": "2024-03-15T17:36:05.382157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ref: block\n",
      "Saved Ref: bg\n",
      "Saved Ref: tract\n",
      "Saved Ref: county\n",
      "Saved Ref: zcta\n"
     ]
    }
   ],
   "source": [
    "# For each .shp file in our unzipped ref directory\n",
    "# we are going to reproject & clip, then write out\n",
    "for path in Path(REF_DIR_UZ).rglob('*.shp'):\n",
    "    # Read in the file\n",
    "    ref_shp = gpd.read_file(path)\n",
    "    \n",
    "    # Process the filename to figure out what \n",
    "    # reference data this is\n",
    "    # the files are written out in the form of\n",
    "    # tl_2022_34_tract.shp, for example\n",
    "    # so we split the string on '_', take the\n",
    "    # last element of the array, and ignore\n",
    "    # the last 4 characters\n",
    "    ref_name = path.name.split('_')[-1][:-4]\n",
    "    # Replace the ref name with our ref_name dict values\n",
    "    ref_name_out = REF_NAMES_DICT[ref_name]\n",
    "\n",
    "    # Reproject and clip our reference shapefile\n",
    "    ref_reproj = ref_shp.to_crs(clip_gdf.crs)\n",
    "    ref_clipped = gpd.clip(ref_reproj, clip_gdf)\n",
    "    \n",
    "    # Write file\n",
    "    ref_out_filep = join(REF_DIR_I, FIPS, ref_name_out + \".gpkg\")\n",
    "    prepare_saving(ref_out_filep)\n",
    "    ref_clipped.to_file(ref_out_filep,\n",
    "                        driver='GPKG')\n",
    "\n",
    "    # Helpful message to track progress\n",
    "    print(\"Saved Ref: \" + ref_name_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe02b8-4e96-4bbf-8048-bfb4b9fda231",
   "metadata": {},
   "source": [
    "## Process Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4a17a46-2d52-4a78-a910-95b86d846fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:37:52.264800Z",
     "iopub.status.busy": "2024-03-15T17:37:52.264670Z",
     "iopub.status.idle": "2024-03-15T17:37:52.616833Z",
     "shell.execute_reply": "2024-03-15T17:37:52.616145Z",
     "shell.execute_reply.started": "2024-03-15T17:37:52.264787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load relevant spatial data (tract, block group)\n",
    "tract_filep = join(REF_DIR_I, FIPS, 'tract.gpkg')\n",
    "bg_filep = join(REF_DIR_I, FIPS, 'bg.gpkg')\n",
    "tract_geo = gpd.read_file(tract_filep)\n",
    "bg_geo = gpd.read_file(bg_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca5d827b-3879-4e5a-a544-90091475a965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:37:52.617560Z",
     "iopub.status.busy": "2024-03-15T17:37:52.617424Z",
     "iopub.status.idle": "2024-03-15T17:37:53.234831Z",
     "shell.execute_reply": "2024-03-15T17:37:53.234393Z",
     "shell.execute_reply.started": "2024-03-15T17:37:52.617548Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3399672/4066402225.py:3: DtypeWarning: Columns (18,26,70,72,85,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cejst = pd.read_csv(ce_filep, dtype={'Census tract 2010 ID': 'str'})\n"
     ]
    }
   ],
   "source": [
    "# CEJST data\n",
    "ce_filep = join(VULN_DIR_R, 'social', NATION, 'cejst.csv')\n",
    "cejst = pd.read_csv(ce_filep, dtype={'Census tract 2010 ID': 'str'})\n",
    "\n",
    "# Columns to keep\n",
    "# Identified as disadvantaged\n",
    "# Census tract 2010 ID\n",
    "keep_cols = ['Census tract 2010 ID', 'Identified as disadvantaged']\n",
    "cejst_sub = cejst[keep_cols]\n",
    "# Rename columns\n",
    "cejst_sub.columns = ['GEOID', 'disadvantaged']\n",
    "\n",
    "# Merge with tract_geo\n",
    "cejst_f = tract_geo[['GEOID', 'geometry']].merge(cejst_sub,\n",
    "                                                 on='GEOID',\n",
    "                                                 how='inner')\n",
    "\n",
    "# Retain only the disadvantaged \n",
    "cejst_f = cejst_f[cejst_f['disadvantaged'] == True].drop(columns='disadvantaged')\n",
    "\n",
    "# Write file\n",
    "cejst_out_filep = join(VULN_DIR_I, 'social', FIPS, 'cejst.gpkg')\n",
    "prepare_saving(cejst_out_filep)\n",
    "cejst_f.to_file(cejst_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72b8abed-5fa4-4e90-99cc-c6ff83beede1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:37:53.235575Z",
     "iopub.status.busy": "2024-03-15T17:37:53.235421Z",
     "iopub.status.idle": "2024-03-15T17:37:54.618721Z",
     "shell.execute_reply": "2024-03-15T17:37:54.617988Z",
     "shell.execute_reply.started": "2024-03-15T17:37:53.235561Z"
    }
   },
   "outputs": [],
   "source": [
    "# NJ overburdened data\n",
    "\n",
    "# Read data\n",
    "ovb_filep = join(VULN_DIR_UZ, 'social', STATEABBR,\n",
    "                 'Govt_census_group_2022_EJ.gdb')\n",
    "ovb = gpd.read_file(ovb_filep)\n",
    "\n",
    "# Rename some columns\n",
    "ovb = ovb.rename(columns={'OVERBURDENED_COMMUNITY_CRITERI': 'ovb_crit'})\n",
    "\n",
    "# Keep a subset of columns\n",
    "ovb_f = ovb[['GEOID', 'ovb_crit', 'geometry']]\n",
    "\n",
    "# The data already is limited to overburdened categories\n",
    "\n",
    "# Subset to our study area\n",
    "ovb_reproj = ovb_f.to_crs(clip_gdf.crs)\n",
    "ovb_clipped = gpd.clip(ovb_reproj, clip_gdf)\n",
    "\n",
    "# Write file\n",
    "ovb_out_filep = join(VULN_DIR_I, 'social', FIPS, 'ovb.gpkg')\n",
    "ovb_clipped.to_file(ovb_out_filep, driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad2c5e50-cae9-4bd0-87d9-475a3ea93609",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:37:54.619447Z",
     "iopub.status.busy": "2024-03-15T17:37:54.619316Z",
     "iopub.status.idle": "2024-03-15T17:37:55.535384Z",
     "shell.execute_reply": "2024-03-15T17:37:55.534559Z",
     "shell.execute_reply.started": "2024-03-15T17:37:54.619435Z"
    }
   },
   "outputs": [],
   "source": [
    "# CDC SVI data\n",
    "svi_filename = 'svi.csv'\n",
    "svi_filep = join(VULN_DIR_R, 'social', NATION, svi_filename)\n",
    "svi = pd.read_csv(svi_filep)\n",
    "\n",
    "# Subset columns\n",
    "# The overall summary ranking variable is RPL_THEMES\n",
    "# From https://www.atsdr.cdc.gov/placeandhealth/svi/\n",
    "# documentation/SVI_documentation_2020.html\n",
    "keep_cols = ['FIPS', 'RPL_THEMES']\n",
    "svi_high = svi[keep_cols]\n",
    "\n",
    "# Rename FIPS to GEOID\n",
    "# Rename RPL_THEMES to sovi\n",
    "# GEOID needs to be a str, 11 characters long\n",
    "svi_high = svi_high.rename(columns={'FIPS': 'GEOID',\n",
    "                                    'RPL_THEMES': 'sovi'})\n",
    "svi_high['GEOID'] = svi_high['GEOID'].astype(str).str.zfill(11)\n",
    "\n",
    "# Subset to tracts in our study area (using the tract_geo geometries)\n",
    "svi_f = tract_geo[['GEOID', 'geometry']].merge(svi_high,\n",
    "                                               on='GEOID',\n",
    "                                               how='inner')\n",
    "\n",
    "# Write out file\n",
    "sovi_out_filep = join(VULN_DIR_I, 'social', FIPS, 'sovi.gpkg')\n",
    "svi_f.to_file(sovi_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c10c115-8995-4da8-80fb-6f69d9f60b96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:37:55.536204Z",
     "iopub.status.busy": "2024-03-15T17:37:55.536072Z",
     "iopub.status.idle": "2024-03-15T17:38:21.817859Z",
     "shell.execute_reply": "2024-03-15T17:38:21.817159Z",
     "shell.execute_reply.started": "2024-03-15T17:37:55.536191Z"
    }
   },
   "outputs": [],
   "source": [
    "# LMI data\n",
    "# Read data\n",
    "lmi_filename = 'ACS_2015_lowmod_blockgroup_all.xlsx'\n",
    "lmi_filep = join(VULN_DIR_R, 'social', NATION, lmi_filename)\n",
    "lmi = pd.read_excel(lmi_filep, engine='openpyxl')\n",
    "# Get GEOID for merge (last 12 characters is the bg id)\n",
    "lmi['GEOID'] = lmi['GEOID'].str[-12:]\n",
    "\n",
    "# Retain GEOID and Lowmod_pct\n",
    "keep_cols = ['GEOID', 'Lowmod_pct']\n",
    "lmi_f = bg_geo[['GEOID', 'geometry']].merge(lmi[keep_cols],\n",
    "                                            on='GEOID',\n",
    "                                            how='inner')\n",
    "\n",
    "# Write file\n",
    "lmi_out_filep = join(VULN_DIR_I, 'social', FIPS, 'lmi.gpkg')\n",
    "lmi_f.to_file(lmi_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e07e31-71f1-46f0-a6a6-6bf8f1c5fa3f",
   "metadata": {},
   "source": [
    "# Link everything to NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "044aa2f8-8802-4cb0-be12-8b7baf88e750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:38:21.818727Z",
     "iopub.status.busy": "2024-03-15T17:38:21.818593Z",
     "iopub.status.idle": "2024-03-15T17:38:22.303265Z",
     "shell.execute_reply": "2024-03-15T17:38:22.302533Z",
     "shell.execute_reply.started": "2024-03-15T17:38:21.818714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just for jupyter notebooks\n",
    "# Scripts don't need to reload the data since it runs all at once\n",
    "# Jupyter is more for development, and might only run\n",
    "# some sections at a time\n",
    "# Using same names as above for consistency\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f461bca-7b68-43d3-b4c7-dc7c54a8898a",
   "metadata": {},
   "source": [
    "## Link NSI with Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3b1bf23-e259-421c-90af-926d59e9b6a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:38:22.303971Z",
     "iopub.status.busy": "2024-03-15T17:38:22.303842Z",
     "iopub.status.idle": "2024-03-15T17:38:22.759902Z",
     "shell.execute_reply": "2024-03-15T17:38:22.759196Z",
     "shell.execute_reply.started": "2024-03-15T17:38:22.303958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "862b61c3-50ff-45d2-89bd-2c3db1efde76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:38:22.760440Z",
     "iopub.status.busy": "2024-03-15T17:38:22.760321Z",
     "iopub.status.idle": "2024-03-15T17:38:23.094073Z",
     "shell.execute_reply": "2024-03-15T17:38:23.093386Z",
     "shell.execute_reply.started": "2024-03-15T17:38:22.760429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked reference to NSI: tract_id\n",
      "Linked reference to NSI: block_id\n",
      "Linked reference to NSI: bg_id\n",
      "Linked reference to NSI: zcta_id\n"
     ]
    }
   ],
   "source": [
    "# For zcta, tract, bg, and block\n",
    "# we want to do spatial joins to link\n",
    "# up fd_id in the NSI with the ref\n",
    "# We will use config data to do this\n",
    "# since other references may be brought in \n",
    "# down the line\n",
    "# We are going to store fd_id/ref_id links in a dataframe\n",
    "ref_df_list = []\n",
    "for ref_name, ref_id in REF_ID_NAMES_DICT.items():\n",
    "    ref_filep = join(REF_DIR_I, FIPS, ref_name + \".gpkg\")\n",
    "\n",
    "    # Load in the ref file\n",
    "    ref_geo = gpd.read_file(ref_filep)\n",
    "\n",
    "    # Limit the geodataframe to our ref id and 'geometry' column\n",
    "    keep_col = [ref_id, 'geometry']\n",
    "    ref_geo_sub = ref_geo[keep_col]\n",
    "\n",
    "    # Limit the NSI to our fd_id and geometry column\n",
    "    keep_col_nsi = ['fd_id', 'geometry']\n",
    "    nsi_sub = nsi_clip_out[keep_col_nsi]\n",
    "\n",
    "    # Reproj nsi_sub to the reference crs\n",
    "    nsi_reproj = nsi_sub.to_crs(ref_geo.crs)\n",
    "\n",
    "    # Do a spatial join\n",
    "    nsi_ref = gpd.sjoin(nsi_reproj, ref_geo_sub, predicate='within')\n",
    "\n",
    "    # Set index to fd_id and just keep the ref_id\n",
    "    # Rename that column to our ref_name + '_id'\n",
    "    # Append this to our ref_df_list\n",
    "    nsi_ref_f = nsi_ref.set_index('fd_id')[[ref_id]]\n",
    "    nsi_ref_f = nsi_ref_f.rename(columns={ref_id: ref_name + '_id'})\n",
    "    ref_df_list.append(nsi_ref_f)\n",
    "\n",
    "    # Helpful message\n",
    "    print('Linked reference to NSI: ' + ref_name + '_id')\n",
    "\n",
    "# Can concat and write\n",
    "nsi_refs = pd.concat(ref_df_list, axis=1).reset_index()\n",
    "ref_filep = join(EXP_DIR_I,  FIPS, 'nsi_ref.pqt')\n",
    "prepare_saving(ref_filep)\n",
    "nsi_refs.to_parquet(ref_filep)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48257494-dca6-4c15-9397-509a9a22450e",
   "metadata": {},
   "source": [
    "## Link NSI with Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a26e01cb-8048-418e-b8e5-ee1bdda75578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:38:23.094710Z",
     "iopub.status.busy": "2024-03-15T17:38:23.094583Z",
     "iopub.status.idle": "2024-03-15T17:38:23.557324Z",
     "shell.execute_reply": "2024-03-15T17:38:23.556613Z",
     "shell.execute_reply.started": "2024-03-15T17:38:23.094698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "# Read in single family home gpkg\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2a1cfee-ec4c-41c0-aaa8-217c5dbc82aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T17:43:29.164264Z",
     "iopub.status.busy": "2024-03-15T17:43:29.163688Z",
     "iopub.status.idle": "2024-03-15T17:43:29.746871Z",
     "shell.execute_reply": "2024-03-15T17:43:29.745745Z",
     "shell.execute_reply.started": "2024-03-15T17:43:29.164216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked vulnerability to NSI: lmi\n",
      "Linked vulnerability to NSI: sovi\n",
      "Linked vulnerability to NSI: ovb\n",
      "Linked vulnerability to NSI: cejst\n"
     ]
    }
   ],
   "source": [
    "# Read in processed sovi data\n",
    "# Loop through the community boundary data\n",
    "# Get links to the single family home data\n",
    "# Store in single dataframe\n",
    "# Write out\n",
    "\n",
    "sovi_dir = join(VULN_DIR_I, 'social', FIPS)\n",
    "filenames = ['lmi', 'sovi', 'ovb', 'cejst']\n",
    "\n",
    "sovi_df_list = []\n",
    "# Let's add a list of just fd_id\n",
    "# This makes sure every property is\n",
    "# linked to the social vulnerability categories\n",
    "sovi_df_list.append(nsi_clip_out[['fd_id']].set_index('fd_id'))\n",
    "\n",
    "for fn in filenames:\n",
    "    # Read in each gpkg\n",
    "    fp = join(sovi_dir, fn + '.gpkg')\n",
    "    sovi_geo = gpd.read_file(fp)\n",
    "\n",
    "    # Subset sovi_geo based on thresholds\n",
    "    # For cejst and ovb this is already done\n",
    "    # For lmi and ovb need to do the filter as follows\n",
    "    if fn == 'lmi':\n",
    "        # See https://www.hudoig.gov/reports-publications/\n",
    "        # report/cdbg-dr-program-generally-\n",
    "        # met-low-and-moderate-income-requirements\n",
    "        # The statutory threshold is 50%, so retain those\n",
    "        sovi_sub = sovi_geo[sovi_geo['Lowmod_pct'] > .5]\n",
    "    elif fn == 'sovi':\n",
    "        # Subset to threshhold for FMA (from 2022 NOFO)\n",
    "        sovi_sub = sovi_geo[sovi_geo['sovi'] > .6]\n",
    "    elif fn == 'ovb':\n",
    "        sovi_sub = sovi_geo[sovi_geo['ovb_crit'] != 'Adjacent']\n",
    "    else:\n",
    "        sovi_sub = sovi_geo\n",
    "\n",
    "    # Only need the geometry for sovi_sub\n",
    "    sovi_sub = sovi_sub[['geometry']]\n",
    "    \n",
    "    # Limit the NSI to our fd_id and geometry column\n",
    "    keep_col_nsi = ['fd_id', 'geometry']\n",
    "    nsi_sub = nsi_clip_out[keep_col_nsi]\n",
    "\n",
    "    # Reproj nsi_sub to the reference crs\n",
    "    nsi_reproj = nsi_sub.to_crs(sovi_geo.crs)\n",
    "\n",
    "    # Do a spatial join\n",
    "    nsi_sovi = gpd.sjoin(nsi_reproj, sovi_sub, predicate='within')\n",
    "\n",
    "    # Add indicator column\n",
    "    nsi_sovi[fn] = True\n",
    "\n",
    "    # Append this to our sovi_df_list\n",
    "    sovi_df_list.append(nsi_sovi[['fd_id', fn]].set_index('fd_id'))\n",
    "\n",
    "    # Helpful message\n",
    "    print('Linked vulnerability to NSI: ' + fn)\n",
    "\n",
    "sovi_df_f = pd.concat(sovi_df_list, axis=1).fillna(False)\n",
    "sovi_out_filepath = join(sovi_dir, 'c_indicators.pqt')\n",
    "sovi_df_f.to_parquet(sovi_out_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

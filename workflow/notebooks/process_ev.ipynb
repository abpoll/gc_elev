{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ae3738-6ce1-4625-9777-cb42c218896b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:35.698700Z",
     "iopub.status.busy": "2024-02-29T20:17:35.698364Z",
     "iopub.status.idle": "2024-02-29T20:17:35.862202Z",
     "shell.execute_reply": "2024-02-29T20:17:35.861521Z",
     "shell.execute_reply.started": "2024-02-29T20:17:35.698671Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf7aaad-086f-40a9-92e4-374d1efb7495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:36.259180Z",
     "iopub.status.busy": "2024-02-29T20:17:36.258721Z",
     "iopub.status.idle": "2024-02-29T20:17:38.021905Z",
     "shell.execute_reply": "2024-02-29T20:17:38.021043Z",
     "shell.execute_reply.started": "2024-02-29T20:17:36.259146Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import shape\n",
    "import rasterio \n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae98b55-a0e0-4e75-b3f3-b9dcc294234c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:38.025940Z",
     "iopub.status.busy": "2024-02-29T20:17:38.023342Z",
     "iopub.status.idle": "2024-02-29T20:17:38.043984Z",
     "shell.execute_reply": "2024-02-29T20:17:38.043394Z",
     "shell.execute_reply.started": "2024-02-29T20:17:38.025905Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec963f-fac9-4bb2-ac41-364fde7dbcb8",
   "metadata": {},
   "source": [
    "# Process - everything ends up at county level and clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879462d-4490-4f69-88bf-15efa5de8d78",
   "metadata": {},
   "source": [
    "## Process clip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462240d3-88c1-461a-bcbd-22f0585387f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:39.252016Z",
     "iopub.status.busy": "2024-02-29T20:17:39.251614Z",
     "iopub.status.idle": "2024-02-29T20:17:39.271661Z",
     "shell.execute_reply": "2024-02-29T20:17:39.271080Z",
     "shell.execute_reply.started": "2024-02-29T20:17:39.251984Z"
    }
   },
   "outputs": [],
   "source": [
    "# For our case study, we are going to focus on Gloucester City, NJ\n",
    "# Our config.yaml loads in a county indexed clip file\n",
    "# so that we can restrict all our data to the GC boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4108629a-beb2-44ba-94a2-634f69a44dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:39.428978Z",
     "iopub.status.busy": "2024-02-29T20:17:39.428663Z",
     "iopub.status.idle": "2024-02-29T20:17:39.465988Z",
     "shell.execute_reply": "2024-02-29T20:17:39.465311Z",
     "shell.execute_reply.started": "2024-02-29T20:17:39.428949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data we downloaded from the county's REST API server\n",
    "clip_filep = join(REF_DIR_R, FIPS, 'clip.json')\n",
    "with open(clip_filep) as f:\n",
    "    clip_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08a969c-fd99-4dae-ae15-041f5be159f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:39.732449Z",
     "iopub.status.busy": "2024-02-29T20:17:39.732053Z",
     "iopub.status.idle": "2024-02-29T20:17:39.927456Z",
     "shell.execute_reply": "2024-02-29T20:17:39.926841Z",
     "shell.execute_reply.started": "2024-02-29T20:17:39.732422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use pandas to get the data in a form that is easier\n",
    "# to turn into a geodataframe for clipping\n",
    "clip_df = pd.json_normalize(clip_data['features'])\n",
    "# We want to make a polygon out of the geometry coordinates\n",
    "# We can access that from the original json object\n",
    "clip_geo = [shape(i['geometry']) for i in clip_data['features']]\n",
    "# We can create a geodataframe of clip_df by adding clip_geo\n",
    "# as its geometry column\n",
    "clip_gdf = gpd.GeoDataFrame(clip_df,\n",
    "                            crs=CLIP_CRS,\n",
    "                            geometry=clip_geo)\n",
    "\n",
    "# We can clean up the gdf by removing the\n",
    "# type, id, geometry.type and geometry.coordinates columns\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "clip_gdf = clip_gdf.drop(columns=drop_col)\n",
    "\n",
    "# Write the file out to interim\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "prepare_saving(clip_out_filep)\n",
    "clip_gdf.to_file(clip_out_filep,\n",
    "                 driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb2e91-abce-44c5-8a49-14efa3cae4e4",
   "metadata": {},
   "source": [
    "## Process NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30dc5b6-24f2-4552-a274-465aab776350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:39.940503Z",
     "iopub.status.busy": "2024-02-29T20:17:39.940272Z",
     "iopub.status.idle": "2024-02-29T20:17:39.950957Z",
     "shell.execute_reply": "2024-02-29T20:17:39.950568Z",
     "shell.execute_reply.started": "2024-02-29T20:17:39.940487Z"
    }
   },
   "outputs": [],
   "source": [
    "# The NSI comes with all the data necessary for performing a standard \n",
    "# flood risk assessment. It is still useful to process the raw data.\n",
    "# Here, we subset to residential properties with 1 to 2 stories\n",
    "# and save as a geodataframe. These are the types of residences we have\n",
    "# multiple depth-damage functions for and a literature base to draw \n",
    "# from to introduce uncertainty in these loss estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d179e9-26c9-4f13-88c8-c54d7c1188c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:40.383722Z",
     "iopub.status.busy": "2024-02-29T20:17:40.383432Z",
     "iopub.status.idle": "2024-02-29T20:17:48.405206Z",
     "shell.execute_reply": "2024-02-29T20:17:48.404323Z",
     "shell.execute_reply.started": "2024-02-29T20:17:40.383695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read NSI\n",
    "nsi_filep = join(EXP_DIR_R, FIPS, 'nsi.json')\n",
    "with open(nsi_filep, 'r') as fp:\n",
    "    nsi_full = json.load(fp)\n",
    "\n",
    "# json normalize \n",
    "nsi_df = pd.json_normalize(nsi_full['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b268cd77-1947-4d88-8e34-b6010345b208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:48.407404Z",
     "iopub.status.busy": "2024-02-29T20:17:48.406652Z",
     "iopub.status.idle": "2024-02-29T20:17:48.934411Z",
     "shell.execute_reply": "2024-02-29T20:17:48.933692Z",
     "shell.execute_reply.started": "2024-02-29T20:17:48.407369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to gdf\n",
    "# This is useful for some spatial joins we need to perform\n",
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_df['properties.x'],\n",
    "                              nsi_df['properties.y'])\n",
    "nsi_gdf = gpd.GeoDataFrame(nsi_df, geometry=geometry,\n",
    "                           crs=NSI_CRS)\n",
    "\n",
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dacc2ed-62c5-481c-8b0c-9c2b173d8604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:48.935756Z",
     "iopub.status.busy": "2024-02-29T20:17:48.935370Z",
     "iopub.status.idle": "2024-02-29T20:17:49.190844Z",
     "shell.execute_reply": "2024-02-29T20:17:49.190177Z",
     "shell.execute_reply.started": "2024-02-29T20:17:48.935727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to residential properties and update\n",
    "# RES 1 - single family\n",
    "# RES 2 - manufactured home\n",
    "# RES 3 - multifamily (but could fit into a depth-damage function\n",
    "# archetype depending on # stories)\n",
    "# We are going to use RES1 for this case-study\n",
    "# It is the only occtype with hazus and naccs\n",
    "# DDFs and has less ambiguous classification\n",
    "\n",
    "# occtype category for easier use in loss estimation steps\n",
    "\n",
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['occtype'].str[:4] == 'RES1']\n",
    "\n",
    "# For this case-study, don't use any building with more \n",
    "# than 2 stories\n",
    "res1_3s_ind = nsi_res['num_story'] > 2\n",
    "# Final residential dataframe\n",
    "res_f = nsi_res.loc[~res1_3s_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dca1d9e1-e9f1-4c5b-b55a-6c80e1f18b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:49.193152Z",
     "iopub.status.busy": "2024-02-29T20:17:49.192349Z",
     "iopub.status.idle": "2024-02-29T20:17:49.318016Z",
     "shell.execute_reply": "2024-02-29T20:17:49.317215Z",
     "shell.execute_reply.started": "2024-02-29T20:17:49.193122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to relevant columns\n",
    "cols = ['fd_id', 'occtype', 'found_type', 'cbfips', 'bldgtype',\n",
    "        'ftprntsrc', 'found_ht', 'val_struct', 'sqft',\n",
    "        'val_cont', 'source', 'firmzone', 'ground_elv_m',\n",
    "        'geometry']\n",
    "\n",
    "res_out = res_f.loc[:,cols]\n",
    "\n",
    "# Clip to our clip boundary\n",
    "# They are in the same CRS\n",
    "nsi_clip_out = gpd.clip(res_out, clip_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5fffede-d15f-4a04-8bc0-7865247078a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:49.319628Z",
     "iopub.status.busy": "2024-02-29T20:17:49.318807Z",
     "iopub.status.idle": "2024-02-29T20:17:50.057141Z",
     "shell.execute_reply": "2024-02-29T20:17:50.056289Z",
     "shell.execute_reply.started": "2024-02-29T20:17:49.319594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out to interim/exposure/FIPS/\n",
    "# Single family homes -- sf\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "prepare_saving(EXP_OUT_FILEP)\n",
    "# Limit to sqft <= 99th percentile\n",
    "# Arbitrary cutoff. The max value from the steps above\n",
    "# is 400858 which is way too large\n",
    "# There are other large values that are dropped with this\n",
    "# arbitrary cutoff\n",
    "# For GC case study, this value is 2696.41999\n",
    "sqft_clip = nsi_clip_out['sqft'].quantile(.99)\n",
    "nsi_clip_out[nsi_clip_out['sqft'] <= sqft_clip].to_file(EXP_OUT_FILEP,\n",
    "                                                        driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6fa61-56df-40a0-9c3f-219d09d52597",
   "metadata": {},
   "source": [
    "## Process Depth-Damage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2558d82-b45e-4249-8bd8-c416d0d5ca37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:50.081198Z",
     "iopub.status.busy": "2024-02-29T20:17:50.080955Z",
     "iopub.status.idle": "2024-02-29T20:17:50.219395Z",
     "shell.execute_reply": "2024-02-29T20:17:50.218668Z",
     "shell.execute_reply.started": "2024-02-29T20:17:50.081175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read depth damage functions\n",
    "ddf_filedir = join(VULN_DIR_UZ, \"physical\", NATION)\n",
    "naccs = pd.read_csv(join(ddf_filedir, \"naccs_ddfs.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e508c-7c50-4a49-8665-56267f8de4e6",
   "metadata": {},
   "source": [
    "### NACCS DDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f838a2d4-2491-449b-92dc-fc345ceea1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:52.821274Z",
     "iopub.status.busy": "2024-02-29T20:17:52.820750Z",
     "iopub.status.idle": "2024-02-29T20:17:52.844277Z",
     "shell.execute_reply": "2024-02-29T20:17:52.843685Z",
     "shell.execute_reply.started": "2024-02-29T20:17:52.821246Z"
    }
   },
   "outputs": [],
   "source": [
    "# For NACCS, we have the RES 1 DDFs\n",
    "# First, subset to the relevant Occupancy types\n",
    "# We want to end up with ddf ids 1swb, open, etc.\n",
    "# don't need to keep the RES1- part for this case study\n",
    "naccs['res_type'] = naccs['Occupancy'].str.split('-').str[0]\n",
    "naccs['bld_type'] = naccs['Occupancy'].str.split('-').str[1]\n",
    "occ_types = ['1SWB', '2SWB', '1SNB', '2SNB']\n",
    "naccs_res = naccs.loc[(naccs['bld_type'].isin(occ_types)) &\n",
    "                      ((naccs['res_type'] == 'RES1') |\n",
    "                       (naccs['res_type'] == 'RES'))]\n",
    "\n",
    "# Next, drop columns we don't need\n",
    "drop_cols = ['Description', 'Source', 'Occupancy', 'res_type']\n",
    "naccs_res = naccs_res.drop(columns=drop_cols)\n",
    "\n",
    "# Rename DamageCategory\n",
    "naccs_res = naccs_res.rename(columns={'DamageCategory': 'dam_cat',\n",
    "                                      'bld_type': 'ddf_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50cd8f69-6b40-47a5-a9e0-ce956e07f956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:53.073857Z",
     "iopub.status.busy": "2024-02-29T20:17:53.073596Z",
     "iopub.status.idle": "2024-02-29T20:17:53.120549Z",
     "shell.execute_reply": "2024-02-29T20:17:53.120107Z",
     "shell.execute_reply.started": "2024-02-29T20:17:53.073834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now get the melted dataframe\n",
    "idvars = ['ddf_id', 'dam_cat']\n",
    "naccs_melt = tidy_ddfs(naccs_res, idvars)\n",
    "\n",
    "# Drop columns we don't need\n",
    "drop_cols = ['depth_str', 'pct_dam']\n",
    "naccs_f = naccs_melt.drop(columns=drop_cols)\n",
    "\n",
    "# We want to pivot the dataframe so that Min/ML/Max are our columns\n",
    "naccs_piv = naccs_f.pivot(index=['ddf_id', 'depth_ft'],\n",
    "                          columns='dam_cat')['rel_dam'].reset_index()\n",
    "\n",
    "\n",
    "# We do the interpolation again\n",
    "df_int_list = []\n",
    "for ddf_id, df in naccs_piv.groupby('ddf_id'):\n",
    "    # This creates the duplicate rows\n",
    "    ddf_int = df.loc[np.repeat(df.index, 10)].reset_index(drop=True)\n",
    "    # Now we have to make them nulls by finding\n",
    "    # the \"original\" indexed rows\n",
    "    ddf_int.loc[ddf_int.index % 10 != 0,\n",
    "                ['depth_ft', 'ML', 'Max', 'Min']] = np.nan\n",
    "    # Now we interpolate\n",
    "    ddf_int = ddf_int.interpolate().round(2)\n",
    "    # Drop duplicate rows (this happens for the max depth values)\n",
    "    ddf_int = ddf_int.drop_duplicates()\n",
    "    # And append\n",
    "    df_int_list.append(ddf_int)\n",
    "naccs_ddfs = pd.concat(df_int_list, axis=0)\n",
    "\n",
    "# We want to obtain our 'params' column\n",
    "# same as above\n",
    "p_cols = ['Min', 'ML', 'Max']\n",
    "tri_params = naccs_ddfs[p_cols].values\n",
    "# Drop the p_cols\n",
    "naccs_out = naccs_ddfs.drop(columns=p_cols)\n",
    "naccs_out = naccs_out.assign(params=tri_params.tolist())\n",
    "\n",
    "# Get out dict of max depths\n",
    "NACCS_MAX_DICT = ddf_max_depth_dict(naccs_out.reset_index(drop=True),\n",
    "                                    'params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84161e-2cd2-4bbf-aeb2-96d68386c666",
   "metadata": {},
   "source": [
    "### Save our processed ddfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ffbc854-d3f2-4e3d-ae8b-554426645f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T20:17:54.122001Z",
     "iopub.status.busy": "2024-02-29T20:17:54.120942Z",
     "iopub.status.idle": "2024-02-29T20:17:54.302811Z",
     "shell.execute_reply": "2024-02-29T20:17:54.302248Z",
     "shell.execute_reply.started": "2024-02-29T20:17:54.121974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main directory\n",
    "ddf_out_dir = join(VULN_DIR_I, 'physical')\n",
    "# Main ddf files\n",
    "naccs_out_filep = join(ddf_out_dir, 'naccs_ddfs.pqt')\n",
    "# Dictionaries - save as .json for simplicity\n",
    "naccs_max_filep = join(ddf_out_dir, 'naccs.json')\n",
    "\n",
    "# Only need to call this for one of the files\n",
    "# since they share the same parent directory\n",
    "prepare_saving(naccs_out_filep)\n",
    "\n",
    "# Save as parquet files since\n",
    "# these will directly read in the\n",
    "# DDF params as a list, not as a string\n",
    "naccs_out.to_parquet(naccs_out_filep)\n",
    "\n",
    "# Save the json files\n",
    "with open(naccs_max_filep, 'w') as fp:\n",
    "    json.dump(NACCS_MAX_DICT, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a49725d-26dd-44d0-98bc-b89a45fc70ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:28:06.636062Z",
     "iopub.status.busy": "2023-10-22T20:28:06.635303Z",
     "iopub.status.idle": "2023-10-22T20:28:08.559754Z",
     "shell.execute_reply": "2023-10-22T20:28:08.559031Z",
     "shell.execute_reply.started": "2023-10-22T20:28:06.636019Z"
    }
   },
   "outputs": [],
   "source": [
    "# We want S_FLD_HAZ_AR \n",
    "fld_haz_fp = join(POL_DIR_UZ, FIPS, 'S_FLD_HAZ_AR.shp')\n",
    "nfhl = gpd.read_file(fld_haz_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f19ac8f5-eb5d-4755-9c70-5e9bf06e7e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:32:30.197520Z",
     "iopub.status.busy": "2023-10-22T20:32:30.197255Z",
     "iopub.status.idle": "2023-10-22T20:32:31.864864Z",
     "shell.execute_reply": "2023-10-22T20:32:31.864144Z",
     "shell.execute_reply.started": "2023-10-22T20:32:30.197498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep FLD_ZONE, FLD_AR_ID, STATIC_BFE, geometry\n",
    "keep_cols = ['FLD_ZONE', 'FLD_AR_ID', 'STATIC_BFE', 'ZONE_SUBTY',\n",
    "             'geometry']\n",
    "nfhl_f = nfhl.loc[:,keep_cols]\n",
    "\n",
    "# Adjust .2 pct X zones to X_500\n",
    "nfhl_f.loc[nfhl_f['ZONE_SUBTY'] == '0.2 PCT ANNUAL CHANCE FLOOD HAZARD',\n",
    "           'FLD_ZONE'] = nfhl_f['FLD_ZONE'] + '_500'\n",
    "\n",
    "# Update column names\n",
    "# Lower case\n",
    "nfhl_f.columns = [x.lower() for x in nfhl_f.columns]\n",
    "\n",
    "# Drop ZONE_SUBTY\n",
    "nfhl_f = nfhl_f.drop(columns=['zone_subty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b141b40-0141-4e4e-874b-6813a2c780ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:51:09.615247Z",
     "iopub.status.busy": "2023-10-22T20:51:09.614969Z",
     "iopub.status.idle": "2023-10-22T20:51:10.171497Z",
     "shell.execute_reply": "2023-10-22T20:51:10.170761Z",
     "shell.execute_reply.started": "2023-10-22T20:51:09.615220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clip flood zones to our study area\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "clip_gdf = gpd.read_file(clip_out_filep)\n",
    "\n",
    "# Reproj flood zones\n",
    "nfhl_reproj = nfhl_f.to_crs(clip_gdf.crs)\n",
    "\n",
    "# Clip\n",
    "nfhl_clip = gpd.clip(nfhl_reproj, clip_gdf)\n",
    "\n",
    "# Reproject back\n",
    "nfhl_clip_out = nfhl_clip.to_crs(nfhl_f.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eedff84c-ddb7-47b4-9551-6df4e7525478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:51:11.666793Z",
     "iopub.status.busy": "2023-10-22T20:51:11.666602Z",
     "iopub.status.idle": "2023-10-22T20:51:13.234539Z",
     "shell.execute_reply": "2023-10-22T20:51:13.233721Z",
     "shell.execute_reply.started": "2023-10-22T20:51:11.666778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write file\n",
    "nfhl_out_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "prepare_saving(nfhl_out_filep)\n",
    "nfhl_clip_out.to_file(nfhl_out_filep,\n",
    "                      driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5095b468-9d10-4c7e-94f1-d74e80a4d746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:31:31.909701Z",
     "iopub.status.busy": "2023-10-22T20:31:31.909472Z",
     "iopub.status.idle": "2023-10-22T20:31:32.304728Z",
     "shell.execute_reply": "2023-10-22T20:31:32.303576Z",
     "shell.execute_reply.started": "2023-10-22T20:31:31.909683Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is optional: delete the nfhl directory to reduce\n",
    "# the file storage burden\n",
    "if RM_NFHL:\n",
    "    # Get directory name\n",
    "    nfhl_dir = join(POL_DIR_UZ, FIPS)\n",
    "    \n",
    "    # Try to remove the tree; if it fails,\n",
    "    # throw an error using try...except.\n",
    "    try:\n",
    "        shutil.rmtree(nfhl_dir)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77a02a-d329-479f-8355-93f0a34ded2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T17:55:19.418873Z",
     "iopub.status.busy": "2023-10-20T17:55:19.418001Z",
     "iopub.status.idle": "2023-10-20T17:55:21.014322Z",
     "shell.execute_reply": "2023-10-20T17:55:21.012336Z",
     "shell.execute_reply.started": "2023-10-20T17:55:19.418819Z"
    }
   },
   "source": [
    "## Process Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c51a0c-0455-4a05-8e62-6708bd75e67d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:10:26.499295Z",
     "iopub.status.busy": "2023-10-27T20:10:26.498758Z",
     "iopub.status.idle": "2023-10-27T20:10:26.789208Z",
     "shell.execute_reply": "2023-10-27T20:10:26.787836Z",
     "shell.execute_reply.started": "2023-10-27T20:10:26.499247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "clip_gdf = gpd.read_file(clip_out_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "881d0eee-bee3-45fe-97a8-f4f2eeda0bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:12:35.039960Z",
     "iopub.status.busy": "2023-10-27T20:12:35.039391Z",
     "iopub.status.idle": "2023-10-27T20:14:33.367301Z",
     "shell.execute_reply": "2023-10-27T20:14:33.366002Z",
     "shell.execute_reply.started": "2023-10-27T20:12:35.039907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ref: tract\n",
      "Saved Ref: block\n",
      "Saved Ref: bg\n",
      "Saved Ref: county\n",
      "Saved Ref: zcta\n"
     ]
    }
   ],
   "source": [
    "# For each .shp file in our unzipped ref directory\n",
    "# we are going to reproject & clip, then write out\n",
    "for path in Path(REF_DIR_UZ).rglob('*.shp'):\n",
    "    # Read in the file\n",
    "    ref_shp = gpd.read_file(path)\n",
    "    \n",
    "    # Process the filename to figure out what \n",
    "    # reference data this is\n",
    "    # the files are written out in the form of\n",
    "    # tl_2022_34_tract.shp, for example\n",
    "    # so we split the string on '_', take the\n",
    "    # last element of the array, and ignore\n",
    "    # the last 4 characters\n",
    "    ref_name = path.name.split('_')[-1][:-4]\n",
    "    # Replace the ref name with our ref_name dict values\n",
    "    ref_name_out = REF_NAMES_DICT[ref_name]\n",
    "\n",
    "    # Reproject and clip our reference shapefile\n",
    "    ref_reproj = ref_shp.to_crs(clip_gdf.crs)\n",
    "    ref_clipped = gpd.clip(ref_reproj, clip_gdf)\n",
    "    \n",
    "    # Write file\n",
    "    ref_out_filep = join(REF_DIR_I, FIPS, ref_name_out + \".gpkg\")\n",
    "    prepare_saving(ref_out_filep)\n",
    "    ref_clipped.to_file(ref_out_filep,\n",
    "                        driver='GPKG')\n",
    "\n",
    "    # Helpful message to track progress\n",
    "    print(\"Saved Ref: \" + ref_name_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe02b8-4e96-4bbf-8048-bfb4b9fda231",
   "metadata": {},
   "source": [
    "## Process Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4a17a46-2d52-4a78-a910-95b86d846fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T16:38:34.997434Z",
     "iopub.status.busy": "2023-12-28T16:38:34.996885Z",
     "iopub.status.idle": "2023-12-28T16:38:35.322389Z",
     "shell.execute_reply": "2023-12-28T16:38:35.321342Z",
     "shell.execute_reply.started": "2023-12-28T16:38:34.997387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load relevant spatial data (tract, block group)\n",
    "tract_filep = join(REF_DIR_I, FIPS, 'tract.gpkg')\n",
    "bg_filep = join(REF_DIR_I, FIPS, 'bg.gpkg')\n",
    "tract_geo = gpd.read_file(tract_filep)\n",
    "bg_geo = gpd.read_file(bg_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca5d827b-3879-4e5a-a544-90091475a965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T16:40:40.162305Z",
     "iopub.status.busy": "2023-12-28T16:40:40.161738Z",
     "iopub.status.idle": "2023-12-28T16:40:41.206441Z",
     "shell.execute_reply": "2023-12-28T16:40:41.204953Z",
     "shell.execute_reply.started": "2023-12-28T16:40:40.162257Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648190/4066402225.py:3: DtypeWarning: Columns (18,26,70,72,85,131) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cejst = pd.read_csv(ce_filep, dtype={'Census tract 2010 ID': 'str'})\n"
     ]
    }
   ],
   "source": [
    "# CEJST data\n",
    "ce_filep = join(VULN_DIR_R, 'social', NATION, 'cejst.csv')\n",
    "cejst = pd.read_csv(ce_filep, dtype={'Census tract 2010 ID': 'str'})\n",
    "\n",
    "# Columns to keep\n",
    "# Identified as disadvantaged\n",
    "# Census tract 2010 ID\n",
    "keep_cols = ['Census tract 2010 ID', 'Identified as disadvantaged']\n",
    "cejst_sub = cejst[keep_cols]\n",
    "# Rename columns\n",
    "cejst_sub.columns = ['GEOID', 'disadvantaged']\n",
    "\n",
    "# Merge with tract_geo\n",
    "cejst_f = tract_geo[['GEOID', 'geometry']].merge(cejst_sub,\n",
    "                                                 on='GEOID',\n",
    "                                                 how='inner')\n",
    "\n",
    "# Retain only the disadvantaged \n",
    "cejst_f = cejst_f[cejst_f['disadvantaged'] == True].drop(columns='disadvantaged')\n",
    "\n",
    "# Write file\n",
    "cejst_out_filep = join(VULN_DIR_I, 'social', FIPS, 'cejst.gpkg')\n",
    "prepare_saving(cejst_out_filep)\n",
    "cejst_f.to_file(cejst_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72b8abed-5fa4-4e90-99cc-c6ff83beede1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T16:44:38.956604Z",
     "iopub.status.busy": "2023-12-28T16:44:38.956360Z",
     "iopub.status.idle": "2023-12-28T16:44:39.903132Z",
     "shell.execute_reply": "2023-12-28T16:44:39.902033Z",
     "shell.execute_reply.started": "2023-12-28T16:44:38.956585Z"
    }
   },
   "outputs": [],
   "source": [
    "# NJ overburdened data\n",
    "\n",
    "# Read data\n",
    "ovb_filep = join(VULN_DIR_R, 'social', STATEABBR, 'overburdened.gpkg')\n",
    "ovb = gpd.read_file(ovb_filep)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in ovb.columns]\n",
    "ovb.columns = col_updates\n",
    "\n",
    "# Rename some columns\n",
    "ovb = ovb.rename(columns={'OVERBURDENED_COMMUNITY_CRITERI': 'ovb_crit'})\n",
    "\n",
    "# Keep a subset of columns\n",
    "ovb_f = ovb[['GEOID', 'ovb_crit', 'geometry']]\n",
    "\n",
    "# The data already is limited to overburdened categories\n",
    "\n",
    "# Write file\n",
    "ovb_out_filep = join(VULN_DIR_I, 'social', FIPS, 'ovb.gpkg')\n",
    "ovb_f.to_file(ovb_out_filep, driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad2c5e50-cae9-4bd0-87d9-475a3ea93609",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T17:01:18.552366Z",
     "iopub.status.busy": "2023-12-28T17:01:18.552136Z",
     "iopub.status.idle": "2023-12-28T17:01:22.481409Z",
     "shell.execute_reply": "2023-12-28T17:01:22.480671Z",
     "shell.execute_reply.started": "2023-12-28T17:01:18.552347Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOAA SOVI data\n",
    "sovi_suffix = 'SoVI2010_' + STATEABBR\n",
    "sovi_filename = 'SoVI0610_' + STATEABBR + '.shp'\n",
    "sovi_filep = join(VULN_DIR_UZ, 'social', STATEABBR,\n",
    "                  sovi_suffix, sovi_filename)\n",
    "sovi = gpd.read_file(sovi_filep)\n",
    "\n",
    "# Subset columns\n",
    "keep_cols = ['GEOID10', 'SOVI0610_1', 'SOVI0610_2',\n",
    "             'SOVI0610' + STATEABBR]\n",
    "sovi_high = sovi[keep_cols]\n",
    "\n",
    "# Rename GEOID10 to GEOID\n",
    "sovi_high = sovi_high.rename(columns={'GEOID10': 'GEOID'})\n",
    "\n",
    "# Subset to tracts in our study area (using the tract_geo geometries)\n",
    "sovi_f = tract_geo[['GEOID', 'geometry']].merge(sovi_high,\n",
    "                                                on='GEOID',\n",
    "                                                how='inner')\n",
    "\n",
    "# Write out file\n",
    "sovi_out_filep = join(VULN_DIR_I, 'social', FIPS, 'sovi.gpkg')\n",
    "sovi_f.to_file(sovi_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c10c115-8995-4da8-80fb-6f69d9f60b96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T17:01:28.673364Z",
     "iopub.status.busy": "2023-12-28T17:01:28.672795Z",
     "iopub.status.idle": "2023-12-28T17:01:54.325751Z",
     "shell.execute_reply": "2023-12-28T17:01:54.325017Z",
     "shell.execute_reply.started": "2023-12-28T17:01:28.673316Z"
    }
   },
   "outputs": [],
   "source": [
    "# LMI data\n",
    "# Read data\n",
    "lmi_filename = 'ACS_2015_lowmod_blockgroup_all.xlsx'\n",
    "lmi_filep = join(VULN_DIR_R, 'social', NATION, lmi_filename)\n",
    "lmi = pd.read_excel(lmi_filep, engine='openpyxl')\n",
    "# Get GEOID for merge (last 12 characters is the bg id)\n",
    "lmi['GEOID'] = lmi['GEOID'].str[-12:]\n",
    "\n",
    "# Retain GEOID and Lowmod_pct\n",
    "keep_cols = ['GEOID', 'Lowmod_pct']\n",
    "lmi_f = bg_geo[['GEOID', 'geometry']].merge(lmi[keep_cols],\n",
    "                                            on='GEOID',\n",
    "                                            how='inner')\n",
    "\n",
    "# Write file\n",
    "lmi_out_filep = join(VULN_DIR_I, 'social', FIPS, 'lmi.gpkg')\n",
    "lmi_f.to_file(lmi_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e07e31-71f1-46f0-a6a6-6bf8f1c5fa3f",
   "metadata": {},
   "source": [
    "# Link everything to NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "044aa2f8-8802-4cb0-be12-8b7baf88e750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:51:30.935675Z",
     "iopub.status.busy": "2023-10-22T20:51:30.935001Z",
     "iopub.status.idle": "2023-10-22T20:51:31.563827Z",
     "shell.execute_reply": "2023-10-22T20:51:31.563100Z",
     "shell.execute_reply.started": "2023-10-22T20:51:30.935626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just for jupyter notebooks\n",
    "# Scripts don't need to reload the data since it runs all at once\n",
    "# Jupyter is more for development, and might only run\n",
    "# some sections at a time\n",
    "# Using same names as above for consistency\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)\n",
    "\n",
    "nfhl_out_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "nfhl_clip_out = gpd.read_file(nfhl_out_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef159940-f944-474d-954d-8abf84ff2fd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:52:07.532239Z",
     "iopub.status.busy": "2023-10-22T20:52:07.531969Z",
     "iopub.status.idle": "2023-10-22T20:52:08.459985Z",
     "shell.execute_reply": "2023-10-22T20:52:08.458792Z",
     "shell.execute_reply.started": "2023-10-22T20:52:07.532217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project nsi to flood zone crs\n",
    "nsi_fz = nsi_clip_out.to_crs(nfhl_clip_out.crs)\n",
    "\n",
    "# Spatial join, retaining flood zone cols\n",
    "# Only need the id and geom from nsi for this\n",
    "fz_m = gpd.sjoin(nsi_fz[['fd_id', 'geometry']],\n",
    "                 nfhl_clip_out,\n",
    "                 predicate='within')\n",
    "\n",
    "# I checked for issues like overlapping flood zones\n",
    "# resulting in NSI structures in multiple polygons\n",
    "# and did not find any. That's good, but chances\n",
    "# are there will be counties where this happens\n",
    "# and we will need code to handle these consistently\n",
    "\n",
    "# Write out fd_id/fld_ar_id/fld_zone/static_bfe\n",
    "keep_cols = ['fd_id', 'fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "fz_m_out = fz_m[keep_cols]\n",
    "\n",
    "nsi_fz_filep = join(EXP_DIR_I, FIPS, 'nsi_fz.pqt')\n",
    "prepare_saving(nsi_fz_filep)\n",
    "fz_m_out.to_parquet(nsi_fz_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f461bca-7b68-43d3-b4c7-dc7c54a8898a",
   "metadata": {},
   "source": [
    "## Link NSI with Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b1bf23-e259-421c-90af-926d59e9b6a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:26:25.699324Z",
     "iopub.status.busy": "2023-10-27T20:26:25.698372Z",
     "iopub.status.idle": "2023-10-27T20:26:26.569780Z",
     "shell.execute_reply": "2023-10-27T20:26:26.568715Z",
     "shell.execute_reply.started": "2023-10-27T20:26:25.699254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862b61c3-50ff-45d2-89bd-2c3db1efde76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:28:09.669185Z",
     "iopub.status.busy": "2023-10-27T20:28:09.668578Z",
     "iopub.status.idle": "2023-10-27T20:28:10.803838Z",
     "shell.execute_reply": "2023-10-27T20:28:10.802486Z",
     "shell.execute_reply.started": "2023-10-27T20:28:09.669129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked reference to NSI: tract_id\n",
      "Linked reference to NSI: block_id\n",
      "Linked reference to NSI: bg_id\n",
      "Linked reference to NSI: zcta_id\n"
     ]
    }
   ],
   "source": [
    "# For zcta, tract, bg, and block\n",
    "# we want to do spatial joins to link\n",
    "# up fd_id in the NSI with the ref\n",
    "# We will use config data to do this\n",
    "# since other references may be brought in \n",
    "# down the line\n",
    "# We are going to store fd_id/ref_id links in a dataframe\n",
    "ref_df_list = []\n",
    "for ref_name, ref_id in REF_ID_NAMES_DICT.items():\n",
    "    ref_filep = join(REF_DIR_I, FIPS, ref_name + \".gpkg\")\n",
    "\n",
    "    # Load in the ref file\n",
    "    ref_geo = gpd.read_file(ref_filep)\n",
    "\n",
    "    # Limit the geodataframe to our ref id and 'geometry' column\n",
    "    keep_col = [ref_id, 'geometry']\n",
    "    ref_geo_sub = ref_geo[keep_col]\n",
    "\n",
    "    # Limit the NSI to our fd_id and geometry column\n",
    "    keep_col_nsi = ['fd_id', 'geometry']\n",
    "    nsi_sub = nsi_clip_out[keep_col_nsi]\n",
    "\n",
    "    # Reproj nsi_sub to the reference crs\n",
    "    nsi_reproj = nsi_sub.to_crs(ref_geo.crs)\n",
    "\n",
    "    # Do a spatial join\n",
    "    nsi_ref = gpd.sjoin(nsi_reproj, ref_geo_sub, predicate='within')\n",
    "\n",
    "    # Set index to fd_id and just keep the ref_id\n",
    "    # Rename that column to our ref_name + '_id'\n",
    "    # Append this to our ref_df_list\n",
    "    nsi_ref_f = nsi_ref.set_index('fd_id')[[ref_id]]\n",
    "    nsi_ref_f = nsi_ref_f.rename(columns={ref_id: ref_name + '_id'})\n",
    "    ref_df_list.append(nsi_ref_f)\n",
    "\n",
    "    # Helpful message\n",
    "    print('Linked reference to NSI: ' + ref_name + '_id')\n",
    "\n",
    "# Can concat and write\n",
    "nsi_refs = pd.concat(ref_df_list, axis=1).reset_index()\n",
    "ref_filep = join(EXP_DIR_I,  FIPS, 'nsi_ref.pqt')\n",
    "prepare_saving(ref_filep)\n",
    "nsi_refs.to_parquet(ref_filep)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48257494-dca6-4c15-9397-509a9a22450e",
   "metadata": {},
   "source": [
    "## Link NSI with Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26e01cb-8048-418e-b8e5-ee1bdda75578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T16:27:45.721655Z",
     "iopub.status.busy": "2023-12-28T16:27:45.721451Z",
     "iopub.status.idle": "2023-12-28T16:27:46.908616Z",
     "shell.execute_reply": "2023-12-28T16:27:46.907764Z",
     "shell.execute_reply.started": "2023-12-28T16:27:45.721638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "# Read in single family home gpkg\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2a1cfee-ec4c-41c0-aaa8-217c5dbc82aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T17:08:26.463411Z",
     "iopub.status.busy": "2023-12-28T17:08:26.462832Z",
     "iopub.status.idle": "2023-12-28T17:08:27.242875Z",
     "shell.execute_reply": "2023-12-28T17:08:27.241230Z",
     "shell.execute_reply.started": "2023-12-28T17:08:26.463362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked vulnerability to NSI: lmi\n",
      "Linked vulnerability to NSI: sovi\n",
      "Linked vulnerability to NSI: ovb\n",
      "Linked vulnerability to NSI: cejst\n"
     ]
    }
   ],
   "source": [
    "# Read in processed sovi data\n",
    "# Loop through the community boundary data\n",
    "# Get links to the single family home data\n",
    "# Store in single dataframe\n",
    "# Write out\n",
    "\n",
    "sovi_dir = join(VULN_DIR_I, 'social', FIPS)\n",
    "filenames = ['lmi', 'sovi', 'ovb', 'cejst']\n",
    "\n",
    "sovi_df_list = []\n",
    "for fn in filenames:\n",
    "    # Read in each gpkg\n",
    "    fp = join(sovi_dir, fn + '.gpkg')\n",
    "    sovi_geo = gpd.read_file(fp)\n",
    "\n",
    "    # Subset sovi_geo based on thresholds\n",
    "    # For cejst and ovb this is already done\n",
    "    # For lmi and ovb need to do the filter as follows\n",
    "    if fn == 'lmi':\n",
    "        # See https://www.hudoig.gov/reports-publications/\n",
    "        # report/cdbg-dr-program-generally-\n",
    "        # met-low-and-moderate-income-requirements\n",
    "        # The statutory hreshold is 50%, so retain those\n",
    "        sovi_sub = sovi_geo[sovi_geo['Lowmod_pct'] > .5]\n",
    "    elif fn == 'sovi':\n",
    "        # Subset to threshhold for FMA (from 2022 NOFO)\n",
    "        sovi_sub = sovi_geo[sovi_geo['SOVI0610' + STATEABBR] > .75]\n",
    "    else:\n",
    "        sovi_sub = sovi_geo\n",
    "\n",
    "    # Only need the geometry for sovi_sub\n",
    "    sovi_sub = sovi_sub[['geometry']]\n",
    "    \n",
    "    # Limit the NSI to our fd_id and geometry column\n",
    "    keep_col_nsi = ['fd_id', 'geometry']\n",
    "    nsi_sub = nsi_clip_out[keep_col_nsi]\n",
    "\n",
    "    # Reproj nsi_sub to the reference crs\n",
    "    nsi_reproj = nsi_sub.to_crs(sovi_geo.crs)\n",
    "\n",
    "    # Do a spatial join\n",
    "    nsi_sovi = gpd.sjoin(nsi_reproj, sovi_sub, predicate='within')\n",
    "\n",
    "    # Add indicator column\n",
    "    nsi_sovi[fn] = True\n",
    "\n",
    "    # Append this to our sovi_df_list\n",
    "    sovi_df_list.append(nsi_sovi[['fd_id', fn]].set_index('fd_id'))\n",
    "\n",
    "    # Helpful message\n",
    "    print('Linked vulnerability to NSI: ' + fn)\n",
    "\n",
    "sovi_df_f = pd.concat(sovi_df_list, axis=1).fillna(False)\n",
    "sovi_out_filepath = join(sovi_dir, 'c_indicators.pqt')\n",
    "sovi_df_f.to_parquet(sovi_out_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c15125c-9c16-4a41-a152-a0df4d59b5bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T20:41:52.813092Z",
     "iopub.status.busy": "2023-12-28T20:41:52.812537Z",
     "iopub.status.idle": "2023-12-28T20:41:54.229579Z",
     "shell.execute_reply": "2023-12-28T20:41:54.228446Z",
     "shell.execute_reply.started": "2023-12-28T20:41:52.813045Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter only\n",
    "# Load in flood zone and single family files\n",
    "nsi_fz = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_fz.pqt'))\n",
    "nsi_sf = gpd.read_file(join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc2f825b-6d7d-4e5c-a345-742842552bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T20:46:32.514058Z",
     "iopub.status.busy": "2023-12-28T20:46:32.513541Z",
     "iopub.status.idle": "2023-12-28T20:46:32.580057Z",
     "shell.execute_reply": "2023-12-28T20:46:32.578384Z",
     "shell.execute_reply.started": "2023-12-28T20:46:32.514015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Limit to structures with BFE estimates\n",
    "fz_ids = nsi_fz[nsi_fz['static_bfe'] > 0]['fd_id']\n",
    "# Subset sf homes to these\n",
    "nsi_sf_sub = nsi_sf[nsi_sf['fd_id'].isin(fz_ids)]\n",
    "# Merge in\n",
    "nsi_sf_sub = nsi_sf_sub.merge(nsi_fz[['fd_id', 'static_bfe']],\n",
    "                              on='fd_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5e4c7d90-c42b-4f37-888a-383c02ea8dc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T20:49:47.509528Z",
     "iopub.status.busy": "2023-12-28T20:49:47.508965Z",
     "iopub.status.idle": "2023-12-28T20:49:47.557357Z",
     "shell.execute_reply": "2023-12-28T20:49:47.555532Z",
     "shell.execute_reply.started": "2023-12-28T20:49:47.509480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert ground elevation to feet\n",
    "nsi_sf_sub['ground_elev_ft'] = nsi_sf_sub['ground_elv_m']*MTR_TO_FT\n",
    "# Substract this from BFE\n",
    "nsi_sf_sub['bfe_gap'] = (nsi_sf_sub['static_bfe']\n",
    "                         - nsi_sf_sub['ground_elev_ft'])\n",
    "# Get the design flood elevation, based on current found_ht\n",
    "# You substract current found_ht and add 4, based on \n",
    "# current NJ standards. Round up\n",
    "nsi_sf_sub['dfe'] = np.ceil(nsi_sf_sub['bfe_gap'] -\n",
    "                            nsi_sf_sub['found_ht'] + 4)\n",
    "# Also, min elevation of 3 feet\n",
    "nsi_sf_sub.loc[nsi_sf_sub['dfe'] < 3,\n",
    "               'dfe'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "17e77478-8750-4cca-ae7f-d37916888db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T20:50:47.348119Z",
     "iopub.status.busy": "2023-12-28T20:50:47.347546Z",
     "iopub.status.idle": "2023-12-28T20:50:48.097687Z",
     "shell.execute_reply": "2023-12-28T20:50:48.096113Z",
     "shell.execute_reply.started": "2023-12-28T20:50:47.348072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out design flood elevations\n",
    "dfe_out_filep = join(EXP_DIR_I, FIPS, 'nsi_dfe.pqt')\n",
    "nsi_sf_sub[['fd_id', 'dfe']].to_parquet(dfe_out_filep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07ae3738-6ce1-4625-9777-cb42c218896b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T21:59:50.776036Z",
     "iopub.status.busy": "2023-11-06T21:59:50.775491Z",
     "iopub.status.idle": "2023-11-06T21:59:51.552743Z",
     "shell.execute_reply": "2023-11-06T21:59:51.551165Z",
     "shell.execute_reply.started": "2023-11-06T21:59:50.775991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cf7aaad-086f-40a9-92e4-374d1efb7495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T21:59:51.555626Z",
     "iopub.status.busy": "2023-11-06T21:59:51.555161Z",
     "iopub.status.idle": "2023-11-06T21:59:51.590138Z",
     "shell.execute_reply": "2023-11-06T21:59:51.588633Z",
     "shell.execute_reply.started": "2023-11-06T21:59:51.555582Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import shape\n",
    "import rasterio \n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ae98b55-a0e0-4e75-b3f3-b9dcc294234c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T21:59:51.593316Z",
     "iopub.status.busy": "2023-11-06T21:59:51.592358Z",
     "iopub.status.idle": "2023-11-06T21:59:51.622439Z",
     "shell.execute_reply": "2023-11-06T21:59:51.620962Z",
     "shell.execute_reply.started": "2023-11-06T21:59:51.593271Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec963f-fac9-4bb2-ac41-364fde7dbcb8",
   "metadata": {},
   "source": [
    "# Process - everything ends up at county level and clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879462d-4490-4f69-88bf-15efa5de8d78",
   "metadata": {},
   "source": [
    "## Process clip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "462240d3-88c1-461a-bcbd-22f0585387f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T22:02:52.334858Z",
     "iopub.status.busy": "2023-11-06T22:02:52.334621Z",
     "iopub.status.idle": "2023-11-06T22:02:52.357738Z",
     "shell.execute_reply": "2023-11-06T22:02:52.356043Z",
     "shell.execute_reply.started": "2023-11-06T22:02:52.334842Z"
    }
   },
   "outputs": [],
   "source": [
    "# For our case study, we are going to focus on Gloucester City, NJ\n",
    "# Our config.yaml loads in a county indexed clip file\n",
    "# so that we can restrict all our data to the GC boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4108629a-beb2-44ba-94a2-634f69a44dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T22:02:54.009721Z",
     "iopub.status.busy": "2023-11-06T22:02:54.009221Z",
     "iopub.status.idle": "2023-11-06T22:02:54.064799Z",
     "shell.execute_reply": "2023-11-06T22:02:54.063275Z",
     "shell.execute_reply.started": "2023-11-06T22:02:54.009680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data we downloaded from the county's REST API server\n",
    "clip_filep = join(REF_DIR_R, FIPS, 'clip.json')\n",
    "with open(clip_filep) as f:\n",
    "    clip_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b08a969c-fd99-4dae-ae15-041f5be159f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T22:02:55.778020Z",
     "iopub.status.busy": "2023-11-06T22:02:55.776598Z",
     "iopub.status.idle": "2023-11-06T22:02:57.050762Z",
     "shell.execute_reply": "2023-11-06T22:02:57.049162Z",
     "shell.execute_reply.started": "2023-11-06T22:02:55.777956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use pandas to get the data in a form that is easier\n",
    "# to turn into a geodataframe for clipping\n",
    "clip_df = pd.json_normalize(clip_data['features'])\n",
    "# We want to make a polygon out of the geometry coordinates\n",
    "# We can access that from the original json object\n",
    "clip_geo = [shape(i['geometry']) for i in clip_data['features']]\n",
    "# We can create a geodataframe of clip_df by adding clip_geo\n",
    "# as its geometry column\n",
    "clip_gdf = gpd.GeoDataFrame(clip_df,\n",
    "                            crs=CLIP_CRS,\n",
    "                            geometry=clip_geo)\n",
    "\n",
    "# We can clean up the gdf by removing the\n",
    "# type, id, geometry.type and geometry.coordinates columns\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "clip_gdf = clip_gdf.drop(columns=drop_col)\n",
    "\n",
    "# Write the file out to interim\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "prepare_saving(clip_out_filep)\n",
    "clip_gdf.to_file(clip_out_filep,\n",
    "                 driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb2e91-abce-44c5-8a49-14efa3cae4e4",
   "metadata": {},
   "source": [
    "## Process NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b30dc5b6-24f2-4552-a274-465aab776350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T22:02:57.146862Z",
     "iopub.status.busy": "2023-11-06T22:02:57.146356Z",
     "iopub.status.idle": "2023-11-06T22:02:57.180778Z",
     "shell.execute_reply": "2023-11-06T22:02:57.178881Z",
     "shell.execute_reply.started": "2023-11-06T22:02:57.146816Z"
    }
   },
   "outputs": [],
   "source": [
    "# The NSI comes with all the data necessary for performing a standard \n",
    "# flood risk assessment. It is still useful to process the raw data.\n",
    "# Here, we subset to residential properties with 1 to 2 stories\n",
    "# and save as a geodataframe. These are the types of residences we have\n",
    "# multiple depth-damage functions for and a literature base to draw \n",
    "# from to introduce uncertainty in these loss estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80d179e9-26c9-4f13-88c8-c54d7c1188c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T22:02:57.552652Z",
     "iopub.status.busy": "2023-11-06T22:02:57.550770Z",
     "iopub.status.idle": "2023-11-06T22:02:58.078487Z",
     "shell.execute_reply": "2023-11-06T22:02:58.076701Z",
     "shell.execute_reply.started": "2023-11-06T22:02:57.552588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read NSI, reset index upon reading\n",
    "# TODO - this will be json...\n",
    "nsi_filep = join(EXP_DIR_R, FIPS, 'nsi.pqt')\n",
    "nsi_full = pd.read_parquet(nsi_filep).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b268cd77-1947-4d88-8e34-b6010345b208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T22:02:58.081039Z",
     "iopub.status.busy": "2023-11-06T22:02:58.080604Z",
     "iopub.status.idle": "2023-11-06T22:02:58.382287Z",
     "shell.execute_reply": "2023-11-06T22:02:58.381155Z",
     "shell.execute_reply.started": "2023-11-06T22:02:58.080998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to gdf\n",
    "# This is useful for some spatial joins we need to perform\n",
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_full['properties.x'],\n",
    "                             nsi_full['properties.y'])\n",
    "# The NSI CRS is EPSG 4326\n",
    "nsi_gdf = gpd.GeoDataFrame(nsi_full, geometry=geometry,\n",
    "                           crs=NSI_CRS)\n",
    "\n",
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dacc2ed-62c5-481c-8b0c-9c2b173d8604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T22:02:58.469197Z",
     "iopub.status.busy": "2023-11-06T22:02:58.468964Z",
     "iopub.status.idle": "2023-11-06T22:02:58.695099Z",
     "shell.execute_reply": "2023-11-06T22:02:58.693632Z",
     "shell.execute_reply.started": "2023-11-06T22:02:58.469176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to residential properties and update\n",
    "# RES 1 - single family\n",
    "# RES 2 - manufactured home\n",
    "# RES 3 - multifamily (but could fit into a depth-damage function\n",
    "# archetype depending on # stories)\n",
    "# We are going to use RES1 for this case-study\n",
    "# It is the only occtype with hazus and naccs\n",
    "# DDFs and has less ambiguous classification\n",
    "\n",
    "# occtype category for easier use in loss estimation steps\n",
    "\n",
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['occtype'].str[:4] == 'RES1']\n",
    "\n",
    "# For this case-study, don't use any building with more \n",
    "# than 2 stories\n",
    "res1_3s_ind = nsi_res['num_story'] > 2\n",
    "# Final residential dataframe\n",
    "res_f = nsi_res.loc[~res1_3s_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dca1d9e1-e9f1-4c5b-b55a-6c80e1f18b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T22:02:59.151645Z",
     "iopub.status.busy": "2023-11-06T22:02:59.151459Z",
     "iopub.status.idle": "2023-11-06T22:02:59.661813Z",
     "shell.execute_reply": "2023-11-06T22:02:59.660425Z",
     "shell.execute_reply.started": "2023-11-06T22:02:59.151628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to relevant columns\n",
    "cols = ['fd_id', 'occtype', 'found_type', 'cbfips',\n",
    "        'ftprntsrc', 'found_ht', 'val_struct', 'sqft',\n",
    "        'val_cont', 'source', 'firmzone', 'ground_elv_m',\n",
    "        'geometry']\n",
    "\n",
    "res_out = res_f.loc[:,cols]\n",
    "\n",
    "# Clip to our clip boundary\n",
    "# They are in the same CRS\n",
    "nsi_clip_out = gpd.clip(res_out, clip_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5fffede-d15f-4a04-8bc0-7865247078a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T22:05:28.244101Z",
     "iopub.status.busy": "2023-11-06T22:05:28.243550Z",
     "iopub.status.idle": "2023-11-06T22:05:30.481878Z",
     "shell.execute_reply": "2023-11-06T22:05:30.480368Z",
     "shell.execute_reply.started": "2023-11-06T22:05:28.244053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out to interim/exposure/FIPS/\n",
    "# Single family homes -- sf\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "prepare_saving(EXP_OUT_FILEP)\n",
    "# Limit to sqft <= 2500\n",
    "# Arbitrary cutoff. The max value from the steps above\n",
    "# is 400858 which is way too large\n",
    "# There are other large values that are dropped with this\n",
    "# arbitrary cutoff\n",
    "nsi_clip_out[nsi_clip_out['sqft'] <= 2500].to_file(EXP_OUT_FILEP,\n",
    "                                                   driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6fa61-56df-40a0-9c3f-219d09d52597",
   "metadata": {},
   "source": [
    "## Process Depth-Damage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a4fbb6-23b2-49b0-a485-6bc268bc92c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:48:48.755151Z",
     "iopub.status.busy": "2023-11-06T19:48:48.754844Z",
     "iopub.status.idle": "2023-11-06T19:48:48.787282Z",
     "shell.execute_reply": "2023-11-06T19:48:48.785486Z",
     "shell.execute_reply.started": "2023-11-06T19:48:48.755123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generally, we will process these DDFs the same way since they\n",
    "# are written in mostly the same format\n",
    "# However, there are a few preprocessing steps necessary for the hazus\n",
    "# ddfs. Also, there are some differences for NACCS vs. HAZUS\n",
    "# shallow uncertainty representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2558d82-b45e-4249-8bd8-c416d0d5ca37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:48:48.966645Z",
     "iopub.status.busy": "2023-11-06T19:48:48.965535Z",
     "iopub.status.idle": "2023-11-06T19:48:49.119088Z",
     "shell.execute_reply": "2023-11-06T19:48:49.117324Z",
     "shell.execute_reply.started": "2023-11-06T19:48:48.966578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read depth damage functions\n",
    "ddf_filedir = join(VULN_DIR_UZ, \"physical\", NATION)\n",
    "naccs = pd.read_csv(join(ddf_filedir, \"naccs_ddfs.csv\"))\n",
    "hazus = pd.read_csv(join(ddf_filedir, \"haz_fl_dept.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec16a6f-2dab-456a-af85-968779cd07cc",
   "metadata": {},
   "source": [
    "### HAZUS DDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a06109-cc56-4abf-8973-dfd3ebd531fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:48:49.655073Z",
     "iopub.status.busy": "2023-11-06T19:48:49.654801Z",
     "iopub.status.idle": "2023-11-06T19:48:49.682000Z",
     "shell.execute_reply": "2023-11-06T19:48:49.681394Z",
     "shell.execute_reply.started": "2023-11-06T19:48:49.655050Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, preprocessing for hazus ddfs\n",
    "# For basements, use FIA (MOD.) which does one and two floors by\n",
    "# A and V zones\n",
    "# For no basements, use USACE - IWR\n",
    "# which does one and two floor, no flood zone specified\n",
    "# 106: FIA (MOD.) 1S WB A zone\n",
    "# 114: \"\" V zone\n",
    "# 108: FIA (MOD.) 1S WB A zone\n",
    "# 116: \"\" V zone\n",
    "# 129: USACE - IWR 1S NB\n",
    "# 130: USCAE - IWR 2S+ NB\n",
    "# For elevating homes, we can use Pile foundation DDFs\n",
    "# from USACE - Wilmington\n",
    "# 178 - 1S Pile Foundation\n",
    "# 183 - 2S Pile Foundation\n",
    "# These are no basement homes, so to speak\n",
    "# The USACE New Orleans DDFs have some pier foundation\n",
    "# DDFs with fresh & salt water and long & short duration\n",
    "# but this does not appear to apply to out study area\n",
    "# Subset to DmgFnId in the codes above\n",
    "dmg_ids = [106, 108, 114, 116, 129, 130, 178, 183]\n",
    "hazus_res = hazus[(hazus['DmgFnId'].isin(dmg_ids)) & \n",
    "                  (hazus['Occupancy'] == 'RES1')]\n",
    "\n",
    "# Make occtype column in the same form that the NSI has\n",
    "# e.g. RES1-1SNB\n",
    "# Add column for A or V zone\n",
    "# Note: outside SFHA basement homes will take A zone\n",
    "# What other option do we have? \n",
    "\n",
    "# Split Description by comma. \n",
    "# The split[0] element tells us stories (but description sometimes\n",
    "# says floors instead of story...)\n",
    "# Can get around this issue by looking at first word\n",
    "# The split[1] element\n",
    "# tells us w/ basement or no basement. Use this to create occtype\n",
    "desc = hazus_res['Description'].str.split(',')\n",
    "s_type = desc.str[0].str.split(' ').str[0]\n",
    "s_type = s_type.str.replace('one', '1').str.replace('two', '2')\n",
    "b_type = desc.str[1].str.strip()\n",
    "# Below, we are just trying to get archetypes like\n",
    "# 1SNB, 2SWB, 1SPL -- for pile foundation\n",
    "occtype = np.where(b_type == 'w/ basement',\n",
    "                   s_type + 'SWB',\n",
    "                   s_type + 'SNB')\n",
    "occtype = np.where(b_type == 'Pile foundation',\n",
    "                   s_type + 'SPL',\n",
    "                   occtype)\n",
    "# Some of these HAZUS DDFs require us to keep track of the\n",
    "# flood zone they're in\n",
    "# I don't think this matters for our case study since\n",
    "# there are no high wave coastsal zones\n",
    "# This line is designed to work specifically \n",
    "# with the way the descriptions\n",
    "# are written out for the DDFs used in this case study\n",
    "fz = desc.str[-1].str.lower().str.replace('structure', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f934250-6c91-47fd-9497-13702fd8f776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:48:50.232188Z",
     "iopub.status.busy": "2023-11-06T19:48:50.231719Z",
     "iopub.status.idle": "2023-11-06T19:48:50.301217Z",
     "shell.execute_reply": "2023-11-06T19:48:50.299550Z",
     "shell.execute_reply.started": "2023-11-06T19:48:50.232147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need occtype, flood zone, depth_ft, and rel_dam columns\n",
    "# Follow steps from naccs processing to get depth_ft and rel_dam\n",
    "# First, drop unecessary columns\n",
    "# Don't need Source_Table, Occupy_Class, Cover_Class, empty columns\n",
    "# Description, Source, DmgFnId, Occupancy and first col (Unnamed: 0)\n",
    "# because index was written out\n",
    "# Don't need all na columns either (just for automobiles, apparently)\n",
    "hazus_res = hazus_res.loc[:,[col for col in hazus_res.columns if 'ft' in col]]\n",
    "hazus_res = hazus_res.dropna(axis=1, how='all')\n",
    "# Add the occtype and fld_zone columns\n",
    "hazus_res = hazus_res.assign(occtype=occtype,\n",
    "                             fld_zone=fz.str[0])\n",
    "\n",
    "# Then, occtype and fld_zone as index and melt rest of columns. \n",
    "idvars = ['occtype', 'fld_zone']\n",
    "\n",
    "# Get a tidy ddf back\n",
    "hazus_melt = tidy_ddfs(hazus_res, idvars)\n",
    "\n",
    "# Delete depth_str and pctdam and standardize\n",
    "# column names\n",
    "# Since we just have the building types, call this\n",
    "# bld_type instead of occtype\n",
    "dropcols = ['depth_str', 'pct_dam', 'occtype', 'fld_zone']\n",
    "\n",
    "# We create an \"id\" col for the ddfs\n",
    "# Our key for HAZUS is bld_type & fld_zone\n",
    "ddf_id = np.where(hazus_melt['fld_zone'].notnull(),\n",
    "                  hazus_melt['occtype'] + '_' + hazus_melt['fld_zone'],\n",
    "                  hazus_melt['occtype'])\n",
    "\n",
    "# Add this to our dataframe so that we can drop bld_type & fld_zone\n",
    "# Easier to have the flood zone as a capital letter\n",
    "# It's lower case because of earlier code to do\n",
    "# some processing\n",
    "hazus_melt = hazus_melt.assign(ddf_id=pd.Series(ddf_id).str.upper())\n",
    "# Drop columns\n",
    "hazus = hazus_melt.drop(columns=dropcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78001c46-8d0f-4836-86c1-3d1d74e22da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:48:50.818580Z",
     "iopub.status.busy": "2023-11-06T19:48:50.818025Z",
     "iopub.status.idle": "2023-11-06T19:48:50.876389Z",
     "shell.execute_reply": "2023-11-06T19:48:50.874875Z",
     "shell.execute_reply.started": "2023-11-06T19:48:50.818527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we're going to process this tidy dataframe into a dictionary\n",
    "# for easier ensemble generation\n",
    "\n",
    "# After we get this new column, we are going to create two\n",
    "# new columns based on the +/- .3*pt_estimate (30% uncertainty) \n",
    "# assumption from Maggie's paper \n",
    "# (https://www.nature.com/articles/s41467-020-19188-9)\n",
    "# We will take the ddf_id, depth_ft, and these two columns\n",
    "# to do the same thing as before for the dict of dicts\n",
    "# We need to use max(0, ) and min(1, ) to make sure the +/- .3\n",
    "# doesn't lead to negative losses, greater than 100% losses\n",
    "# Since Maggie's paper, though, there have been studies\n",
    "# suggesting that the damage distribution at each depth\n",
    "# follows more of a long upper tailed Beta distribution. \n",
    "# While we don't have parameters for this, we can at least \n",
    "# represent a wider upper tail. So, -.3 and +.5 can better\n",
    "# represent this\n",
    "# A key assumption is that\n",
    "# we can round depths to the nearest value in the\n",
    "# dictionary to estimate their loss. There is no guidance in the\n",
    "# use of DDFs about interpolating between values given on the DDF\n",
    "# NFIP assessed damages data (recently released with the new v2 of\n",
    "# the NFIP claims) only provides depth in feet, rounded to the\n",
    "# nearest foot. So, any uncertainty surrounding the depth-damage\n",
    "# relationship for any foot should include some component of \n",
    "# measurement error in representing some non rounded depth value\n",
    "# to the rounded value and estimating a relationship\n",
    "# To implement this, we will round all depths to the nearest foot\n",
    "# before we check for whether they are inside the bounds for\n",
    "# estimating losses with a particular depth-damage function\n",
    "# Because of this, rounding the parameters to the nearest\n",
    "# hundredth is a much lower order concern\n",
    "dam_low = np.maximum(0, hazus['rel_dam'] - .3*hazus['rel_dam']).round(2)\n",
    "dam_high = np.minimum(1, hazus['rel_dam'] + .5*hazus['rel_dam']).round(2)\n",
    "\n",
    "# Add these columns into our dataframe\n",
    "hazus = hazus.assign(low=dam_low,\n",
    "                     high=dam_high)\n",
    "\n",
    "# For reasons that will become more obvious later,\n",
    "# it is really helpful to store our params as a list\n",
    "# Get param cols\n",
    "uni_params = ['low', 'high']\n",
    "\n",
    "# Get df of ddf_id, depth_ft\n",
    "hazus_f = hazus[['ddf_id', 'depth_ft']]\n",
    "# Now store params as a list\n",
    "hazus_f = hazus_f.assign(params=hazus[uni_params].values.tolist())\n",
    "\n",
    "# We are going to write out hazus_f \n",
    "# In generating the ensemble for losses\n",
    "# we are going to merge this dataframe\n",
    "# with our structure ensemble - merged with\n",
    "# depths. So, on haz_depth & depth_ft from hazus_f\n",
    "# plus the structure archetype, we can get\n",
    "# the rel_dam parameters. We will draw from this\n",
    "# and get the rel_dam realization for this\n",
    "# state of the world\n",
    "# But, he way this data is stored requires a few assumptions\n",
    "# about loss estimation\n",
    "# First, any depths below that lowest depth have 0 loss\n",
    "# Second, any depths above the highest depth have the same\n",
    "# loss as the highest depth \n",
    "# To implement this, we will check depths (after drawing from their\n",
    "# distribution at each location) for whether they are inside\n",
    "# the range of the particular DDF which can be defined with \n",
    "# conastants. If below, loss is 0. If above, swap with\n",
    "# the upper bound\n",
    "# This is why it's very helpful to have the params stored as \n",
    "# a list, because now we can get unique key/value pairs\n",
    "# for the ddf_id and the params\n",
    "# We need two dicts for HAZUS\n",
    "# One is with the params list\n",
    "# One is just ddf_id to rel_dam (for benchmark loss calculations\n",
    "# when uncertainty is not considered)\n",
    "\n",
    "# We can call our helper function to get our dictionaries\n",
    "HAZUS_MAX_DICT = ddf_max_depth_dict(hazus_f, 'params')\n",
    "HAZUS_MAX_NOUNC_DICT = ddf_max_depth_dict(hazus, 'rel_dam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e508c-7c50-4a49-8665-56267f8de4e6",
   "metadata": {},
   "source": [
    "### NACCS DDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33aca7c2-b81d-48b0-ba0a-a2ba3391c913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:48:53.619241Z",
     "iopub.status.busy": "2023-11-06T19:48:53.618726Z",
     "iopub.status.idle": "2023-11-06T19:48:53.650266Z",
     "shell.execute_reply": "2023-11-06T19:48:53.648475Z",
     "shell.execute_reply.started": "2023-11-06T19:48:53.619195Z"
    }
   },
   "outputs": [],
   "source": [
    "# For NACCS, we have the RES 1 DDFs\n",
    "# For elevation, we have RES-OPEN and RES-ENC\n",
    "# These are very similar in terms of damages so\n",
    "# only need to retain RES-OPEN for simplicity for\n",
    "# our current case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f838a2d4-2491-449b-92dc-fc345ceea1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:48:53.861959Z",
     "iopub.status.busy": "2023-11-06T19:48:53.860766Z",
     "iopub.status.idle": "2023-11-06T19:48:53.905819Z",
     "shell.execute_reply": "2023-11-06T19:48:53.904326Z",
     "shell.execute_reply.started": "2023-11-06T19:48:53.861909Z"
    }
   },
   "outputs": [],
   "source": [
    "# NACCS need some preprocessing as well\n",
    "# First, subset to the relevant Occupancy types\n",
    "# We want to end up with ddf ids 1swb, open, etc.\n",
    "# don't need to keep the RES1- part for this case study\n",
    "naccs['res_type'] = naccs['Occupancy'].str.split('-').str[0]\n",
    "naccs['bld_type'] = naccs['Occupancy'].str.split('-').str[1]\n",
    "occ_types = ['1SWB', '2SWB', '1SNB', '2SNB', 'OPEN']\n",
    "naccs_res = naccs.loc[(naccs['bld_type'].isin(occ_types)) &\n",
    "                      ((naccs['res_type'] == 'RES1') |\n",
    "                       (naccs['res_type'] == 'RES'))]\n",
    "\n",
    "# Next, drop columns we don't need\n",
    "drop_cols = ['Description', 'Source', 'Occupancy', 'res_type']\n",
    "naccs_res = naccs_res.drop(columns=drop_cols)\n",
    "\n",
    "# Rename DamageCategory\n",
    "naccs_res = naccs_res.rename(columns={'DamageCategory': 'dam_cat',\n",
    "                                      'bld_type': 'ddf_id'})\n",
    "# Get ddf_id from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50cd8f69-6b40-47a5-a9e0-ce956e07f956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:48:55.941279Z",
     "iopub.status.busy": "2023-11-06T19:48:55.940774Z",
     "iopub.status.idle": "2023-11-06T19:48:56.027430Z",
     "shell.execute_reply": "2023-11-06T19:48:56.025966Z",
     "shell.execute_reply.started": "2023-11-06T19:48:55.941233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now get the melted dataframe\n",
    "idvars = ['ddf_id', 'dam_cat']\n",
    "naccs_melt = tidy_ddfs(naccs_res, idvars)\n",
    "\n",
    "# Drop columns we don't need\n",
    "drop_cols = ['depth_str', 'pct_dam']\n",
    "naccs_f = naccs_melt.drop(columns=drop_cols)\n",
    "\n",
    "# We want to pivot the dataframe so that Min/ML/Max are our columns\n",
    "naccs_piv = naccs_f.pivot(index=['ddf_id', 'depth_ft'],\n",
    "                          columns='dam_cat')['rel_dam'].reset_index()\n",
    "\n",
    "# We want to obtain our 'params' column\n",
    "# same as above\n",
    "p_cols = ['Min', 'ML', 'Max']\n",
    "tri_params = naccs_piv[p_cols].values\n",
    "# Drop the p_cols\n",
    "naccs_out = naccs_piv.drop(columns=p_cols)\n",
    "naccs_out = naccs_out.assign(params=tri_params.tolist())\n",
    "\n",
    "# Get out dict of max depths\n",
    "NACCS_MAX_DICT = ddf_max_depth_dict(naccs_out, 'params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84161e-2cd2-4bbf-aeb2-96d68386c666",
   "metadata": {},
   "source": [
    "### Save our processed ddfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ffbc854-d3f2-4e3d-ae8b-554426645f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:48:58.436253Z",
     "iopub.status.busy": "2023-11-06T19:48:58.435747Z",
     "iopub.status.idle": "2023-11-06T19:48:58.824053Z",
     "shell.execute_reply": "2023-11-06T19:48:58.822569Z",
     "shell.execute_reply.started": "2023-11-06T19:48:58.436208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main directory\n",
    "ddf_out_dir = join(VULN_DIR_I, 'physical')\n",
    "# Main ddf files\n",
    "hazus_out_filep = join(ddf_out_dir, 'hazus_ddfs.pqt')\n",
    "naccs_out_filep = join(ddf_out_dir, 'naccs_ddfs.pqt')\n",
    "# Dictionaries - save as .json for simplicity\n",
    "naccs_max_filep = join(ddf_out_dir, 'naccs.json')\n",
    "hazus_max_filep = join(ddf_out_dir, 'hazus.json')\n",
    "hazus_max_nounc_filep = join(ddf_out_dir, 'hazus_nounc.json')\n",
    "\n",
    "# Only need to call this for one of the files\n",
    "# since they share the same parent directory\n",
    "prepare_saving(hazus_out_filep)\n",
    "\n",
    "# Save as parquet files since\n",
    "# these will directly read in the\n",
    "# DDF params as a list, not as a string\n",
    "hazus_f.to_parquet(hazus_out_filep)\n",
    "naccs_out.to_parquet(naccs_out_filep)\n",
    "\n",
    "# Save the json files\n",
    "with open(naccs_max_filep, 'w') as fp:\n",
    "    json.dump(NACCS_MAX_DICT, fp)\n",
    "\n",
    "with open(hazus_max_filep, 'w') as fp:\n",
    "    json.dump(HAZUS_MAX_DICT, fp)\n",
    "\n",
    "with open(hazus_max_nounc_filep, 'w') as fp:\n",
    "    json.dump(HAZUS_MAX_NOUNC_DICT, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29922b9-012b-4969-a04d-3738e7c5b567",
   "metadata": {},
   "source": [
    "## Proccess NFHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a49725d-26dd-44d0-98bc-b89a45fc70ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:28:06.636062Z",
     "iopub.status.busy": "2023-10-22T20:28:06.635303Z",
     "iopub.status.idle": "2023-10-22T20:28:08.559754Z",
     "shell.execute_reply": "2023-10-22T20:28:08.559031Z",
     "shell.execute_reply.started": "2023-10-22T20:28:06.636019Z"
    }
   },
   "outputs": [],
   "source": [
    "# We want S_FLD_HAZ_AR \n",
    "fld_haz_fp = join(POL_DIR_UZ, FIPS, 'S_FLD_HAZ_AR.shp')\n",
    "nfhl = gpd.read_file(fld_haz_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f19ac8f5-eb5d-4755-9c70-5e9bf06e7e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:32:30.197520Z",
     "iopub.status.busy": "2023-10-22T20:32:30.197255Z",
     "iopub.status.idle": "2023-10-22T20:32:31.864864Z",
     "shell.execute_reply": "2023-10-22T20:32:31.864144Z",
     "shell.execute_reply.started": "2023-10-22T20:32:30.197498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep FLD_ZONE, FLD_AR_ID, STATIC_BFE, geometry\n",
    "keep_cols = ['FLD_ZONE', 'FLD_AR_ID', 'STATIC_BFE', 'ZONE_SUBTY',\n",
    "             'geometry']\n",
    "nfhl_f = nfhl.loc[:,keep_cols]\n",
    "\n",
    "# Adjust .2 pct X zones to X_500\n",
    "nfhl_f.loc[nfhl_f['ZONE_SUBTY'] == '0.2 PCT ANNUAL CHANCE FLOOD HAZARD',\n",
    "           'FLD_ZONE'] = nfhl_f['FLD_ZONE'] + '_500'\n",
    "\n",
    "# Update column names\n",
    "# Lower case\n",
    "nfhl_f.columns = [x.lower() for x in nfhl_f.columns]\n",
    "\n",
    "# Drop ZONE_SUBTY\n",
    "nfhl_f = nfhl_f.drop(columns=['zone_subty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b141b40-0141-4e4e-874b-6813a2c780ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:51:09.615247Z",
     "iopub.status.busy": "2023-10-22T20:51:09.614969Z",
     "iopub.status.idle": "2023-10-22T20:51:10.171497Z",
     "shell.execute_reply": "2023-10-22T20:51:10.170761Z",
     "shell.execute_reply.started": "2023-10-22T20:51:09.615220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clip flood zones to our study area\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "clip_gdf = gpd.read_file(clip_out_filep)\n",
    "\n",
    "# Reproj flood zones\n",
    "nfhl_reproj = nfhl_f.to_crs(clip_gdf.crs)\n",
    "\n",
    "# Clip\n",
    "nfhl_clip = gpd.clip(nfhl_reproj, clip_gdf)\n",
    "\n",
    "# Reproject back\n",
    "nfhl_clip_out = nfhl_clip.to_crs(nfhl_f.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eedff84c-ddb7-47b4-9551-6df4e7525478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:51:11.666793Z",
     "iopub.status.busy": "2023-10-22T20:51:11.666602Z",
     "iopub.status.idle": "2023-10-22T20:51:13.234539Z",
     "shell.execute_reply": "2023-10-22T20:51:13.233721Z",
     "shell.execute_reply.started": "2023-10-22T20:51:11.666778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write file\n",
    "nfhl_out_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "prepare_saving(nfhl_out_filep)\n",
    "nfhl_clip_out.to_file(nfhl_out_filep,\n",
    "                      driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5095b468-9d10-4c7e-94f1-d74e80a4d746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:31:31.909701Z",
     "iopub.status.busy": "2023-10-22T20:31:31.909472Z",
     "iopub.status.idle": "2023-10-22T20:31:32.304728Z",
     "shell.execute_reply": "2023-10-22T20:31:32.303576Z",
     "shell.execute_reply.started": "2023-10-22T20:31:31.909683Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is optional: delete the nfhl directory to reduce\n",
    "# the file storage burden\n",
    "if RM_NFHL:\n",
    "    # Get directory name\n",
    "    nfhl_dir = join(POL_DIR_UZ, FIPS)\n",
    "    \n",
    "    # Try to remove the tree; if it fails,\n",
    "    # throw an error using try...except.\n",
    "    try:\n",
    "        shutil.rmtree(nfhl_dir)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77a02a-d329-479f-8355-93f0a34ded2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T17:55:19.418873Z",
     "iopub.status.busy": "2023-10-20T17:55:19.418001Z",
     "iopub.status.idle": "2023-10-20T17:55:21.014322Z",
     "shell.execute_reply": "2023-10-20T17:55:21.012336Z",
     "shell.execute_reply.started": "2023-10-20T17:55:19.418819Z"
    }
   },
   "source": [
    "## Process Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c51a0c-0455-4a05-8e62-6708bd75e67d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:10:26.499295Z",
     "iopub.status.busy": "2023-10-27T20:10:26.498758Z",
     "iopub.status.idle": "2023-10-27T20:10:26.789208Z",
     "shell.execute_reply": "2023-10-27T20:10:26.787836Z",
     "shell.execute_reply.started": "2023-10-27T20:10:26.499247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "clip_gdf = gpd.read_file(clip_out_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "881d0eee-bee3-45fe-97a8-f4f2eeda0bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:12:35.039960Z",
     "iopub.status.busy": "2023-10-27T20:12:35.039391Z",
     "iopub.status.idle": "2023-10-27T20:14:33.367301Z",
     "shell.execute_reply": "2023-10-27T20:14:33.366002Z",
     "shell.execute_reply.started": "2023-10-27T20:12:35.039907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ref: tract\n",
      "Saved Ref: block\n",
      "Saved Ref: bg\n",
      "Saved Ref: county\n",
      "Saved Ref: zcta\n"
     ]
    }
   ],
   "source": [
    "# For each .shp file in our unzipped ref directory\n",
    "# we are going to reproject & clip, then write out\n",
    "for path in Path(REF_DIR_UZ).rglob('*.shp'):\n",
    "    # Read in the file\n",
    "    ref_shp = gpd.read_file(path)\n",
    "    \n",
    "    # Process the filename to figure out what \n",
    "    # reference data this is\n",
    "    # the files are written out in the form of\n",
    "    # tl_2022_34_tract.shp, for example\n",
    "    # so we split the string on '_', take the\n",
    "    # last element of the array, and ignore\n",
    "    # the last 4 characters\n",
    "    ref_name = path.name.split('_')[-1][:-4]\n",
    "    # Replace the ref name with our ref_name dict values\n",
    "    ref_name_out = REF_NAMES_DICT[ref_name]\n",
    "\n",
    "    # Reproject and clip our reference shapefile\n",
    "    ref_reproj = ref_shp.to_crs(clip_gdf.crs)\n",
    "    ref_clipped = gpd.clip(ref_reproj, clip_gdf)\n",
    "    \n",
    "    # Write file\n",
    "    ref_out_filep = join(REF_DIR_I, FIPS, ref_name_out + \".gpkg\")\n",
    "    prepare_saving(ref_out_filep)\n",
    "    ref_clipped.to_file(ref_out_filep,\n",
    "                        driver='GPKG')\n",
    "\n",
    "    # Helpful message to track progress\n",
    "    print(\"Saved Ref: \" + ref_name_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe02b8-4e96-4bbf-8048-bfb4b9fda231",
   "metadata": {},
   "source": [
    "## Process Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d22ef-3464-4c89-94ce-e357e6607a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to process these all at the county level\n",
    "# sovi is in VULN_DIR_UZ/social/{STATEABBR}/SoVI2010_{STATEABBR}/...\n",
    "# can glob for .shp (no .shp.xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e07e31-71f1-46f0-a6a6-6bf8f1c5fa3f",
   "metadata": {},
   "source": [
    "# Link everything to NSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2018d7-a6db-4a2d-a2df-a57581e1d2c9",
   "metadata": {},
   "source": [
    "## Link NSI with Flood Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "044aa2f8-8802-4cb0-be12-8b7baf88e750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:51:30.935675Z",
     "iopub.status.busy": "2023-10-22T20:51:30.935001Z",
     "iopub.status.idle": "2023-10-22T20:51:31.563827Z",
     "shell.execute_reply": "2023-10-22T20:51:31.563100Z",
     "shell.execute_reply.started": "2023-10-22T20:51:30.935626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just for jupyter notebooks\n",
    "# Scripts don't need to reload the data since it runs all at once\n",
    "# Jupyter is more for development, and might only run\n",
    "# some sections at a time\n",
    "# Using same names as above for consistency\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)\n",
    "\n",
    "nfhl_out_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "nfhl_clip_out = gpd.read_file(nfhl_out_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef159940-f944-474d-954d-8abf84ff2fd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T20:52:07.532239Z",
     "iopub.status.busy": "2023-10-22T20:52:07.531969Z",
     "iopub.status.idle": "2023-10-22T20:52:08.459985Z",
     "shell.execute_reply": "2023-10-22T20:52:08.458792Z",
     "shell.execute_reply.started": "2023-10-22T20:52:07.532217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project nsi to flood zone crs\n",
    "nsi_fz = nsi_clip_out.to_crs(nfhl_clip_out.crs)\n",
    "\n",
    "# Spatial join, retaining flood zone cols\n",
    "# Only need the id and geom from nsi for this\n",
    "fz_m = gpd.sjoin(nsi_fz[['fd_id', 'geometry']],\n",
    "                 nfhl_clip_out,\n",
    "                 predicate='within')\n",
    "\n",
    "# I checked for issues like overlapping flood zones\n",
    "# resulting in NSI structures in multiple polygons\n",
    "# and did not find any. That's good, but chances\n",
    "# are there will be counties where this happens\n",
    "# and we will need code to handle these consistently\n",
    "\n",
    "# Write out fd_id/fld_ar_id/fld_zone/static_bfe\n",
    "keep_cols = ['fd_id', 'fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "fz_m_out = fz_m[keep_cols]\n",
    "\n",
    "nsi_fz_filep = join(EXP_DIR_I, FIPS, 'nsi_fz.pqt')\n",
    "prepare_saving(nsi_fz_filep)\n",
    "fz_m_out.to_parquet(nsi_fz_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f461bca-7b68-43d3-b4c7-dc7c54a8898a",
   "metadata": {},
   "source": [
    "## Link NSI with Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b1bf23-e259-421c-90af-926d59e9b6a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:26:25.699324Z",
     "iopub.status.busy": "2023-10-27T20:26:25.698372Z",
     "iopub.status.idle": "2023-10-27T20:26:26.569780Z",
     "shell.execute_reply": "2023-10-27T20:26:26.568715Z",
     "shell.execute_reply.started": "2023-10-27T20:26:25.699254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter nb only\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "nsi_clip_out = gpd.read_file(EXP_OUT_FILEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862b61c3-50ff-45d2-89bd-2c3db1efde76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T20:28:09.669185Z",
     "iopub.status.busy": "2023-10-27T20:28:09.668578Z",
     "iopub.status.idle": "2023-10-27T20:28:10.803838Z",
     "shell.execute_reply": "2023-10-27T20:28:10.802486Z",
     "shell.execute_reply.started": "2023-10-27T20:28:09.669129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked reference to NSI: tract_id\n",
      "Linked reference to NSI: block_id\n",
      "Linked reference to NSI: bg_id\n",
      "Linked reference to NSI: zcta_id\n"
     ]
    }
   ],
   "source": [
    "# For zcta, tract, bg, and block\n",
    "# we want to do spatial joins to link\n",
    "# up fd_id in the NSI with the ref\n",
    "# We will use config data to do this\n",
    "# since other references may be brought in \n",
    "# down the line\n",
    "# We are going to store fd_id/ref_id links in a dataframe\n",
    "ref_df_list = []\n",
    "for ref_name, ref_id in REF_ID_NAMES_DICT.items():\n",
    "    ref_filep = join(REF_DIR_I, FIPS, ref_name + \".gpkg\")\n",
    "\n",
    "    # Load in the ref file\n",
    "    ref_geo = gpd.read_file(ref_filep)\n",
    "\n",
    "    # Limit the geodataframe to our ref id and 'geometry' column\n",
    "    keep_col = [ref_id, 'geometry']\n",
    "    ref_geo_sub = ref_geo[keep_col]\n",
    "\n",
    "    # Limit the NSI to our fd_id and geometry column\n",
    "    keep_col_nsi = ['fd_id', 'geometry']\n",
    "    nsi_sub = nsi_clip_out[keep_col_nsi]\n",
    "\n",
    "    # Reproj nsi_sub to the reference crs\n",
    "    nsi_reproj = nsi_sub.to_crs(ref_geo.crs)\n",
    "\n",
    "    # Do a spatial join\n",
    "    nsi_ref = gpd.sjoin(nsi_reproj, ref_geo_sub, predicate='within')\n",
    "\n",
    "    # Set index to fd_id and just keep the ref_id\n",
    "    # Rename that column to our ref_name + '_id'\n",
    "    # Append this to our ref_df_list\n",
    "    nsi_ref_f = nsi_ref.set_index('fd_id')[[ref_id]]\n",
    "    nsi_ref_f = nsi_ref_f.rename(columns={ref_id: ref_name + '_id'})\n",
    "    ref_df_list.append(nsi_ref_f)\n",
    "\n",
    "    # Helpful message\n",
    "    print('Linked reference to NSI: ' + ref_name + '_id')\n",
    "\n",
    "# Can concat and write\n",
    "nsi_refs = pd.concat(ref_df_list, axis=1).reset_index()\n",
    "ref_filep = join(EXP_DIR_I,  FIPS, 'nsi_ref.pqt')\n",
    "prepare_saving(ref_filep)\n",
    "nsi_refs.to_parquet(ref_filep)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110e1bc-8f47-49c3-9f68-89247784d83c",
   "metadata": {},
   "source": [
    "## Link NSI with Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891c8d6-8b60-460d-acda-82a09febfa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ae3738-6ce1-4625-9777-cb42c218896b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T19:18:34.376501Z",
     "iopub.status.busy": "2023-10-21T19:18:34.375754Z",
     "iopub.status.idle": "2023-10-21T19:18:34.418667Z",
     "shell.execute_reply": "2023-10-21T19:18:34.417529Z",
     "shell.execute_reply.started": "2023-10-21T19:18:34.376451Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf7aaad-086f-40a9-92e4-374d1efb7495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T19:18:34.422743Z",
     "iopub.status.busy": "2023-10-21T19:18:34.421798Z",
     "iopub.status.idle": "2023-10-21T19:18:36.792080Z",
     "shell.execute_reply": "2023-10-21T19:18:36.791133Z",
     "shell.execute_reply.started": "2023-10-21T19:18:34.422667Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import shape\n",
    "import rasterio \n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae98b55-a0e0-4e75-b3f3-b9dcc294234c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T19:18:36.793561Z",
     "iopub.status.busy": "2023-10-21T19:18:36.793170Z",
     "iopub.status.idle": "2023-10-21T19:18:36.812216Z",
     "shell.execute_reply": "2023-10-21T19:18:36.811375Z",
     "shell.execute_reply.started": "2023-10-21T19:18:36.793534Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec963f-fac9-4bb2-ac41-364fde7dbcb8",
   "metadata": {},
   "source": [
    "# Process - everything ends up at county level and clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879462d-4490-4f69-88bf-15efa5de8d78",
   "metadata": {},
   "source": [
    "## Process clip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462240d3-88c1-461a-bcbd-22f0585387f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:00:13.357822Z",
     "iopub.status.busy": "2023-10-20T22:00:13.357586Z",
     "iopub.status.idle": "2023-10-20T22:00:13.386346Z",
     "shell.execute_reply": "2023-10-20T22:00:13.385135Z",
     "shell.execute_reply.started": "2023-10-20T22:00:13.357799Z"
    }
   },
   "outputs": [],
   "source": [
    "# For our case study, we are going to focus on Gloucester City, NJ\n",
    "# Our config.yaml loads in a county indexed clip file\n",
    "# so that we can restrict all our data to the GC boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4108629a-beb2-44ba-94a2-634f69a44dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:00:13.795836Z",
     "iopub.status.busy": "2023-10-20T22:00:13.795345Z",
     "iopub.status.idle": "2023-10-20T22:00:13.829820Z",
     "shell.execute_reply": "2023-10-20T22:00:13.828661Z",
     "shell.execute_reply.started": "2023-10-20T22:00:13.795789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data we downloaded from the county's REST API server\n",
    "clip_filep = join(REF_DIR_R, FIPS, 'clip.json')\n",
    "with open(clip_filep) as f:\n",
    "    clip_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08a969c-fd99-4dae-ae15-041f5be159f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:00:14.307472Z",
     "iopub.status.busy": "2023-10-20T22:00:14.305803Z",
     "iopub.status.idle": "2023-10-20T22:00:15.504046Z",
     "shell.execute_reply": "2023-10-20T22:00:15.502806Z",
     "shell.execute_reply.started": "2023-10-20T22:00:14.307410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use pandas to get the data in a form that is easier\n",
    "# to turn into a geodataframe for clipping\n",
    "clip_df = pd.json_normalize(clip_data['features'])\n",
    "# We want to make a polygon out of the geometry coordinates\n",
    "# We can access that from the original json object\n",
    "clip_geo = [shape(i['geometry']) for i in clip_data['features']]\n",
    "# We can create a geodataframe of clip_df by adding clip_geo\n",
    "# as its geometry column\n",
    "clip_gdf = gpd.GeoDataFrame(clip_df,\n",
    "                            crs=CLIP_CRS,\n",
    "                            geometry=clip_geo)\n",
    "\n",
    "# We can clean up the gdf by removing the\n",
    "# type, id, geometry.type and geometry.coordinates columns\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "clip_gdf = clip_gdf.drop(columns=drop_col)\n",
    "\n",
    "# Write the file out to interim\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "prepare_saving(clip_out_filep)\n",
    "clip_gdf.to_file(clip_out_filep,\n",
    "                 driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb2e91-abce-44c5-8a49-14efa3cae4e4",
   "metadata": {},
   "source": [
    "## Process NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30dc5b6-24f2-4552-a274-465aab776350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T20:11:01.336658Z",
     "iopub.status.busy": "2023-10-20T20:11:01.336524Z",
     "iopub.status.idle": "2023-10-20T20:11:01.374731Z",
     "shell.execute_reply": "2023-10-20T20:11:01.373440Z",
     "shell.execute_reply.started": "2023-10-20T20:11:01.336643Z"
    }
   },
   "outputs": [],
   "source": [
    "# The NSI comes with all the data necessary for performing a standard \n",
    "# flood risk assessment. It is still useful to process the raw data.\n",
    "# Here, we subset to residential properties with 1 to 2 stories\n",
    "# and save as a geodataframe. These are the types of residences we have\n",
    "# multiple depth-damage functions for and a literature base to draw \n",
    "# from to introduce uncertainty in these loss estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d179e9-26c9-4f13-88c8-c54d7c1188c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:00:17.500417Z",
     "iopub.status.busy": "2023-10-20T22:00:17.499889Z",
     "iopub.status.idle": "2023-10-20T22:00:18.107178Z",
     "shell.execute_reply": "2023-10-20T22:00:18.106014Z",
     "shell.execute_reply.started": "2023-10-20T22:00:17.500369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read NSI, reset index upon reading\n",
    "# TODO - this will be json...\n",
    "nsi_filep = join(EXP_DIR_R, FIPS, 'nsi.pqt')\n",
    "nsi_full = pd.read_parquet(nsi_filep).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b268cd77-1947-4d88-8e34-b6010345b208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:00:18.234753Z",
     "iopub.status.busy": "2023-10-20T22:00:18.234287Z",
     "iopub.status.idle": "2023-10-20T22:00:18.487750Z",
     "shell.execute_reply": "2023-10-20T22:00:18.486899Z",
     "shell.execute_reply.started": "2023-10-20T22:00:18.234707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to gdf\n",
    "# This is useful for some spatial joins we need to perform\n",
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_full['properties.x'],\n",
    "                             nsi_full['properties.y'])\n",
    "# The NSI CRS is EPSG 4326\n",
    "nsi_gdf = gpd.GeoDataFrame(nsi_full, geometry=geometry,\n",
    "                           crs=NSI_CRS)\n",
    "\n",
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dacc2ed-62c5-481c-8b0c-9c2b173d8604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:00:19.962273Z",
     "iopub.status.busy": "2023-10-20T22:00:19.962043Z",
     "iopub.status.idle": "2023-10-20T22:00:20.149774Z",
     "shell.execute_reply": "2023-10-20T22:00:20.148930Z",
     "shell.execute_reply.started": "2023-10-20T22:00:19.962253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to residential properties and update\n",
    "# RES 1 - single family\n",
    "# RES 2 - manufactured home\n",
    "# RES 3 - multifamily (but could fit into a depth-damage function\n",
    "# archetype depending on # stories)\n",
    "# We are going to use RES1 for this case-study\n",
    "# It is the only occtype with hazus and naccs\n",
    "# DDFs and has less ambiguous classification\n",
    "\n",
    "# occtype category for easier use in loss estimation steps\n",
    "\n",
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['occtype'].str[:4] == 'RES1']\n",
    "\n",
    "# For this case-study, don't use any building with more \n",
    "# than 2 stories\n",
    "res1_3s_ind = nsi_res['num_story'] > 2\n",
    "# Final residential dataframe\n",
    "res_f = nsi_res.loc[~res1_3s_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca1d9e1-e9f1-4c5b-b55a-6c80e1f18b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:00:37.798764Z",
     "iopub.status.busy": "2023-10-20T22:00:37.798528Z",
     "iopub.status.idle": "2023-10-20T22:00:38.018600Z",
     "shell.execute_reply": "2023-10-20T22:00:38.017631Z",
     "shell.execute_reply.started": "2023-10-20T22:00:37.798745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to relevant columns\n",
    "cols = ['fd_id', 'occtype', 'found_type', 'cbfips',\n",
    "        'ftprntsrc', 'found_ht', 'val_struct',\n",
    "        'val_cont', 'source', 'firmzone', 'ground_elv_m',\n",
    "        'geometry']\n",
    "\n",
    "res_out = res_f.loc[:,cols]\n",
    "\n",
    "# Clip to our clip boundary\n",
    "# They are in the same CRS\n",
    "nsi_clip_out = gpd.clip(res_out, clip_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5fffede-d15f-4a04-8bc0-7865247078a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:02:02.695741Z",
     "iopub.status.busy": "2023-10-20T22:02:02.695549Z",
     "iopub.status.idle": "2023-10-20T22:02:06.572811Z",
     "shell.execute_reply": "2023-10-20T22:02:06.570999Z",
     "shell.execute_reply.started": "2023-10-20T22:02:02.695726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out to interim/exposure/FIPS/\n",
    "# Single family homes -- sf\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "prepare_saving(EXP_OUT_FILEP)\n",
    "nsi_clip_out.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6fa61-56df-40a0-9c3f-219d09d52597",
   "metadata": {},
   "source": [
    "## Process Depth-Damage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26a4fbb6-23b2-49b0-a485-6bc268bc92c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T18:36:29.778168Z",
     "iopub.status.busy": "2023-10-21T18:36:29.776983Z",
     "iopub.status.idle": "2023-10-21T18:36:30.180945Z",
     "shell.execute_reply": "2023-10-21T18:36:30.179816Z",
     "shell.execute_reply.started": "2023-10-21T18:36:29.778116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generally, we will process these DDFs the same way since they\n",
    "# are written in mostly the same format\n",
    "# However, there are a few preprocessing steps necessary for the hazus\n",
    "# ddfs. Also, there are some differences for NACCS vs. HAZUS\n",
    "# shallow uncertainty representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2558d82-b45e-4249-8bd8-c416d0d5ca37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:26:16.050429Z",
     "iopub.status.busy": "2023-10-21T20:26:16.049398Z",
     "iopub.status.idle": "2023-10-21T20:26:16.501317Z",
     "shell.execute_reply": "2023-10-21T20:26:16.500151Z",
     "shell.execute_reply.started": "2023-10-21T20:26:16.050379Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read depth damage functions\n",
    "ddf_filedir = join(VULN_DIR_UZ, \"physical\", NATION)\n",
    "naccs = pd.read_csv(join(ddf_filedir, \"naccs_ddfs.csv\"))\n",
    "hazus = pd.read_csv(join(ddf_filedir, \"haz_fl_dept.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec16a6f-2dab-456a-af85-968779cd07cc",
   "metadata": {},
   "source": [
    "### HAZUS DDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01a06109-cc56-4abf-8973-dfd3ebd531fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:26:16.504458Z",
     "iopub.status.busy": "2023-10-21T20:26:16.503584Z",
     "iopub.status.idle": "2023-10-21T20:26:16.544604Z",
     "shell.execute_reply": "2023-10-21T20:26:16.543643Z",
     "shell.execute_reply.started": "2023-10-21T20:26:16.504411Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, preprocessing for hazus ddfs\n",
    "# For basements, use FIA (MOD.) which does one and two floors by\n",
    "# A and V zones\n",
    "# For no basements, use USACE - IWR\n",
    "# which does one and two floor, no flood zone specified\n",
    "# 106: FIA (MOD.) 1S WB A zone\n",
    "# 114: \"\" V zone\n",
    "# 108: FIA (MOD.) 1S WB A zone\n",
    "# 116: \"\" V zone\n",
    "# 129: USACE - IWR 1S NB\n",
    "# 130: USCAE - IWR 2S+ NB\n",
    "# For elevating homes, we can use Pile foundation DDFs\n",
    "# from USACE - Wilmington\n",
    "# 178 - 1S Pile Foundation\n",
    "# 183 - 2S Pile Foundation\n",
    "# These are no basement homes, so to speak\n",
    "# The USACE New Orleans DDFs have some pier foundation\n",
    "# DDFs with fresh & salt water and long & short duration\n",
    "# but this does not appear to apply to out study area\n",
    "# Subset to DmgFnId in the codes above\n",
    "dmg_ids = [106, 108, 114, 116, 129, 130, 178, 183]\n",
    "hazus_res = hazus[(hazus['DmgFnId'].isin(dmg_ids)) & \n",
    "                  (hazus['Occupancy'] == 'RES1')]\n",
    "\n",
    "# Make occtype column in the same form that the NSI has\n",
    "# e.g. RES1-1SNB\n",
    "# Add column for A or V zone\n",
    "# Note: outside SFHA basement homes will take A zone\n",
    "# What other option do we have? \n",
    "\n",
    "# Split Description by comma. \n",
    "# The split[0] element tells us stories (but description sometimes\n",
    "# says floors instead of story...)\n",
    "# Can get around this issue by looking at first word\n",
    "# The split[1] element\n",
    "# tells us w/ basement or no basement. Use this to create occtype\n",
    "desc = hazus_res['Description'].str.split(',')\n",
    "s_type = desc.str[0].str.split(' ').str[0]\n",
    "s_type = s_type.str.replace('one', '1').str.replace('two', '2')\n",
    "b_type = desc.str[1].str.strip()\n",
    "# Below, we are just trying to get archetypes like\n",
    "# 1SNB, 2SWB, 1SPL -- for pile foundation\n",
    "occtype = np.where(b_type == 'w/ basement',\n",
    "                   s_type + 'SWB',\n",
    "                   s_type + 'SNB')\n",
    "occtype = np.where(b_type == 'Pile foundation',\n",
    "                   s_type + 'SPL',\n",
    "                   occtype)\n",
    "# Some of these HAZUS DDFs require us to keep track of the\n",
    "# flood zone they're in\n",
    "# I don't think this matters for our case study since\n",
    "# there are no high wave coastsal zones\n",
    "# This line is designed to work specifically \n",
    "# with the way the descriptions\n",
    "# are written out for the DDFs used in this case study\n",
    "fz = desc.str[-1].str.lower().str.replace('structure', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f934250-6c91-47fd-9497-13702fd8f776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:26:16.881121Z",
     "iopub.status.busy": "2023-10-21T20:26:16.880534Z",
     "iopub.status.idle": "2023-10-21T20:26:16.941044Z",
     "shell.execute_reply": "2023-10-21T20:26:16.939955Z",
     "shell.execute_reply.started": "2023-10-21T20:26:16.881077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need occtype, flood zone, depth_ft, and rel_dam columns\n",
    "# Follow steps from naccs processing to get depth_ft and rel_dam\n",
    "# First, drop unecessary columns\n",
    "# Don't need Source_Table, Occupy_Class, Cover_Class, empty columns\n",
    "# Description, Source, DmgFnId, Occupancy and first col (Unnamed: 0)\n",
    "# because index was written out\n",
    "# Don't need all na columns either (just for automobiles, apparently)\n",
    "hazus_res = hazus_res.loc[:,[col for col in hazus_res.columns if 'ft' in col]]\n",
    "hazus_res = hazus_res.dropna(axis=1, how='all')\n",
    "# Add the occtype and fld_zone columns\n",
    "hazus_res = hazus_res.assign(occtype=occtype,\n",
    "                             fld_zone=fz.str[0])\n",
    "\n",
    "# Then, occtype and fld_zone as index and melt rest of columns. \n",
    "idvars = ['occtype', 'fld_zone']\n",
    "\n",
    "# Get a tidy ddf back\n",
    "hazus_melt = tidy_ddfs(hazus_res, idvars)\n",
    "\n",
    "# Delete depth_str and pctdam and standardize\n",
    "# column names\n",
    "# Since we just have the building types, call this\n",
    "# bld_type instead of occtype\n",
    "dropcols = ['depth_str', 'pct_dam', 'occtype', 'fld_zone']\n",
    "\n",
    "# We create an \"id\" col for the ddfs\n",
    "# Our key for HAZUS is bld_type & fld_zone\n",
    "ddf_id = np.where(hazus_melt['fld_zone'].notnull(),\n",
    "                  hazus_melt['occtype'] + '_' + hazus_melt['fld_zone'],\n",
    "                  hazus_melt['occtype'])\n",
    "\n",
    "# Add this to our dataframe so that we can drop bld_type & fld_zone\n",
    "# Easier to have the flood zone as a capital letter\n",
    "# It's lower case because of earlier code to do\n",
    "# some processing\n",
    "hazus_melt = hazus_melt.assign(ddf_id=pd.Series(ddf_id).str.upper())\n",
    "# Drop columns\n",
    "hazus = hazus_melt.drop(columns=dropcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "78001c46-8d0f-4836-86c1-3d1d74e22da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:26:17.350359Z",
     "iopub.status.busy": "2023-10-21T20:26:17.349713Z",
     "iopub.status.idle": "2023-10-21T20:26:17.390459Z",
     "shell.execute_reply": "2023-10-21T20:26:17.389272Z",
     "shell.execute_reply.started": "2023-10-21T20:26:17.350309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we're going to process this tidy dataframe into a dictionary\n",
    "# for easier ensemble generation\n",
    "\n",
    "# After we get this new column, we are going to create two\n",
    "# new columns based on the +/- .3*pt_estimate (30% uncertainty) \n",
    "# assumption from Maggie's paper \n",
    "# (https://www.nature.com/articles/s41467-020-19188-9)\n",
    "# We will take the ddf_id, depth_ft, and these two columns\n",
    "# to do the same thing as before for the dict of dicts\n",
    "# We need to use max(0, ) and min(1, ) to make sure the +/- .3\n",
    "# doesn't lead to negative losses, greater than 100% losses\n",
    "# Since Maggie's paper, though, there have been studies\n",
    "# suggesting that the damage distribution at each depth\n",
    "# follows more of a long upper tailed Beta distribution. \n",
    "# While we don't have parameters for this, we can at least \n",
    "# represent a wider upper tail. So, -.3 and +.5 can better\n",
    "# represent this\n",
    "# A key assumption is that\n",
    "# we can round depths to the nearest value in the\n",
    "# dictionary to estimate their loss. There is no guidance in the\n",
    "# use of DDFs about interpolating between values given on the DDF\n",
    "# NFIP assessed damages data (recently released with the new v2 of\n",
    "# the NFIP claims) only provides depth in feet, rounded to the\n",
    "# nearest foot. So, any uncertainty surrounding the depth-damage\n",
    "# relationship for any foot should include some component of \n",
    "# measurement error in representing some non rounded depth value\n",
    "# to the rounded value and estimating a relationship\n",
    "# To implement this, we will round all depths to the nearest foot\n",
    "# before we check for whether they are inside the bounds for\n",
    "# estimating losses with a particular depth-damage function\n",
    "# Because of this, rounding the parameters to the nearest\n",
    "# hundredth is a much lower order concern\n",
    "dam_low = np.maximum(0, hazus['rel_dam'] - .3*hazus['rel_dam']).round(2)\n",
    "dam_high = np.minimum(1, hazus['rel_dam'] + .5*hazus['rel_dam']).round(2)\n",
    "\n",
    "# Add these columns into our dataframe\n",
    "hazus = hazus.assign(low=dam_low,\n",
    "                     high=dam_high)\n",
    "\n",
    "# Next, we are getting our params\n",
    "# Since we're uniformly sampling from the range\n",
    "# our params are endpoints\n",
    "# We don't need the rel_dam column anymore \n",
    "hazus_f = hazus.drop(columns='rel_dam')\n",
    "\n",
    "# We are going to write out hazus_f \n",
    "# In generating the ensemble for losses\n",
    "# we are going to merge this dataframe\n",
    "# with our structure ensemble - merged with\n",
    "# depths. So, on haz_depth & depth_ft from hazus_f\n",
    "# plus the structure archetype, we can get\n",
    "# the rel_dam parameters. We will draw from this\n",
    "# and get the rel_dam realization for this\n",
    "# state of the world\n",
    "# But, he way this data is stored requires a few assumptions\n",
    "# about loss estimation\n",
    "# First, any depths below that lowest depth have 0 loss\n",
    "# Second, any depths above the highest depth have the same\n",
    "# loss as the highest depth \n",
    "# To implement this, we will check depths (after drawing from their\n",
    "# distribution at each location) for whether they are inside\n",
    "# the range of the particular DDF which can be defined with \n",
    "# conastants. If below, loss is 0. If above, swap with\n",
    "# the upper bound\n",
    "# We are going to do the processing for that in the script\n",
    "# where we generate the loss ensemble using a helper function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e508c-7c50-4a49-8665-56267f8de4e6",
   "metadata": {},
   "source": [
    "### NACCS DDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aca7c2-b81d-48b0-ba0a-a2ba3391c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NACCS, we have the RES 1 DDFs\n",
    "# For elevation, we have RES-OPEN and RES-ENC\n",
    "# These are very similar in terms of damages so\n",
    "# only need to retain RES-OPEN for simplicity for\n",
    "# our current case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f838a2d4-2491-449b-92dc-fc345ceea1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:51:34.838210Z",
     "iopub.status.busy": "2023-10-21T20:51:34.837616Z",
     "iopub.status.idle": "2023-10-21T20:51:34.884762Z",
     "shell.execute_reply": "2023-10-21T20:51:34.882893Z",
     "shell.execute_reply.started": "2023-10-21T20:51:34.838161Z"
    }
   },
   "outputs": [],
   "source": [
    "# NACCS need some preprocessing as well\n",
    "# First, subset to the relevant Occupancy types\n",
    "# We want to end up with ddf ids 1swb, open, etc.\n",
    "# don't need to keep the RES1- part for this case study\n",
    "naccs['res_type'] = naccs['Occupancy'].str.split('-').str[0]\n",
    "naccs['bld_type'] = naccs['Occupancy'].str.split('-').str[1]\n",
    "occ_types = ['1SWB', '2SWB', '1SNB', '2SNB', 'OPEN']\n",
    "naccs_res = naccs.loc[(naccs['bld_type'].isin(occ_types)) &\n",
    "                      ((naccs['res_type'] == 'RES1') |\n",
    "                       (naccs['res_type'] == 'RES'))]\n",
    "\n",
    "# Next, drop columns we don't need\n",
    "drop_cols = ['Description', 'Source', 'Occupancy', 'res_type']\n",
    "naccs_res = naccs_res.drop(columns=drop_cols)\n",
    "\n",
    "# Rename DamageCategory\n",
    "naccs_res = naccs_res.rename(columns={'DamageCategory': 'dam_cat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "50cd8f69-6b40-47a5-a9e0-ce956e07f956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:52:18.886970Z",
     "iopub.status.busy": "2023-10-21T20:52:18.886407Z",
     "iopub.status.idle": "2023-10-21T20:52:18.956009Z",
     "shell.execute_reply": "2023-10-21T20:52:18.954195Z",
     "shell.execute_reply.started": "2023-10-21T20:52:18.886922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now get the melted dataframe\n",
    "idvars = ['bld_type', 'dam_cat']\n",
    "naccs_melt = tidy_ddfs(naccs_res, idvars)\n",
    "\n",
    "# Drop columns we don't need\n",
    "drop_cols = ['depth_str', 'pct_dam']\n",
    "naccs_f = naccs_melt.drop(columns=drop_cols)\n",
    "\n",
    "# We want to pivot the dataframe so that Min/ML/Max are our columns\n",
    "naccs_piv = naccs_f.pivot(index=['bld_type', 'depth_ft'],\n",
    "                          columns='dam_cat')['rel_dam'].reset_index()\n",
    "\n",
    "# These our the parameters for the triangular distributions\n",
    "# we will draw from\n",
    "# Rename them as the parameter names - left, mode, right\n",
    "naccs_out = naccs_piv.rename(columns={'Min': 'left',\n",
    "                                      'ML': 'mode',\n",
    "                                      'Max': 'right'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84161e-2cd2-4bbf-aeb2-96d68386c666",
   "metadata": {},
   "source": [
    "### Save our processed ddfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ffbc854-d3f2-4e3d-ae8b-554426645f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:53:09.744020Z",
     "iopub.status.busy": "2023-10-21T20:53:09.742993Z",
     "iopub.status.idle": "2023-10-21T20:53:10.206181Z",
     "shell.execute_reply": "2023-10-21T20:53:10.204976Z",
     "shell.execute_reply.started": "2023-10-21T20:53:09.743969Z"
    }
   },
   "outputs": [],
   "source": [
    "ddf_out_dir = join(VULN_DIR_I, 'physical')\n",
    "prepare_saving(ddf_out_dir)\n",
    "\n",
    "# Save hazus_f as hazus_ddfs.csv\n",
    "hazus_f.to_csv(join(VULN_DIR_I, 'hazus_ddfs.csv'),\n",
    "               index=False)\n",
    "\n",
    "# Save naccs_out as naccs_ddfs.csv\n",
    "naccs_out.to_csv(join(VULN_DIR_I, 'naccs_ddfs.csv'),\n",
    "                 index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77a02a-d329-479f-8355-93f0a34ded2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T17:55:19.418873Z",
     "iopub.status.busy": "2023-10-20T17:55:19.418001Z",
     "iopub.status.idle": "2023-10-20T17:55:21.014322Z",
     "shell.execute_reply": "2023-10-20T17:55:21.012336Z",
     "shell.execute_reply.started": "2023-10-20T17:55:19.418819Z"
    }
   },
   "source": [
    "## Process Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d0eee-bee3-45fe-97a8-f4f2eeda0bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3455e12-484b-4413-938e-7b2cfe935231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob for shp files in REF_DIR_UZ - we need to get these \n",
    "# clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe02b8-4e96-4bbf-8048-bfb4b9fda231",
   "metadata": {},
   "source": [
    "# Process Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d22ef-3464-4c89-94ce-e357e6607a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to process these all at the county level\n",
    "# sovi is in VULN_DIR_UZ/social/{STATEABBR}/SoVI2010_{STATEABBR}/...\n",
    "# can glob for .shp (no .shp.xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e07e31-71f1-46f0-a6a6-6bf8f1c5fa3f",
   "metadata": {},
   "source": [
    "# Link - everything to structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2018d7-a6db-4a2d-a2df-a57581e1d2c9",
   "metadata": {},
   "source": [
    "## Link NSI with Flood Zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f461bca-7b68-43d3-b4c7-dc7c54a8898a",
   "metadata": {},
   "source": [
    "## Link NSI with Reference Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110e1bc-8f47-49c3-9f68-89247784d83c",
   "metadata": {},
   "source": [
    "## Link NSI with Social Vulnerability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891c8d6-8b60-460d-acda-82a09febfa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

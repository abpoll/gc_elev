{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df130da-8ff7-481a-9f41-49220ebbb9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:47.668093Z",
     "iopub.status.busy": "2023-12-05T20:09:47.667236Z",
     "iopub.status.idle": "2023-12-05T20:09:47.852372Z",
     "shell.execute_reply": "2023-12-05T20:09:47.851239Z",
     "shell.execute_reply.started": "2023-12-05T20:09:47.668015Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea6c73e-9b4f-4bf1-a40c-0d4105e9eafb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:47.857855Z",
     "iopub.status.busy": "2023-12-05T20:09:47.856713Z",
     "iopub.status.idle": "2023-12-05T20:09:48.587354Z",
     "shell.execute_reply": "2023-12-05T20:09:48.586647Z",
     "shell.execute_reply.started": "2023-12-05T20:09:47.857798Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a43d5d-4b14-43c7-80ec-eb029a6e5529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:48.588156Z",
     "iopub.status.busy": "2023-12-05T20:09:48.587959Z",
     "iopub.status.idle": "2023-12-05T20:09:48.598752Z",
     "shell.execute_reply": "2023-12-05T20:09:48.598145Z",
     "shell.execute_reply.started": "2023-12-05T20:09:48.588142Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6111f5a-c61f-469a-92b2-265c90f3cd6d",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120b6681-df66-4e80-8f4b-d2e7495fe878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:48.599311Z",
     "iopub.status.busy": "2023-12-05T20:09:48.599191Z",
     "iopub.status.idle": "2023-12-05T20:09:48.624940Z",
     "shell.execute_reply": "2023-12-05T20:09:48.624294Z",
     "shell.execute_reply.started": "2023-12-05T20:09:48.599299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate structure ensemble\n",
    "# Merge hazard data in\n",
    "# Sample from the depth grids\n",
    "# Add our vulnerability uncertainty\n",
    "# (it's conditioned on the depth value in \n",
    "# a particular state of the world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23370d86-7c25-4b42-afad-11395b3a819f",
   "metadata": {},
   "source": [
    "## Load and subset exposure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59719ccf-11fb-4507-bc0a-86cf020e58d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:48.625657Z",
     "iopub.status.busy": "2023-12-05T20:09:48.625488Z",
     "iopub.status.idle": "2023-12-05T20:09:49.271848Z",
     "shell.execute_reply": "2023-12-05T20:09:49.271142Z",
     "shell.execute_reply.started": "2023-12-05T20:09:48.625640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the single family homes,\n",
    "# the fd_id/reference file\n",
    "# the fd_id/depths file\n",
    "# the fd_id flood zone file\n",
    "nsi_struct = gpd.read_file(join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg'))\n",
    "nsi_ref = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_ref.pqt'))\n",
    "nsi_depths = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_depths.pqt'))\n",
    "nsi_fz = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_fz.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fcd531e-6d19-4f32-908e-04184401466e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:49.272451Z",
     "iopub.status.busy": "2023-12-05T20:09:49.272312Z",
     "iopub.status.idle": "2023-12-05T20:09:49.486254Z",
     "shell.execute_reply": "2023-12-05T20:09:49.485522Z",
     "shell.execute_reply.started": "2023-12-05T20:09:49.272436Z"
    }
   },
   "outputs": [],
   "source": [
    "# The point of the code below is to make it so that\n",
    "# we can draw from triangular distributions\n",
    "# This is a very case-study specific solution\n",
    "# Ultimately, we'd like to have a pdf of depths\n",
    "# to draw from - either a distribution & fitted parameters\n",
    "# or an empirical distribution\n",
    "# So, we are going to round depths to the nearest foot\n",
    "# and we're going to retain the rows that have non-zero\n",
    "# foot relative to grade depths\n",
    "# Don't want to bake this processing into the process_haz\n",
    "# scripts because it is case-study specific\n",
    "\n",
    "## Note - we're not doing triangular anymore, for now\n",
    "# Instead, we need to do processing that\n",
    "# let's us treat these as separate scenarios\n",
    "\n",
    "# Filter to properties with > 0 \n",
    "nsi_depths = nsi_depths[nsi_depths.iloc[:,1:].sum(axis=1) > 0]\n",
    "\n",
    "# We need to melt our dataframe\n",
    "# Split return periods and scenarios\n",
    "# then pivot with fd_id and scenarios as our id vars\n",
    "nsi_d_melt = nsi_depths.melt(id_vars='fd_id', value_name='depth_ft')\n",
    "nsi_d_melt['rp'] = nsi_d_melt['variable'].str.split('_').str[0]\n",
    "nsi_d_melt['scen'] = nsi_d_melt['variable'].str.split('_').str[1]\n",
    "depths_df = nsi_d_melt.pivot(index=['fd_id', 'scen'], columns=['rp'],\n",
    "                             values='depth_ft').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57e6ce7-c2d3-4fa4-9613-a52c31e004a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:49.487718Z",
     "iopub.status.busy": "2023-12-05T20:09:49.487553Z",
     "iopub.status.idle": "2023-12-05T20:09:49.506504Z",
     "shell.execute_reply": "2023-12-05T20:09:49.505835Z",
     "shell.execute_reply.started": "2023-12-05T20:09:49.487702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need foundation type, number stories, structure value\n",
    "# for our ensemble. Structure value will be the center of \n",
    "# the distribution and will be passed to the loss estimation\n",
    "# function. Foundation type will be drawn from the implicit\n",
    "# distribution in the NSI data. For each census block, \n",
    "# we are going to get the multinomial probabilities of \n",
    "# a building having a certain foundation type & number of stories\n",
    "# Ideally, we would do this conditioned on prefirm but the\n",
    "# building year column is based on median year built from ACS\n",
    "# data\n",
    "# From the foundation type that is drawn from the multinomial in \n",
    "# the ensemble, we will get the FFE from the distribution \n",
    "# defined in the code for the Wing et al. 2022 paper\n",
    "# The point estimate version will just use default values\n",
    "\n",
    "# Start by retaining only relevant columns in nsi_struct\n",
    "# Then subset this and nsi_ref to the fd_id in nsi_depths\n",
    "# We do need sqft for elevation cost or floodproof estimates\n",
    "\n",
    "# Normally we would only keep the below, but I'm commenting those out\n",
    "# because we also want to keep found_ht\n",
    "# keep_cols = ['fd_id', 'occtype', 'val_struct']\n",
    "keep_cols = ['fd_id', 'occtype', 'val_struct', 'found_type']\n",
    "nsi_res = nsi_struct[keep_cols]\n",
    "\n",
    "# Let's merge in refs into nsi_res\n",
    "nsi_res = nsi_res.merge(nsi_ref, on='fd_id')\n",
    "\n",
    "# We're also going to merge in fzs\n",
    "nsi_res = nsi_res.merge(nsi_fz[['fd_id', 'fld_zone']], on='fd_id')\n",
    "\n",
    "# Split occtype to get the number of stories and basement\n",
    "# We only need to keep stories for the purposes\n",
    "# of estimating the distribution that stories comes from\n",
    "# We will draw basement from the foundation type\n",
    "# distribution which also gives us first floor elevation\n",
    "\n",
    "## Can comment these out...\n",
    "\n",
    "# structs = nsi_res['occtype'].str.split('-').str[1]\n",
    "# basements = structs.str[2:]\n",
    "# stories = structs.str[:2]\n",
    "\n",
    "# nsi_res = nsi_res.assign(stories=stories)\n",
    "\n",
    "# Retain only the rows that correspond to structures\n",
    "# that are exposed to flood depths\n",
    "## For this case study, we don't need to merge depths in\n",
    "# at this stage\n",
    "full_df = nsi_res[nsi_res['fd_id'].isin(nsi_depths['fd_id'])]\n",
    "\n",
    "# Merge in the depths to the struct df you are working with\n",
    "# Also merge in the refs - there are inconsistencies\n",
    "# with the cbfips column from nsi directly and the\n",
    "# block data I downloaded from the census webpage\n",
    "# You retain more structures if you use the block data \n",
    "# full_df = nsi_res_f.merge(nsi_depths, on='fd_id')\n",
    "\n",
    "# This dataset can be directly used for estimating the \n",
    "# benchmark losses of using NSI as-is\n",
    "# Use the Hazus DDFs with no uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada9027e-c015-4fec-bee8-cb05132e88b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:49.508718Z",
     "iopub.status.busy": "2023-12-05T20:09:49.508593Z",
     "iopub.status.idle": "2023-12-05T20:09:49.530003Z",
     "shell.execute_reply": "2023-12-05T20:09:49.529330Z",
     "shell.execute_reply.started": "2023-12-05T20:09:49.508704Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the fld_zone column processed for the way it needs\n",
    "# to be done for using hazus ddfs\n",
    "# Get the first character of the flood zone and only retain it\n",
    "# if it's a V zone. We are going to use A zone for A and outside\n",
    "# (if any) flood zone depth exposures\n",
    "ve_zone = np.where(full_df['fld_zone'].str[0] == 'V',\n",
    "                   'V',\n",
    "                   'A')\n",
    "full_df = full_df.assign(fz_ddf = ve_zone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1a487-2c5b-45f2-9d51-0bec86293572",
   "metadata": {},
   "source": [
    "## Get parameters for structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941ef1b8-8c14-4ba0-9ba4-879fe974dbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:49.530615Z",
     "iopub.status.busy": "2023-12-05T20:09:49.530485Z",
     "iopub.status.idle": "2023-12-05T20:09:49.555004Z",
     "shell.execute_reply": "2023-12-05T20:09:49.554373Z",
     "shell.execute_reply.started": "2023-12-05T20:09:49.530602Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are also going to use nsi_struct merged with refs\n",
    "# to determine the multinomial probabilities of basement\n",
    "# and number stories (binomial) from block level which matches\n",
    "# up with NSI tech reference on where data is randomly assigned\n",
    "# from. While there are maps from parcel data, where available, \n",
    "# it's not clear which entries have this non-random assignment. \n",
    "# In addition, it is known that parcel aggregation datasets like\n",
    "# ZTRAX may have data errors. The sources the NSI used\n",
    "# have unknown validation/accuracy so we can treat these as\n",
    "# part of estimating the distribution to draw from\n",
    "\n",
    "# The method for estimating number of stories is based on assignment\n",
    "# from parcel data. Where missing, square footage is divided by the \n",
    "# structure's footprint (when sq. ft. is missing, they take 86% of\n",
    "# the structure's footprint as sq. ft). If > 1.25,\n",
    "# a second floor is assumed\n",
    "# If no footprint is available, \n",
    "# stories is randomly assigned from a distribution that varies by\n",
    "# year built and census region. So, we can use census block again\n",
    "# here\n",
    "\n",
    "# The methodology for the structure valuation is obscure\n",
    "# and there is no reporting on how accurate it is to some\n",
    "# observed data on market values\n",
    "# In a conservative thrust, we can take the reported\n",
    "# coefficient of determination from Philadelphia Assesor's \n",
    "# methodology for estimating market values. This COD can be\n",
    "# multiplied by the estimated value from NSI for a presumably\n",
    "# conservative estimate of the standard deviation surrounding\n",
    "# structure value estimates to observed market values\n",
    "# We can also show in a representative example what would\n",
    "# happen to the loss estimate distribution\n",
    "# if the NSI COD is a factor of 2 larger. We still don't know\n",
    "# if this is a reasonable representation since we assume\n",
    "# there is no bias in the NSI structure valuation by\n",
    "# centering the noise distribution at their estimated value. \n",
    "# In reality, the Philly assessor office reports their estimates\n",
    "# are slightly biased which allows us to use a bias correction\n",
    "# factor if we used that data. Down the line, comparing\n",
    "# what the structure inventory distributions are using different\n",
    "# data sources could be very interesting, including accounting\n",
    "# for different # of RES1 buildings based on more detailed\n",
    "# and survye-based methods in the city assessor data\n",
    "# From the Nolte et al. (2023) large-scale parcel data good\n",
    "# practices data, we know that there are many issues in using parcel\n",
    "# data to fill in important data fields (even occupancy type)\n",
    "# It is not the panacea it appears framed as in the NSI technical\n",
    "# documentation\n",
    "\n",
    "# There are not nearly enough observations at the block level\n",
    "# to reliably estimate the parameter for binomial # stories\n",
    "# or multinomial foundation type. Sometimes just one observation\n",
    "# in general. Tract appears to have enough\n",
    "# This check is based on the subset of tracts (or other ref)\n",
    "# in nsi_res that are also in full_df (these are the ones) we need\n",
    "# the probabilities for\n",
    "# I guess STRUCT_REF should either be a config, \n",
    "# or something identified on-the-fly based on \n",
    "# whether we have enough observations at block or blockgroup\n",
    "# before \"rolling back\" to coarser resolution to get\n",
    "# proportions\n",
    "\n",
    "# I'm commenting this out because while this is part of the UNSAFE\n",
    "# framework, this is not easy to implement\n",
    "# for the decision analysis. \n",
    "\n",
    "# STRUCT_REF = 'tract_id'\n",
    "# struct_tot = nsi_res[nsi_res[STRUCT_REF].isin(full_df[STRUCT_REF])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04bc15-8355-4697-b864-9b77af2a1d25",
   "metadata": {},
   "source": [
    "### Number of stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f63184d1-481b-411b-a2d6-7f192d8bb8c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:49.556051Z",
     "iopub.status.busy": "2023-12-05T20:09:49.555902Z",
     "iopub.status.idle": "2023-12-05T20:09:49.579208Z",
     "shell.execute_reply": "2023-12-05T20:09:49.578470Z",
     "shell.execute_reply.started": "2023-12-05T20:09:49.556036Z"
    }
   },
   "outputs": [],
   "source": [
    "## Going to comment this out\n",
    "\n",
    "# # Get the total number of structures w/ number of stories \n",
    "# # in each block gruop\n",
    "# stories_sum = struct_tot.groupby([STRUCT_REF, 'stories']).size()\n",
    "# # Then get the proportion\n",
    "# stories_prop = stories_sum/struct_tot.groupby([STRUCT_REF]).size()\n",
    "# # Our parameters can be drawn from this table based on the bg_id\n",
    "# # of a structure we are estimating losses for\n",
    "# stories_param = stories_prop.reset_index().pivot(index=STRUCT_REF,\n",
    "#                                                  columns='stories',\n",
    "#                                                  values=0).fillna(0)\n",
    "# # Since it's a binomial distribution, we only need to specify\n",
    "# # one param. Arbitrarily choose 1S\n",
    "# # Round the param to the hundredth place\n",
    "# # Store in a dict\n",
    "# stories_param = stories_param['1S'].round(2)\n",
    "# STRY_DICT = dict(stories_param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e298834-f43d-4d33-9146-2dfc53f20097",
   "metadata": {},
   "source": [
    "### Foundation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9cbc33-bd3c-4379-9dc8-edabb73545f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:50.174893Z",
     "iopub.status.busy": "2023-12-05T20:09:50.174648Z",
     "iopub.status.idle": "2023-12-05T20:09:50.196704Z",
     "shell.execute_reply": "2023-12-05T20:09:50.195597Z",
     "shell.execute_reply.started": "2023-12-05T20:09:50.174873Z"
    }
   },
   "outputs": [],
   "source": [
    "## Going to comment this out\n",
    "\n",
    "# # Repeat procedure above\n",
    "# found_sum = struct_tot.groupby([STRUCT_REF, 'found_type']).size()\n",
    "# found_prop = found_sum/struct_tot.groupby([STRUCT_REF]).size()\n",
    "# found_param = found_prop.reset_index().pivot(index=STRUCT_REF,\n",
    "#                                              columns='found_type',\n",
    "#                                              values=0).fillna(0)\n",
    "\n",
    "# # We want a dictionary of bg_id to a list of B, C, S\n",
    "# # for direct use in our multinomial distribution draw\n",
    "# # Store params in a list (each row is bg_id and corresponds to\n",
    "# # its own probabilities of each foundation type)\n",
    "# params = found_param.values.round(2)\n",
    "# # Then create our dictionary\n",
    "# FND_DICT = dict(zip(found_param.index, params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b7511-969a-4853-8e2e-aa7b9eb805b7",
   "metadata": {},
   "source": [
    "## Load depth damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "db2b3e0d-7253-45b0-9eb0-88b5acd96bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:38:37.217851Z",
     "iopub.status.busy": "2023-12-05T21:38:37.216371Z",
     "iopub.status.idle": "2023-12-05T21:38:37.323194Z",
     "shell.execute_reply": "2023-12-05T21:38:37.321383Z",
     "shell.execute_reply.started": "2023-12-05T21:38:37.217720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load DDFs\n",
    "naccs_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'naccs_ddfs.pqt'))\n",
    "hazus_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs.pqt'))\n",
    "hazus_nounc = pd.read_parquet(join(VULN_DIR_I, 'physical', 'hazus_ddfs_nounc.pqt'))\n",
    "\n",
    "# Load helper dictionaries\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus.json'), 'r') as fp:\n",
    "    HAZUS_MAX_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus_nounc.json'), 'r') as fp:\n",
    "    HAZUS_MAX_NOUNC_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'naccs.json'), 'r') as fp:\n",
    "    NACCS_MAX_DICT = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af95c2-be74-41f7-862e-df0fe0f0beca",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c0b6e3-68e3-447d-ba42-a70f1a5a0421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T20:09:51.756684Z",
     "iopub.status.busy": "2023-12-05T20:09:51.755663Z",
     "iopub.status.idle": "2023-12-05T20:09:51.786947Z",
     "shell.execute_reply": "2023-12-05T20:09:51.785758Z",
     "shell.execute_reply.started": "2023-12-05T20:09:51.756636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reminder of the dataframes/dictionaries we have to help generate\n",
    "# our ensemble members efficiently\n",
    "# STRY_DICT\n",
    "# FND_DICT\n",
    "# FFE_DICT\n",
    "# hazus\n",
    "# naccs\n",
    "# HAZUS_MAX_DICT\n",
    "# NACCS_MAX_DICT\n",
    "# HAZUS_MAX_NOUNC_DICT\n",
    "\n",
    "# And some constants\n",
    "# COEF_VARIATION\n",
    "# N_SOW\n",
    "# RET_PERS\n",
    "\n",
    "# We need a randon number generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d28457a0-b693-4514-b38e-e2ee0a9f8742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:44:53.592258Z",
     "iopub.status.busy": "2023-12-05T21:44:53.590757Z",
     "iopub.status.idle": "2023-12-05T21:44:58.803082Z",
     "shell.execute_reply": "2023-12-05T21:44:58.802178Z",
     "shell.execute_reply.started": "2023-12-05T21:44:53.592129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Index for Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Need to create a dataframe w/ 10,000 rows for each fd_id\n",
    "# From full_df, keep fd_id, val_struct, bg_id, and the\n",
    "# depth columns. \n",
    "# The way I usually do this is with\n",
    "# df.loc[np.repeat(df.index, N)].reset_index(drop=True)\n",
    "# With this approach, we can do everything in a vectorized\n",
    "# form by passing array_like data of size N*len(df)\n",
    "# to different rng() calls to get all the draws from\n",
    "# distributions that we need\n",
    "\n",
    "## The below is what we normally would drop\n",
    "# But I have to change it for this case study\n",
    "# drop_cols = ['occtype', 'found_type', 'block_id', 'fld_zone',\n",
    "#              'stories']\n",
    "drop_cols = ['block_id', 'fld_zone']\n",
    "\n",
    "ens_df = full_df.drop(columns=drop_cols)\n",
    "ens_df = ens_df.loc[np.repeat(ens_df.index, N_SOW)].reset_index(drop=True)\n",
    "print('Created Index for Ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0718d-ebdb-44eb-ab6f-c036548a3062",
   "metadata": {},
   "source": [
    "## Sample depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4c4ec57b-c9d2-42d0-b7a1-6cc809b52561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:44:58.804762Z",
     "iopub.status.busy": "2023-12-05T21:44:58.804483Z",
     "iopub.status.idle": "2023-12-05T21:44:58.824382Z",
     "shell.execute_reply": "2023-12-05T21:44:58.823607Z",
     "shell.execute_reply.started": "2023-12-05T21:44:58.804731Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## I want to keep the code below in case we decide\n",
    "# to go back to triangular... but this does not seem\n",
    "# like the appropriate approach\n",
    "# The more appropriate thing is to treat the\n",
    "# depths as scenarios and generate SOWs around\n",
    "# these -- i.e. the DDF uncertainty importance\n",
    "# for the different scenarios\n",
    "# The way this works is there will be 3 ens_df \n",
    "# for each scenario, and we merge the corresponding\n",
    "# depth_df from depths_df to the ensemble\n",
    "# we've generated (for this case study, just values)\n",
    "\n",
    "# # We want to store our sampled depths for each \n",
    "# # return period in a list\n",
    "# # And will put these in a dataframe later\n",
    "# depths_l = []\n",
    "# # Let's loop through each return period\n",
    "# for rp in RET_PERS:\n",
    "#     # Let's store rp + scen as strings for easy reference\n",
    "#     left = rp + '_Lower'\n",
    "#     mid = rp + '_Mid'\n",
    "#     right = rp + '_Upper'\n",
    "    \n",
    "#     # Let's create a temporary dataframe with the same index\n",
    "#     # as ens_df and for just the three columns we want to use\n",
    "#     # to draw from a triangular distribution\n",
    "#     draw_df = ens_df[[left, mid, right]].copy()\n",
    "\n",
    "#     # We need a mask for when the left and right are the same\n",
    "#     # In such a case we do not need (and cannot) draw from \n",
    "#     # a triangular distribution and just need to take one of \n",
    "#     # the values\n",
    "#     no_tri_mask = draw_df[left] == draw_df[right]\n",
    "\n",
    "#     # Now, we either take the left value, or we draw from\n",
    "#     # the triangular distribution\n",
    "#     draw_df.loc[no_tri_mask, rp] = draw_df.loc[no_tri_mask][left]\n",
    "    \n",
    "#     draws = rng.triangular(draw_df.loc[~no_tri_mask][left],\n",
    "#                            draw_df.loc[~no_tri_mask][mid],\n",
    "#                            draw_df.loc[~no_tri_mask][right])\n",
    "#     draw_df.loc[~no_tri_mask, rp] = draws\n",
    "\n",
    "#     # Replace 0 values with na\n",
    "#     draw_df.loc[draw_df[rp] == 0, rp] = np.nan\n",
    "    \n",
    "#     # Now we just want to store the series for our rp\n",
    "#     # in a list which we will concat after all this\n",
    "#     # looping is done\n",
    "#     # This will be our final depth dataframe\n",
    "#     depths_l.append(draw_df[rp])\n",
    "    \n",
    "#     # Helpful logging message\n",
    "#     print('Got sampled depth values: ' + rp)\n",
    "\n",
    "# \n",
    "# depth_df = pd.concat(depths_l, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1b82c-188c-41d8-b638-69bc88db4ef9",
   "metadata": {},
   "source": [
    "## Sample structure characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9da0470a-5553-47bb-97ab-ac937aef3020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:44:58.825503Z",
     "iopub.status.busy": "2023-12-05T21:44:58.825250Z",
     "iopub.status.idle": "2023-12-05T21:46:39.092293Z",
     "shell.execute_reply": "2023-12-05T21:46:39.091189Z",
     "shell.execute_reply.started": "2023-12-05T21:44:58.825474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw values\n",
      "Generated Structure Characteristics\n"
     ]
    }
   ],
   "source": [
    "# Values\n",
    "# Draw from the structure value distribution for each property\n",
    "# normal(val_struct, val_struct*CF_DET) where these are array_like\n",
    "# I also want to treat this as truncated\n",
    "# on the lower end since there is a risk of drawing impossibly\n",
    "# low numbers (like negative) with this approach\n",
    "# https://github.com/kieranrcampbell/blog-notebooks/blob/master/\n",
    "# Fast%20vectorized%20sampling%20from%20truncated%\n",
    "# 20normal%20distributions%20in%20python.ipynb\n",
    "# outlines an approach to use numpy to do a truncated sample\n",
    "# TODO move this to a util file\n",
    "def truncnorm_rvs_recursive(x, sigma, lower_clip):\n",
    "    rng = np.random.default_rng()\n",
    "    q = rng.normal(x, sigma)\n",
    "    if np.any(q < lower_clip):\n",
    "        # Adjustment to the code provided to index the sigma vector\n",
    "        q[q < lower_clip] = truncnorm_rvs_recursive(x[q < lower_clip],\n",
    "                                                    sigma[q < lower_clip],\n",
    "                                                    lower_clip)\n",
    "\n",
    "    return q\n",
    "# Using 20000 as an artificial, arbitrary lower bound on value\n",
    "ens_df['val_s'] = truncnorm_rvs_recursive(ens_df['val_struct'],\n",
    "                                          ens_df['val_struct']*COEF_VARIATION,\n",
    "                                          20000)\n",
    "\n",
    "print('Draw values')\n",
    "\n",
    "## Commenting out the below for this case study\n",
    "\n",
    "# # Draw from the #stories distribution\n",
    "# # We do this by mapping ens_df values with STRY_DICT\n",
    "# # and passing this parameter to rng.binomial()\n",
    "# # We also need to create an array of 1s with length\n",
    "# # N_SOW * len(full_df) - i.e. len(ens_df)\n",
    "# # full_df['bg_id'].map(STRY_DICT)\n",
    "# bin_n = np.ones(len(ens_df), dtype=np.int8)\n",
    "# bin_p = ens_df[STRUCT_REF].map(STRY_DICT).values\n",
    "# # This gives us an array of 0s and 1s\n",
    "# # Based on how STRY_DICT is defined, the probability of\n",
    "# # success parameter corresponds to 1S, so we need to\n",
    "# # swap out 1 with 1S and 0 with 2S\n",
    "# stories = rng.binomial(bin_n, bin_p)\n",
    "# stories = np.where(stories == 1,\n",
    "#                    '1S',\n",
    "#                    '2S')\n",
    "\n",
    "# print('Draw stories')\n",
    "\n",
    "# # Draw from the fnd_type distribution\n",
    "# # We do the same thing as above but with\n",
    "# # the FND_DICT. This is a multinomial distribution\n",
    "# # and 0, 1, 2 correspond to B, C, S\n",
    "# # We get an array returned of the form \n",
    "# # [0, 0, 1] (if we have Slab foundation, for example)\n",
    "# # so we need to transform this into the corresponding\n",
    "# # foundation type array\n",
    "# # Can do this with fnds[fnds[0] == 1] = 'B'\n",
    "# # fnds[fnds[1]] == 1] = 'C' & fnds[fnds[2] == 1] = 'S'\n",
    "# # One way to do the mapping is by treating each\n",
    "# # row-array as a binary string and converting it\n",
    "# # to an int\n",
    "# # So you get [a, b, c] => a*2^2 + b*2^1 + c*2^0\n",
    "# # This uniquely maps to 4, 2, and 1\n",
    "# # So we can create a dict for 4: 'B', 2: 'C', and 1: 'S'\n",
    "# # and make it a pd.Series() (I think this is useful because\n",
    "# # pandas can combine this with the 1S and 2S string easily\n",
    "# # into a series and we'll need to use that bld_type\n",
    "# # for the other dicts we have)\n",
    "\n",
    "# # This is our ens_df index aligned multinomial\n",
    "# # probabilities array\n",
    "# # np.stack makes sure the dtype is correct\n",
    "# # Not sure why it is cast to object dtype if\n",
    "# # I call .values, but this works...\n",
    "\n",
    "# mult_p = np.stack(ens_df[STRUCT_REF].map(FND_DICT))\n",
    "# # This is our map of binary string/int\n",
    "# # conversions to the foundation type\n",
    "# bin_str_map = {4: 'B', 2: 'C', 1: 'S'}\n",
    "# # We need our np.ones array \n",
    "# mult_n = np.ones(len(ens_df), dtype=np.int8)\n",
    "# # Draw from mult_p\n",
    "# fnds = rng.multinomial(mult_n, mult_p)\n",
    "# # Create a series of 4, 2, and 1 from the binary strings\n",
    "# # This code accomplishes the conversion outlined in the\n",
    "# # note above and comes from this stackoverflow post\n",
    "# # https://stackoverflow.com/questions/41069825/\n",
    "# # convert-binary-01-numpy-to-integer-or-binary-string\n",
    "# fnds_ints = pd.Series(fnds.dot(2**np.arange(fnds.shape[1])[::-1]))\n",
    "# # Replace these values with the fnd_type\n",
    "# fnd_types = fnds_ints.map(bin_str_map)\n",
    "## For this case study, we use \n",
    "\n",
    "# print('Draw foundation type')\n",
    "\n",
    "# # We take fnd_types for two tasks now\n",
    "# # First, if B, it's WB type home and we\n",
    "# # combine this with stories to get the bld_type\n",
    "# # This is naccs_ddf_type \n",
    "# # We combine bld_type with fz_ddf to get hazus_ddf_type\n",
    "# # For our case study, it turns out we will use the same hazus\n",
    "# # ddf for the basement houses (_A) since no V zone houses\n",
    "# # For no basement, hazus_ddf_type does not add the _fz\n",
    "\n",
    "# # Let's get bld_type\n",
    "# # Basement type from fnd_types\n",
    "# base_types = np.where(fnd_types == 'B',\n",
    "#                       'WB',\n",
    "#                       'NB')\n",
    "\n",
    "# # Combine 1S and this\n",
    "# bld_types = pd.Series(stories) + pd.Series(base_types)\n",
    "\n",
    "# For this case study, use the below code\n",
    "# This drops the \"RES1-\" part of the occtype column\n",
    "# and keeps 1SNB, 2SNB, etc.\n",
    "ens_df['bld_types'] = ens_df['occtype'].str.split('-').str[1]\n",
    "\n",
    "# In theory, bld_type is naccs_ddf_type. No need to \n",
    "# take this storage up in practice... just refer to bld_type\n",
    "# when needed\n",
    "# For WB homes, hazus_ddf_type is bld_types + '_' + ens_df['fz_ddf']\n",
    "# For NB homes, it's bld_types\n",
    "# It makes practical sense to create a new series for this\n",
    "ens_df['hazus_types'] = np.where(ens_df['bld_types'].str[-2:] == 'WB',\n",
    "                                 ens_df['bld_types'] + '_' + ens_df['fz_ddf'],\n",
    "                                 ens_df['bld_types'])\n",
    "\n",
    "\n",
    "# We are going to use the fnd_type to draw from the\n",
    "# FFE distribution\n",
    "# Need to use np.stack to get the array of floats\n",
    "tri_params = np.stack(ens_df['found_type'].map(FFE_DICT))\n",
    "\n",
    "# Can use [:] to access like a matrix and directly input to \n",
    "# rng.triangular\n",
    "# 0, 1, and 2 are column indices corresponding to left,\n",
    "# mode, and right\n",
    "# We round this to the nearest foot\n",
    "ffes = np.round(rng.triangular(tri_params[:,0],\n",
    "                               tri_params[:,1],\n",
    "                               tri_params[:,2]))\n",
    "ens_df['ffe'] = ffes\n",
    "\n",
    "print('Generated Structure Characteristics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d7659-03e2-4c2f-bed5-5d435df1233a",
   "metadata": {},
   "source": [
    "## Estimate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d124645c-368f-4b45-b114-df8731e83aa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:46:39.094708Z",
     "iopub.status.busy": "2023-12-05T21:46:39.094414Z",
     "iopub.status.idle": "2023-12-05T21:46:46.585073Z",
     "shell.execute_reply": "2023-12-05T21:46:46.584186Z",
     "shell.execute_reply.started": "2023-12-05T21:46:39.094674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: Lower\n",
      "Adjuted depths by FFE\n",
      "\n",
      "Scenario: Mid\n",
      "Adjuted depths by FFE\n",
      "\n",
      "Scenario: Upper\n",
      "Adjuted depths by FFE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## For this case study, we're using depths as scenarios\n",
    "# Loop through each of Lower, Mid, Upper in the depths_df\n",
    "# and merge this depth_df into ens_df\n",
    "# Store this in a dictionary - it's a little easier\n",
    "ens_dfs = {}\n",
    "# Also helps to have a dictionary for the depths adjusted\n",
    "# by first floor elevation\n",
    "depth_ffes = {}\n",
    "for scen in ['Lower', 'Mid', 'Upper']:\n",
    "    print('Scenario: ' + scen)\n",
    "    # We subset to the scenario\n",
    "    depth_df = depths_df[depths_df['scen'] == scen].drop(columns=['scen'])\n",
    "    # We only need to keep properties with depth[500] > 0\n",
    "    keep_rows = depth_df['500'] > 0\n",
    "    depth_df = depth_df.loc[keep_rows]\n",
    "    # Replace 0 values with na\n",
    "    depth_df[depth_df == 0] = np.nan\n",
    "    # Let's do an inner merge so that we don't have\n",
    "    # to keep the ensemble members that correspond to \n",
    "    # 0 losses under this scenario\n",
    "    ens_dfs[scen] = ens_df.merge(depth_df, how='inner', on='fd_id')\n",
    "    # Dataframe for adjusted depths\n",
    "    # depth_df and ens_dfs\n",
    "    depth_ffes[scen] = ens_dfs[scen][RET_PERS].subtract(ens_dfs[scen]['ffe'],\n",
    "                                                        axis=0).round(1) \n",
    "    print('Adjuted depths by FFE\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "003f5629-dbc4-4da1-b408-5a0f6b96351f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:46:46.586355Z",
     "iopub.status.busy": "2023-12-05T21:46:46.586087Z",
     "iopub.status.idle": "2023-12-05T21:53:24.813825Z",
     "shell.execute_reply": "2023-12-05T21:53:24.812774Z",
     "shell.execute_reply.started": "2023-12-05T21:46:46.586325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: Lower\n",
      "Estimate Losses for NACCS & Hazus, RP: 001\n",
      "Estimate Losses for NACCS & Hazus, RP: 002\n",
      "Estimate Losses for NACCS & Hazus, RP: 005\n",
      "Estimate Losses for NACCS & Hazus, RP: 010\n",
      "Estimate Losses for NACCS & Hazus, RP: 015\n",
      "Estimate Losses for NACCS & Hazus, RP: 020\n",
      "Estimate Losses for NACCS & Hazus, RP: 025\n",
      "Estimate Losses for NACCS & Hazus, RP: 050\n",
      "Estimate Losses for NACCS & Hazus, RP: 075\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 200\n",
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Randomly assigned NACCS or HAZUS Loss\n",
      "Obtained Full Ensemble\n",
      "Calculated EAL\n",
      "Stored in dictionary\n",
      "\n",
      "Scenario: Mid\n",
      "Estimate Losses for NACCS & Hazus, RP: 001\n",
      "Estimate Losses for NACCS & Hazus, RP: 002\n",
      "Estimate Losses for NACCS & Hazus, RP: 005\n",
      "Estimate Losses for NACCS & Hazus, RP: 010\n",
      "Estimate Losses for NACCS & Hazus, RP: 015\n",
      "Estimate Losses for NACCS & Hazus, RP: 020\n",
      "Estimate Losses for NACCS & Hazus, RP: 025\n",
      "Estimate Losses for NACCS & Hazus, RP: 050\n",
      "Estimate Losses for NACCS & Hazus, RP: 075\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 200\n",
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Randomly assigned NACCS or HAZUS Loss\n",
      "Obtained Full Ensemble\n",
      "Calculated EAL\n",
      "Stored in dictionary\n",
      "\n",
      "Scenario: Upper\n",
      "Estimate Losses for NACCS & Hazus, RP: 001\n",
      "Estimate Losses for NACCS & Hazus, RP: 002\n",
      "Estimate Losses for NACCS & Hazus, RP: 005\n",
      "Estimate Losses for NACCS & Hazus, RP: 010\n",
      "Estimate Losses for NACCS & Hazus, RP: 015\n",
      "Estimate Losses for NACCS & Hazus, RP: 020\n",
      "Estimate Losses for NACCS & Hazus, RP: 025\n",
      "Estimate Losses for NACCS & Hazus, RP: 050\n",
      "Estimate Losses for NACCS & Hazus, RP: 075\n",
      "Estimate Losses for NACCS & Hazus, RP: 100\n",
      "Estimate Losses for NACCS & Hazus, RP: 200\n",
      "Estimate Losses for NACCS & Hazus, RP: 500\n",
      "Randomly assigned NACCS or HAZUS Loss\n",
      "Obtained Full Ensemble\n",
      "Calculated EAL\n",
      "Stored in dictionary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, we are going to loop through each return period\n",
    "# and estimate losses for NACCS and HAZUS using our helper\n",
    "# functions for each of these\n",
    "\n",
    "# We do this for each of the ens_df in ens_dfs\n",
    "for scen, ens_df in ens_dfs.items():\n",
    "    print('Scenario: ' + scen)\n",
    "    # Get the depth_ffe_df\n",
    "    depth_ffe_df = depth_ffes[scen]\n",
    "    \n",
    "    # We will store these in dictionaries with return period keys\n",
    "    hazus_loss = {}\n",
    "    naccs_loss = {}\n",
    "    \n",
    "    for rp in RET_PERS:\n",
    "        naccs_loss[rp] = est_naccs_loss(ens_df['bld_types'],\n",
    "                                        depth_ffe_df[rp],\n",
    "                                        naccs_ddfs,\n",
    "                                        NACCS_MAX_DICT)\n",
    "        hazus_loss[rp] = est_hazus_loss(ens_df['hazus_types'],\n",
    "                                        depth_ffe_df[rp],\n",
    "                                        hazus_ddfs,\n",
    "                                        HAZUS_MAX_DICT)\n",
    "    \n",
    "        print('Estimate Losses for NACCS & Hazus, RP: ' + rp)\n",
    "    \n",
    "    # Then, we convert these to dataframes\n",
    "    hazus_df = pd.DataFrame.from_dict(hazus_loss)\n",
    "    naccs_df = pd.DataFrame.from_dict(naccs_loss)\n",
    "    \n",
    "    # And we use a binomial rv to determine if we use the hazus or naccs\n",
    "    # loss estimate for a particular return period\n",
    "    # Binomial\n",
    "    random_loss = rng.binomial(1, .5, size=len(ens_df))\n",
    "    \n",
    "    # Get indices to take from each df\n",
    "    hazus_ind = (random_loss == 0)\n",
    "    naccs_ind = (random_loss == 1)\n",
    "    \n",
    "    # Concat subsetted dataframes\n",
    "    losses_df = pd.concat([hazus_df.loc[hazus_ind],\n",
    "                           naccs_df.loc[naccs_ind]], axis=0).sort_index()\n",
    "    # Rename columns to make it more clear what this is\n",
    "    losses_df.columns = ['rel_dam_' + x for x in losses_df.columns]\n",
    "    \n",
    "    # Add a column indicating NACCS or Hazus ddf\n",
    "    losses_df.loc[hazus_ind, 'ddf'] = 'HAZUS'\n",
    "    losses_df.loc[naccs_ind, 'ddf'] = 'NACCS'\n",
    "    \n",
    "    print('Randomly assigned NACCS or HAZUS Loss')\n",
    "\n",
    "    # Now we concat these with ens_df, stories, fnd_type,\n",
    "    # ffe, structure value, and depth_ffe_df\n",
    "    depth_ffe = pd.DataFrame.from_dict(depth_ffe_df)\n",
    "    \n",
    "    # Add clearer column names\n",
    "    depth_ffe.columns = ['depth_ffe_' + x for x in depth_ffe.columns]\n",
    "    \n",
    "    ## Slight modification for our case study\n",
    "    # Concat for our full ensemble\n",
    "    # ens_df = pd.concat([ens_df, losses_df, depth_ffe,\n",
    "    #                     pd.Series(stories, name='stories'),\n",
    "    #                     pd.Series(fnd_types, name='fnd_type'),\n",
    "    #                     pd.Series(ffes, name='ffe'),\n",
    "    #                     pd.Series(values, name='val_s')],\n",
    "    #                    axis=1)\n",
    "    ## For our case study, ens_df contains occtype & \n",
    "    # found_ht, so don't need to add structure characteristics\n",
    "    # back in\n",
    "    ens_df = pd.concat([ens_df, losses_df, depth_ffe],\n",
    "                       axis=1)\n",
    "    \n",
    "    # Get relative damage columns\n",
    "    rel_cols = [x for x in ens_df.columns if 'rel_dam' in x]\n",
    "    # For each relative damage column, scale by val_s, the structure\n",
    "    # value realization\n",
    "    for col in rel_cols:\n",
    "        rp = col.split('_')[-1]\n",
    "        ens_df['loss_' + rp] = ens_df[col]*ens_df['val_s']\n",
    "    \n",
    "    print('Obtained Full Ensemble')\n",
    "\n",
    "    # Now we calculate EAL\n",
    "    # We will use trapezoidal approximation for this\n",
    "    # Using trapezoid method and adding bin of lowest probability\n",
    "    # events to obtain expected annual \n",
    "    \n",
    "    # We make a list of our loss columns\n",
    "    loss_list = ['loss_' + x for x in RET_PERS]\n",
    "    # As well as the corresponding probabilities\n",
    "    p_rp_list = [round(1/int(x), 4) for x in RET_PERS]\n",
    "    \n",
    "    # Then we create an empty series\n",
    "    eal = pd.Series(index=ens_df.index).fillna(0)\n",
    "    \n",
    "    # We loop through our loss list and apply the \n",
    "    # trapezoidal approximation\n",
    "    for i in range(len(loss_list) - 1):\n",
    "        loss1 = ens_df[loss_list[i]]\n",
    "        loss2 = ens_df[loss_list[i+1]]\n",
    "        rp1 = p_rp_list[i]\n",
    "        rp2 = p_rp_list[i+1]\n",
    "        # We add each approximation\n",
    "        eal += (loss1 + loss2)*(rp1-rp2)/2\n",
    "    # This is the final trapezoid to add in\n",
    "    final_eal = eal + ens_df[loss_list[-1]]*p_rp_list[-1]\n",
    "    print('Calculated EAL')\n",
    "    # Add eal column to our dataframe\n",
    "    ens_df = pd.concat([ens_df, pd.Series(final_eal, name='eal')], axis=1)\n",
    "    \n",
    "    # Let's also get the SOW index - start at 0\n",
    "    sow_ind = np.arange(len(ens_df))%N_SOW\n",
    "    ens_df = pd.concat([ens_df, pd.Series(sow_ind, name='sow_ind')], axis=1)\n",
    "\n",
    "    # Put this back in ens_dfs[scen]\n",
    "    ens_dfs[scen] = ens_df\n",
    "    print('Stored in dictionary\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "36d17a3f-0428-4ae9-8e0e-7bff775c94c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:44:11.319771Z",
     "iopub.status.busy": "2023-12-05T21:44:11.318114Z",
     "iopub.status.idle": "2023-12-05T21:44:11.403184Z",
     "shell.execute_reply": "2023-12-05T21:44:11.401900Z",
     "shell.execute_reply.started": "2023-12-05T21:44:11.319675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rel_dam_001</th>\n",
       "      <th>rel_dam_001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109569</td>\n",
       "      <td>0.049621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.118292</td>\n",
       "      <td>0.240095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089747</td>\n",
       "      <td>0.095902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.304506</td>\n",
       "      <td>0.342093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.132197</td>\n",
       "      <td>0.092615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309995</th>\n",
       "      <td>0.167914</td>\n",
       "      <td>0.188438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309996</th>\n",
       "      <td>0.049742</td>\n",
       "      <td>0.030801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309997</th>\n",
       "      <td>0.073241</td>\n",
       "      <td>0.061083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309998</th>\n",
       "      <td>0.061405</td>\n",
       "      <td>0.056964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309999</th>\n",
       "      <td>0.069146</td>\n",
       "      <td>0.085102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2310000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rel_dam_001  rel_dam_001\n",
       "0           0.109569     0.049621\n",
       "1           0.118292     0.240095\n",
       "2           0.089747     0.095902\n",
       "3           0.304506     0.342093\n",
       "4           0.132197     0.092615\n",
       "...              ...          ...\n",
       "2309995     0.167914     0.188438\n",
       "2309996     0.049742     0.030801\n",
       "2309997     0.073241     0.061083\n",
       "2309998     0.061405     0.056964\n",
       "2309999     0.069146     0.085102\n",
       "\n",
       "[2310000 rows x 2 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a51a9926-7c09-4d66-b60d-92a3ea946afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T19:01:42.250995Z",
     "iopub.status.busy": "2023-12-05T19:01:42.250859Z",
     "iopub.status.idle": "2023-12-05T19:02:22.193049Z",
     "shell.execute_reply": "2023-12-05T19:02:22.192284Z",
     "shell.execute_reply.started": "2023-12-05T19:01:42.250982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out our ensemble dfs\n",
    "ens_out_filep = join(FO, 'ensemble.pqt')\n",
    "prepare_saving(ens_out_filep)\n",
    "ens_dfs['Lower'].to_parquet(join(FO, 'ensemble_Lower.pqt'))\n",
    "ens_dfs['Mid'].to_parquet(join(FO, 'ensemble_Mid.pqt'))\n",
    "ens_dfs['Upper'].to_parquet(join(FO, 'ensemble_Upper.pqt'))\n",
    "\n",
    "# In addition to writing out the scenario ddfs\n",
    "# we also want a single df that gives us the \n",
    "# fd_id, sow_ind, val_s, & eal\n",
    "# for each of the scenarios\n",
    "# val_s is the same for each fd_id/sow_ind, so we really just\n",
    "# need to merge in the eal estimates into\n",
    "# a single df\n",
    "keep_cols = ['fd_id', 'val_s', 'sow_ind', 'eal']\n",
    "merge_cols = ['fd_id', 'sow_ind', 'eal']\n",
    "id_cols = ['fd_id', 'sow_ind']\n",
    "ens_df = ens_dfs['Upper'][keep_cols]\n",
    "ens_df = ens_df.merge(ens_dfs['Mid'][merge_cols], on=id_cols,\n",
    "                      how='left',\n",
    "                      suffixes=['_Upper', None])\n",
    "ens_df = ens_df.merge(ens_dfs['Lower'][merge_cols], on=id_cols,\n",
    "                      how='left',\n",
    "                      suffixes=['_Mid', '_Lower'])\n",
    "\n",
    "ens_df.to_parquet(ens_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67e94c-6821-4f3e-b8ff-4a52ecac8882",
   "metadata": {},
   "source": [
    "# Generate losses without uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba65bf86-18ac-42af-b769-6dbf1b8b5ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T19:02:22.193956Z",
     "iopub.status.busy": "2023-12-05T19:02:22.193803Z",
     "iopub.status.idle": "2023-12-05T19:02:22.275106Z",
     "shell.execute_reply": "2023-12-05T19:02:22.274418Z",
     "shell.execute_reply.started": "2023-12-05T19:02:22.193942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate Losses for Hazus Default, RP: 001\n",
      "Estimate Losses for Hazus Default, RP: 002\n",
      "Estimate Losses for Hazus Default, RP: 005\n",
      "Estimate Losses for Hazus Default, RP: 010\n",
      "Estimate Losses for Hazus Default, RP: 015\n",
      "Estimate Losses for Hazus Default, RP: 020\n",
      "Estimate Losses for Hazus Default, RP: 025\n",
      "Estimate Losses for Hazus Default, RP: 050\n",
      "Estimate Losses for Hazus Default, RP: 075\n",
      "Estimate Losses for Hazus Default, RP: 100\n",
      "Estimate Losses for Hazus Default, RP: 200\n",
      "Estimate Losses for Hazus Default, RP: 500\n"
     ]
    }
   ],
   "source": [
    "# We are taking the full_df dataframe values as-is\n",
    "# to get eal estimates for our benchmark\n",
    "# From occtype, get 1SNB, etc. and combine with fz_ddf\n",
    "# for our ddf_id column\n",
    "b_types = full_df['occtype'].str.split('-').str[1]\n",
    "\n",
    "hazus_ddf_types = pd.Series(np.where(b_types.str[-2:] == 'WB',\n",
    "                                     b_types + '_' + full_df['fz_ddf'],\n",
    "                                     b_types))\n",
    "\n",
    "# We need to get the depth columns translated into ffe adjusted\n",
    "# exposure depths\n",
    "# We can subset on depths_df to get a depth dataframe\n",
    "# and then we can subtract it by the found_ht column\n",
    "# for our adjustment\n",
    "depth_df = depths_df[depths_df['scen'] == 'Mid'].drop(columns=['scen'])\n",
    "depth_df = depth_df.reset_index(drop=True)\n",
    "# Replace 0 with na\n",
    "depth_df[depth_df == 0] = np.nan\n",
    "\n",
    "# Merge to get indices aligned\n",
    "full_df = depth_df.merge(full_df, on='fd_id')\n",
    "\n",
    "# Adjust by ffe\n",
    "depth_ffe_df = full_df[RET_PERS].subtract(full_df['found_ht'], axis=0).round()\n",
    "\n",
    "# Get losses with point estimates \n",
    "hazus_def_loss = {}\n",
    "for rp in RET_PERS:\n",
    "    hazus_def_loss[rp] = est_hazus_loss_nounc(hazus_ddf_types,\n",
    "                                              depth_ffe_df[rp],\n",
    "                                              hazus_nounc,\n",
    "                                              HAZUS_MAX_NOUNC_DICT)\n",
    "\n",
    "    print('Estimate Losses for Hazus Default, RP: ' + rp)\n",
    "\n",
    "# Same processing as before\n",
    "hazus_def_df = pd.DataFrame.from_dict(hazus_def_loss)\n",
    "hazus_def_df.columns = ['rel_dam_' + x for x in hazus_def_df.columns]\n",
    "\n",
    "# Join dataframes\n",
    "hazus_def = pd.concat([full_df, hazus_def_df],\n",
    "                      axis=1)\n",
    "\n",
    "# Get losses from rel_dam columns\n",
    "for col in hazus_def_df.columns:\n",
    "    rp = col.split('_')[-1]\n",
    "    hazus_def['loss_' + rp] = hazus_def[col]*hazus_def['val_struct']\n",
    "\n",
    "# Now, calculate EAL\n",
    "# We can use loss_list and p_rp_list we defined earlier\n",
    "\n",
    "# Then we create an empty series\n",
    "eal = pd.Series(index=hazus_def.index).fillna(0)\n",
    "\n",
    "# We make a list of our loss columns\n",
    "loss_list = ['loss_' + x for x in RET_PERS]\n",
    "# As well as the corresponding probabilities\n",
    "p_rp_list = [round(1/int(x), 4) for x in RET_PERS]\n",
    "\n",
    "# We loop through our loss list and apply the \n",
    "# trapezoidal approximation\n",
    "for i in range(len(loss_list) - 1):\n",
    "    loss1 = hazus_def[loss_list[i]]\n",
    "    loss2 = hazus_def[loss_list[i+1]]\n",
    "    rp1 = p_rp_list[i]\n",
    "    rp2 = p_rp_list[i+1]\n",
    "    # We add each approximation\n",
    "    eal += (loss1 + loss2)*(rp1-rp2)/2\n",
    "# This is the final trapezoid to add in\n",
    "final_eal = eal + hazus_def[loss_list[-1]]*p_rp_list[-1]\n",
    "\n",
    "# Add eal column to our dataframe\n",
    "hazus_def = pd.concat([hazus_def, pd.Series(final_eal, name='eal')], axis=1)\n",
    "\n",
    "# Write out the default damages df\n",
    "hazus_def_out_filep = join(FO, 'benchmark_loss.pqt')\n",
    "prepare_saving(hazus_def_out_filep)\n",
    "hazus_def.to_parquet(hazus_def_out_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d8378-206a-4b4f-8194-2c5f0b9ec87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

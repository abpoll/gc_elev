{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df130da-8ff7-481a-9f41-49220ebbb9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:05:56.688889Z",
     "iopub.status.busy": "2023-10-27T22:05:56.688440Z",
     "iopub.status.idle": "2023-10-27T22:05:56.731968Z",
     "shell.execute_reply": "2023-10-27T22:05:56.730536Z",
     "shell.execute_reply.started": "2023-10-27T22:05:56.688843Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea6c73e-9b4f-4bf1-a40c-0d4105e9eafb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:05:56.736055Z",
     "iopub.status.busy": "2023-10-27T22:05:56.734884Z",
     "iopub.status.idle": "2023-10-27T22:05:59.020093Z",
     "shell.execute_reply": "2023-10-27T22:05:59.018631Z",
     "shell.execute_reply.started": "2023-10-27T22:05:56.736000Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util.files import *\n",
    "from util.const import *\n",
    "from util.ddfs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a43d5d-4b14-43c7-80ec-eb029a6e5529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:05:59.021675Z",
     "iopub.status.busy": "2023-10-27T22:05:59.021187Z",
     "iopub.status.idle": "2023-10-27T22:05:59.040997Z",
     "shell.execute_reply": "2023-10-27T22:05:59.039982Z",
     "shell.execute_reply.started": "2023-10-27T22:05:59.021649Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIPS will be passed in as an argument, one day...\n",
    "FIPS = '34007'\n",
    "# STATE ABBR and NATION will be derived from FIPS, one day...\n",
    "STATEABBR = 'NJ'\n",
    "NATION = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6111f5a-c61f-469a-92b2-265c90f3cd6d",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120b6681-df66-4e80-8f4b-d2e7495fe878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:05:59.041807Z",
     "iopub.status.busy": "2023-10-27T22:05:59.041591Z",
     "iopub.status.idle": "2023-10-27T22:05:59.081990Z",
     "shell.execute_reply": "2023-10-27T22:05:59.080494Z",
     "shell.execute_reply.started": "2023-10-27T22:05:59.041783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate structure ensemble\n",
    "# Merge hazard data in\n",
    "# Sample from the depth grids\n",
    "# Add our vulnerability uncertainty\n",
    "# (it's conditioned on the depth value in \n",
    "# a particular state of the world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23370d86-7c25-4b42-afad-11395b3a819f",
   "metadata": {},
   "source": [
    "## Load and subset exposure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59719ccf-11fb-4507-bc0a-86cf020e58d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:05:59.089014Z",
     "iopub.status.busy": "2023-10-27T22:05:59.088312Z",
     "iopub.status.idle": "2023-10-27T22:06:00.177849Z",
     "shell.execute_reply": "2023-10-27T22:06:00.176990Z",
     "shell.execute_reply.started": "2023-10-27T22:05:59.088970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the single family homes,\n",
    "# the fd_id/reference file\n",
    "# the fd_id/depths file\n",
    "# the fd_id flood zone file\n",
    "nsi_struct = gpd.read_file(join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg'))\n",
    "nsi_ref = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_ref.pqt'))\n",
    "nsi_depths = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_depths.pqt'))\n",
    "nsi_fz = pd.read_parquet(join(EXP_DIR_I, FIPS, 'nsi_fz.pqt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e57e6ce7-c2d3-4fa4-9613-a52c31e004a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:06:00.178739Z",
     "iopub.status.busy": "2023-10-27T22:06:00.178577Z",
     "iopub.status.idle": "2023-10-27T22:06:00.213331Z",
     "shell.execute_reply": "2023-10-27T22:06:00.212309Z",
     "shell.execute_reply.started": "2023-10-27T22:06:00.178723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Need foundation type, number stories, structure value\n",
    "# for our ensemble. Structure value will be the center of \n",
    "# the distribution and will be passed to the loss estimation\n",
    "# function. Foundation type will be drawn from the implicit\n",
    "# distribution in the NSI data. For each census block, \n",
    "# we are going to get the multinomial probabilities of \n",
    "# a building having a certain foundation type & number of stories\n",
    "# Ideally, we would do this conditioned on prefirm but the\n",
    "# building year column is based on median year built from ACS\n",
    "# data\n",
    "# From the foundation type that is drawn from the multinomial in \n",
    "# the ensemble, we will get the FFE from the distribution \n",
    "# defined in the code for the Wing et al. 2022 paper\n",
    "# The point estimate version will just use default values\n",
    "\n",
    "# Start by retaining only relevant columns in nsi_struct\n",
    "# Then subset this and nsi_ref to the fd_id in nsi_depths\n",
    "keep_cols = ['fd_id', 'occtype', 'found_type', 'val_struct']\n",
    "nsi_res = nsi_struct[keep_cols]\n",
    "\n",
    "# Let's merge in refs into nsi_res\n",
    "nsi_res = nsi_res.merge(nsi_ref, on='fd_id')\n",
    "\n",
    "# We're also going to merge in fzs\n",
    "nsi_res = nsi_res.merge(nsi_fz[['fd_id', 'fld_zone']], on='fd_id')\n",
    "\n",
    "# Split occtype to get the number of stories and basement\n",
    "# We only need to keep stories for the purposes\n",
    "# of estimating the distribution that stories comes from\n",
    "# We will draw basement from the foundation type\n",
    "# distribution which also gives us first floor elevation\n",
    "structs = nsi_res['occtype'].str.split('-').str[1]\n",
    "basements = structs.str[2:]\n",
    "stories = structs.str[:2]\n",
    "\n",
    "nsi_res = nsi_res.assign(stories=stories)\n",
    "\n",
    "# Retain only the rows that correspond to structures\n",
    "# that are exposed to flood depths\n",
    "nsi_res_f = nsi_res[nsi_res['fd_id'].isin(nsi_depths['fd_id'])]\n",
    "\n",
    "# Merge in the depths to the struct df you are working with\n",
    "# Also merge in the refs - there are inconsistencies\n",
    "# with the cbfips column from nsi directly and the\n",
    "# block data I downloaded from the census webpage\n",
    "# You retain more structures if you use the block data \n",
    "full_df = nsi_res_f.merge(nsi_depths, on='fd_id')\n",
    "\n",
    "# This dataset can be directly used for estimating the \n",
    "# benchmark losses of using NSI as-is\n",
    "# Use the Hazus DDFs with no uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada9027e-c015-4fec-bee8-cb05132e88b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:06:00.214272Z",
     "iopub.status.busy": "2023-10-27T22:06:00.214117Z",
     "iopub.status.idle": "2023-10-27T22:06:00.231928Z",
     "shell.execute_reply": "2023-10-27T22:06:00.230860Z",
     "shell.execute_reply.started": "2023-10-27T22:06:00.214257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the fld_zone column processed for the way it needs\n",
    "# to be done for using hazus ddfs\n",
    "# Get the first character of the flood zone and only retain it\n",
    "# if it's a V zone. We are going to use A zone for A and outside\n",
    "# (if any) flood zone depth exposures\n",
    "ve_zone = np.where(full_df['fld_zone'].str[0] == 'V',\n",
    "                   'V',\n",
    "                   'A')\n",
    "full_df = full_df.assign(fz_ddf = ve_zone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1a487-2c5b-45f2-9d51-0bec86293572",
   "metadata": {},
   "source": [
    "## Get parameters for structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "941ef1b8-8c14-4ba0-9ba4-879fe974dbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:06:00.232964Z",
     "iopub.status.busy": "2023-10-27T22:06:00.232796Z",
     "iopub.status.idle": "2023-10-27T22:06:00.260993Z",
     "shell.execute_reply": "2023-10-27T22:06:00.259623Z",
     "shell.execute_reply.started": "2023-10-27T22:06:00.232947Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are also going to use nsi_struct merged with refs\n",
    "# to determine the multinomial probabilities of basement\n",
    "# and number stories (binomial) from block level which matches\n",
    "# up with NSI tech reference on where data is randomly assigned\n",
    "# from. While there are maps from parcel data, where available, \n",
    "# it's not clear which entries have this non-random assignment. \n",
    "# In addition, it is known that parcel aggregation datasets like\n",
    "# ZTRAX may have data errors. The sources the NSI used\n",
    "# have unknown validation/accuracy so we can treat these as\n",
    "# part of estimating the distribution to draw from\n",
    "\n",
    "# The method for estimating number of stories is based on assignment\n",
    "# from parcel data. Where missing, square footage is divided by the \n",
    "# structure's footprint (when sq. ft. is missing, they take 86% of\n",
    "# the structure's footprint as sq. ft). If > 1.25,\n",
    "# a second floor is assumed\n",
    "# If no footprint is available, \n",
    "# stories is randomly assigned from a distribution that varies by\n",
    "# year built and census region. So, we can use census block again\n",
    "# here\n",
    "\n",
    "# The methodology for the structure valuation is obscure\n",
    "# and there is no reporting on how accurate it is to some\n",
    "# observed data on market values\n",
    "# In a conservative thrust, we can take the reported\n",
    "# coefficient of determination from Philadelphia Assesor's \n",
    "# methodology for estimating market values. This COD can be\n",
    "# multiplied by the estimated value from NSI for a presumably\n",
    "# conservative estimate of the standard deviation surrounding\n",
    "# structure value estimates to observed market values\n",
    "# We can also show in a representative example what would\n",
    "# happen to the loss estimate distribution\n",
    "# if the NSI COD is a factor of 2 larger. We still don't know\n",
    "# if this is a reasonable representation since we assume\n",
    "# there is no bias in the NSI structure valuation by\n",
    "# centering the noise distribution at their estimated value. \n",
    "# In reality, the Philly assessor office reports their estimates\n",
    "# are slightly biased which allows us to use a bias correction\n",
    "# factor if we used that data. Down the line, comparing\n",
    "# what the structure inventory distributions are using different\n",
    "# data sources could be very interesting, including accounting\n",
    "# for different # of RES1 buildings based on more detailed\n",
    "# and survye-based methods in the city assessor data\n",
    "# From the Nolte et al. (2023) large-scale parcel data good\n",
    "# practices data, we know that there are many issues in using parcel\n",
    "# data to fill in important data fields (even occupancy type)\n",
    "# It is not the panacea it appears framed as in the NSI technical\n",
    "# documentation\n",
    "\n",
    "# There are not nearly enough observations at the block level\n",
    "# to reliably estimate the parameter for binomial # stories\n",
    "# or multinomial foundation type. Sometimes just one observation\n",
    "# in general. Tract appears to have enough\n",
    "# This check is based on the subset of tracts (or other ref)\n",
    "# in nsi_res that are also in full_df (these are the ones) we need\n",
    "# the probabilities for\n",
    "STRUCT_REF = 'tract_id'\n",
    "struct_tot = nsi_res[nsi_res[STRUCT_REF].isin(full_df[STRUCT_REF])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04bc15-8355-4697-b864-9b77af2a1d25",
   "metadata": {},
   "source": [
    "### Number of stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f63184d1-481b-411b-a2d6-7f192d8bb8c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:06:00.262605Z",
     "iopub.status.busy": "2023-10-27T22:06:00.262406Z",
     "iopub.status.idle": "2023-10-27T22:06:00.298131Z",
     "shell.execute_reply": "2023-10-27T22:06:00.297051Z",
     "shell.execute_reply.started": "2023-10-27T22:06:00.262587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the total number of structures w/ number of stories \n",
    "# in each block gruop\n",
    "stories_sum = struct_tot.groupby([STRUCT_REF, 'stories']).size()\n",
    "# Then get the proportion\n",
    "stories_prop = stories_sum/struct_tot.groupby([STRUCT_REF]).size()\n",
    "# Our parameters can be drawn from this table based on the bg_id\n",
    "# of a structure we are estimating losses for\n",
    "stories_param = stories_prop.reset_index().pivot(index=STRUCT_REF,\n",
    "                                                 columns='stories',\n",
    "                                                 values=0).fillna(0)\n",
    "# Since it's a binomial distribution, we only need to specify\n",
    "# one param. Arbitrarily choose 1S\n",
    "# Round the param to the hundredth place\n",
    "# Store in a dict\n",
    "stories_param = stories_param['1S'].round(2)\n",
    "STRY_DICT = dict(stories_param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e298834-f43d-4d33-9146-2dfc53f20097",
   "metadata": {},
   "source": [
    "### Foundation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc9cbc33-bd3c-4379-9dc8-edabb73545f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:06:00.757189Z",
     "iopub.status.busy": "2023-10-27T22:06:00.756939Z",
     "iopub.status.idle": "2023-10-27T22:06:00.804152Z",
     "shell.execute_reply": "2023-10-27T22:06:00.802305Z",
     "shell.execute_reply.started": "2023-10-27T22:06:00.757169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Repeat procedure above\n",
    "found_sum = struct_tot.groupby([STRUCT_REF, 'found_type']).size()\n",
    "found_prop = found_sum/struct_tot.groupby([STRUCT_REF]).size()\n",
    "found_param = found_prop.reset_index().pivot(index=STRUCT_REF,\n",
    "                                             columns='found_type',\n",
    "                                             values=0).fillna(0)\n",
    "\n",
    "# We want a dictionary of bg_id to a list of B, C, S\n",
    "# for direct use in our multinomial distribution draw\n",
    "# Store params in a list (each row is bg_id and corresponds to\n",
    "# its own probabilities of each foundation type)\n",
    "params = found_param.values.round(2)\n",
    "# Then create our dictionary\n",
    "FND_DICT = dict(zip(found_param.index, params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b7511-969a-4853-8e2e-aa7b9eb805b7",
   "metadata": {},
   "source": [
    "## Load depth damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2b3e0d-7253-45b0-9eb0-88b5acd96bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:06:01.675248Z",
     "iopub.status.busy": "2023-10-27T22:06:01.674717Z",
     "iopub.status.idle": "2023-10-27T22:06:01.738230Z",
     "shell.execute_reply": "2023-10-27T22:06:01.736553Z",
     "shell.execute_reply.started": "2023-10-27T22:06:01.675198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load DDFs\n",
    "naccs = pd.read_csv(join(VULN_DIR_I, 'physical', 'naccs_ddfs.csv'))\n",
    "hazus = pd.read_csv(join(VULN_DIR_I, 'physical', 'hazus_ddfs.csv'))\n",
    "\n",
    "# Load helper dictionaries\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus.json'), 'r') as fp:\n",
    "    HAZUS_MAX_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'hazus_nounc.json'), 'r') as fp:\n",
    "    HAZUS_MAX_NOUNC_DICT = json.load(fp)\n",
    "\n",
    "with open(join(VULN_DIR_I, 'physical', 'naccs.json'), 'r') as fp:\n",
    "    NACCS_MAX_DICT = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af95c2-be74-41f7-862e-df0fe0f0beca",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c0b6e3-68e3-447d-ba42-a70f1a5a0421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:06:02.574407Z",
     "iopub.status.busy": "2023-10-27T22:06:02.572839Z",
     "iopub.status.idle": "2023-10-27T22:06:02.607764Z",
     "shell.execute_reply": "2023-10-27T22:06:02.605932Z",
     "shell.execute_reply.started": "2023-10-27T22:06:02.574341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reminder of the dataframes/dictionaries we have to help generate\n",
    "# our ensemble members efficiently\n",
    "# STRY_DICT\n",
    "# FND_DICT\n",
    "# FFE_DICT\n",
    "# hazus\n",
    "# naccs\n",
    "# HAZUS_MAX_DICT\n",
    "# NACCS_MAX_DICT\n",
    "# HAZUS_MAX_NOUNC_DICT\n",
    "\n",
    "# And some constants\n",
    "# COEF_VARIATION\n",
    "# N_SOW\n",
    "# RET_PERS\n",
    "\n",
    "# We need a randon number generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28457a0-b693-4514-b38e-e2ee0a9f8742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:06:03.232497Z",
     "iopub.status.busy": "2023-10-27T22:06:03.231997Z",
     "iopub.status.idle": "2023-10-27T22:06:08.978766Z",
     "shell.execute_reply": "2023-10-27T22:06:08.977938Z",
     "shell.execute_reply.started": "2023-10-27T22:06:03.232451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Index for Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Need to create a dataframe w/ 10,000 rows for each fd_id\n",
    "# From full_df, keep fd_id, val_struct, bg_id, and the\n",
    "# depth columns. \n",
    "# The way I usually do this is with\n",
    "# df.loc[np.repeat(df.index, N)].reset_index(drop=True)\n",
    "# With this approach, we can do everything in a vectorized\n",
    "# form by passing array_like data of size N*len(df)\n",
    "# to different rng() calls to get all the draws from\n",
    "# distributions that we need\n",
    "drop_cols = ['occtype', 'found_type', 'block_id', 'fld_zone',\n",
    "             'tract_id', 'zcta_id', 'stories']\n",
    "\n",
    "ens_df = full_df.drop(columns=drop_cols)\n",
    "ens_df = ens_df.loc[np.repeat(ens_df.index, N_SOW)].reset_index(drop=True)\n",
    "print('Created Index for Ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c4ec57b-c9d2-42d0-b7a1-6cc809b52561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:17:49.849564Z",
     "iopub.status.busy": "2023-10-27T22:17:49.849330Z",
     "iopub.status.idle": "2023-10-27T22:17:50.536227Z",
     "shell.execute_reply": "2023-10-27T22:17:50.534361Z",
     "shell.execute_reply.started": "2023-10-27T22:17:49.849546Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, let's draw the depth values for each return period\n",
    "# We can create a list of the depths that we draw from \n",
    "# each return period\n",
    "# This should be pretty fast\n",
    "# For each rp in RET_PERS\n",
    "# we get the rp_Lower, rp_Mid, rp_Upper columns\n",
    "# We should define subsets where \n",
    "# 1) lower == right\n",
    "# 2) all else\n",
    "# When 1 - take the value from high\n",
    "# When 2 - do triangular\n",
    "\n",
    "# We should end up with a numpy array of depths\n",
    "# which are indexed to the fd_id_SOW_index \n",
    "# We can concatenate on index into a depths dataframe\n",
    "# Then, the rest of the ensemble generation should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ece96c8a-90d1-4da4-aaeb-e666833246ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:23:23.980870Z",
     "iopub.status.busy": "2023-10-27T22:23:23.980309Z",
     "iopub.status.idle": "2023-10-27T22:23:24.019144Z",
     "shell.execute_reply": "2023-10-27T22:23:24.017289Z",
     "shell.execute_reply.started": "2023-10-27T22:23:23.980820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08310191930420069"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.triangular(0, .1, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c87af80-e188-4545-9a3c-707fd409e320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:35:00.453254Z",
     "iopub.status.busy": "2023-10-27T22:35:00.452696Z",
     "iopub.status.idle": "2023-10-27T22:35:02.289953Z",
     "shell.execute_reply": "2023-10-27T22:35:02.288840Z",
     "shell.execute_reply.started": "2023-10-27T22:35:00.453203Z"
    }
   },
   "outputs": [],
   "source": [
    "# Don't need to round - do that in process_haz\n",
    "temp = ens_df[['500_Lower', '500_Mid', '500_Upper']].round()\n",
    "no_tri_mask = temp['500_Lower'] == temp['500_Upper']\n",
    "\n",
    "temp.loc[no_tri_mask, '500'] = temp.loc[no_tri_mask]['500_Lower']\n",
    "\n",
    "temp.loc[~no_tri_mask, '500'] = rng.triangular(temp_tri['500_Lower'],\n",
    "                                               temp_tri['500_Mid'],\n",
    "                                               temp_tri['500_Upper'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8787bd6-dfd0-4a09-895d-9f06c2019464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:35:17.438779Z",
     "iopub.status.busy": "2023-10-27T22:35:17.438216Z",
     "iopub.status.idle": "2023-10-27T22:35:17.489022Z",
     "shell.execute_reply": "2023-10-27T22:35:17.487174Z",
     "shell.execute_reply.started": "2023-10-27T22:35:17.438729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>500_Lower</th>\n",
       "      <th>500_Mid</th>\n",
       "      <th>500_Upper</th>\n",
       "      <th>500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.653215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.543746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.407360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   500_Lower  500_Mid  500_Upper       500\n",
       "0        0.0      0.0        1.0  0.653215\n",
       "1        0.0      0.0        1.0  0.003464\n",
       "2        0.0      0.0        1.0  0.332406\n",
       "3        0.0      0.0        1.0  0.543746\n",
       "4        0.0      0.0        1.0  0.407360"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95c824c0-1059-4718-be43-6df82ef3023d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T22:31:43.785481Z",
     "iopub.status.busy": "2023-10-27T22:31:43.784940Z",
     "iopub.status.idle": "2023-10-27T22:31:43.823888Z",
     "shell.execute_reply": "2023-10-27T22:31:43.822123Z",
     "shell.execute_reply.started": "2023-10-27T22:31:43.785434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46943493, 0.42553854, 0.31828201, ..., 1.75887132, 4.92594985,\n",
       "       1.38506456])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67e94c-6821-4f3e-b8ff-4a52ecac8882",
   "metadata": {},
   "source": [
    "# Generate losses without uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65bf86-18ac-42af-b769-6dbf1b8b5ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
